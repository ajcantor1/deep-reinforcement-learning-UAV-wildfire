{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgpath2mpl\n",
      "  Downloading svgpath2mpl-1.0.0-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "Installing collected packages: svgpath2mpl\n",
      "Successfully installed svgpath2mpl-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstactFireEnv(metaclass = ABCMeta):\n",
    "\n",
    "  def __init__(self, _height, _width):\n",
    "      self._height = _height\n",
    "      self._width  = _width\n",
    "      self._time_steps = 0\n",
    "      self._observation = None\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self._height\n",
    "  \n",
    "  @property\n",
    "  def width(self):\n",
    "    return self._width\n",
    "\n",
    "  @property \n",
    "  def time_steps(self):\n",
    "    return self._time_steps\n",
    "\n",
    "  @time_steps.setter\n",
    "  def time_steps(self, _time_steps):\n",
    "    self._time_steps = _time_steps\n",
    "\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return self._observation\n",
    "\n",
    "  @observation.setter\n",
    "  def observation(self, _observation):\n",
    "    self._observation = _observation\n",
    "\n",
    "  def step(self):\n",
    "    self._time_steps += 1\n",
    "    self.observation = self.next_observation()\n",
    "    return self.observation\n",
    "\n",
    "  def plot_heat_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)  \n",
    "    heat_map_plot = ax.imshow(self.observation, cmap='hot')\n",
    "    return heat_map_plot\n",
    "\n",
    "  def reset(self):\n",
    "    self._time_steps = 0\n",
    "    self.observation = self.reset_observation()\n",
    "    seed = self.observation.copy()\n",
    "    for _ in range(30):\n",
    "      self.step()\n",
    "    return seed\n",
    "\n",
    "  def fire_in_range(self,margin=2):\n",
    "    burnX, burnY = np.where(self.observation==1)\n",
    "    return min(burnX)>=margin and min(burnY)>=margin and max(burnX)<=99-margin and max(burnY)<=99-margin\n",
    "\n",
    "  @abstractmethod\n",
    "  def next_observation(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def reset_observation(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 100\n",
    "width = 100\n",
    "D = 2\n",
    "K = 0.05\n",
    "\n",
    "def getNeighbors(point):\n",
    "    neighbors = []\n",
    "    min_x = max(0, point[1]-D)\n",
    "    max_x = min(99, point[1]+D)\n",
    "    min_y = max(0, point[0]-D)\n",
    "    max_y = min(99, point[0]+D)\n",
    "\n",
    "    for y in range(min_y, max_y): \n",
    "      for x in range(min_x, max_x):\n",
    "        neighbors.append((y, x))\n",
    "    return neighbors\n",
    "\n",
    "class ProbabilisticFireEnv(AbstactFireEnv):\n",
    "\n",
    "  def next_observation(self):\n",
    "\n",
    "    probability_map = np.zeros(shape=(height,width), dtype=float)\n",
    "    for row in range(self.height):\n",
    "      for col in range(self.width):\n",
    "        if self.observation[row,col] == 1:\n",
    "          if self.fuel[row, col] > 0:\n",
    "            self.fuel[row, col] -= 1\n",
    "          else:\n",
    "            self.observation[row,col] = 0\n",
    "\n",
    "        elif self.observation[row,col] == 0 and self.fuel[row, col] > 0:\n",
    "          neighboring_cells = getNeighbors((row, col))\n",
    "          pnm = 1\n",
    "          for neighboring_cell in neighboring_cells:\n",
    "            if self.observation[neighboring_cell] == 1:\n",
    "              dnmkl = np.array([a-b for a, b in zip(neighboring_cell, (row,col))])\n",
    "              norm = np.sum(dnmkl**2)\n",
    "              pnmkl0 = K/norm\n",
    "              pnmklw = K*(dnmkl @ self.wind)/norm \n",
    "              pnmkl  = max(0, min(1, (pnmkl0+pnmklw)))\n",
    "              pnm *= (1-pnmkl)\n",
    "          pmn = 1 - pnm\n",
    "          probability_map[row, col] = pmn\n",
    "\n",
    "    for row in range(self.height):\n",
    "      for col in range(self.width):\n",
    "        if probability_map[row, col] > random.random():\n",
    "          self.observation[row, col] = 1\n",
    "\n",
    "    return self.observation\n",
    "\n",
    "  def reset_observation(self):\n",
    "    center = [49, 49]\n",
    "    self.observation = np.zeros(shape=(self.height, self.width), dtype=int)\n",
    "    self.observation[center[0]-2:center[0]+2, center[1]-2:center[1]+2] = 1\n",
    "    self.fuel = np.random.randint(low=15, high=20, size=(self.height, self.width))\n",
    "    self.wind = np.random.uniform(low=-0.25, high=0.25, size=2)\n",
    "    return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_1 = 0.35\n",
    "LAMBDA_2 = 0.35\n",
    "LAMBDA_3 = 0.15\n",
    "LAMBDA_4 = 0.15 \n",
    "C = 50\n",
    "Cx = 50\n",
    "Cy = 50\n",
    "\n",
    "VELOCITY = 2\n",
    "GRAVITY  = 0.981\n",
    "MINRANGE = 15   # Minimium initial distance from wildfire seed\n",
    "MAXRANGE = 30   # Maximum initial distance from wildfire seed\n",
    "BANK_ANGLE_DELTA  = 5\n",
    "\n",
    "plane_marker = parse_path('M 11.640625 15.0625 L 9.304688 13.015625 L 9.300781 9.621094 L 15.125 11.511719 L 15.117188 10.109375 L 9.257812 5.535156 L 9.25 2.851562 L 9.25 1.296875 C 9.253906 1.019531 9.140625 0.777344 8.960938 0.585938 C 8.738281 0.324219 8.410156 0.15625 8.039062 0.160156 C 8.027344 0.160156 8.011719 0.164062 8 0.164062 C 7.988281 0.164062 7.972656 0.160156 7.960938 0.160156 C 7.589844 0.15625 7.257812 0.324219 7.035156 0.585938 C 6.859375 0.777344 6.746094 1.019531 6.746094 1.296875 L 6.746094 2.851562 L 6.742188 5.535156 L 0.882812 10.109375 L 0.875 11.511719 L 6.699219 9.621094 L 6.691406 13.011719 L 4.359375 15.0625 L 4.355469 15.761719 L 4.628906 15.695312 L 4.628906 15.839844 L 7.511719 14.992188 L 8 14.875 L 8.484375 14.992188 L 11.371094 15.839844 L 11.375 15.695312 L 11.644531 15.761719 Z M 11.640625 15.0625 ')\n",
    "plane_marker.vertices -= plane_marker.vertices.mean(axis=0)\n",
    "#plane_marker = plane_marker.transformed(matplotlib.transforms.Affine2D().rotate_deg(180))\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "  return math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "\n",
    "def shift_matrix(matrix, x, y, padding_value=0):\n",
    "  deltaX = Cx-x\n",
    "  deltaY = Cy-y\n",
    "\n",
    "\n",
    "  if deltaX==0 and deltaY==0:\n",
    "    return matrix\n",
    "\n",
    "  return shift(matrix, (deltaY, deltaX), cval = padding_value)\n",
    "  \n",
    "\n",
    "class Drone:\n",
    "\n",
    "  def __init__(self, _droneEnv, _dt, _dti):\n",
    "    self._bank_angle = 0\n",
    "    self._droneEnv = _droneEnv\n",
    "    self._trajectory = []\n",
    "    self._otherDrone = None\n",
    "    self.dt = _dt\n",
    "    self.dti = _dti\n",
    "\n",
    "  def reset(self):\n",
    "    radius = random.random()*(MAXRANGE-MINRANGE) + MINRANGE\n",
    "    angle = (random.random()-0.5)*2*np.pi\n",
    "    self._x = radius*np.cos(angle) + 50\n",
    "    self._y = radius*np.sin(angle) + 50\n",
    "    self._bank_angle = 0\n",
    "    self._trajectory = [(self.x, self.y)]\n",
    "    self._heading_angle = (random.random()-0.5)*2*np.pi\n",
    "\n",
    "  @property\n",
    "  def otherDrone(self):\n",
    "    return self._otherDrone\n",
    "\n",
    "  @otherDrone.setter\n",
    "  def otherDrone(self, _otherDrone):\n",
    "    self._otherDrone = _otherDrone\n",
    "\n",
    "  @property\n",
    "  def trajectory(self):\n",
    "    return self._trajectory\n",
    "\n",
    "  @trajectory.setter\n",
    "  def trajectory(self, _trajectory):\n",
    "    self._trajectory = _trajectory\n",
    "\n",
    "  @property\n",
    "  def x(self):\n",
    "    return self._x\n",
    "\n",
    "  @x.setter\n",
    "  def x(self, _x):\n",
    "    self._x = _x\n",
    "\n",
    "  @property\n",
    "  def y(self):\n",
    "    return self._y\n",
    "\n",
    "  @y.setter\n",
    "  def y(self, _y):\n",
    "    self._y = _y\n",
    "\n",
    "\n",
    "  @property\n",
    "  def bank_angle(self):\n",
    "    return self._bank_angle\n",
    "\n",
    "  @bank_angle.setter\n",
    "  def bank_angle(self, _bank_angle):\n",
    "    self._bank_angle = _bank_angle\n",
    "\n",
    "  @property\n",
    "  def heading_angle(self):\n",
    "    return self._heading_angle\n",
    "\n",
    "  @heading_angle.setter\n",
    "  def heading_angle(self, _heading_angle):\n",
    "    self._heading_angle = _heading_angle\n",
    "  \n",
    "  @property\n",
    "  def rho(self):\n",
    "    return euclidean_distance(self.x, self.y, self.otherDrone.x, self.otherDrone.y)\n",
    "\n",
    "  @property\n",
    "  def theta(self):\n",
    "    _theta = np.arctan2((self.otherDrone.y-self.y),(self.otherDrone.x-self.x)) - self.heading_angle\n",
    "    \n",
    "    if (_theta > math.pi):\n",
    "      _theta -= 2*math.pi\n",
    "    elif (_theta<-math.pi):\n",
    "      _theta+= 2*math.pi\n",
    "\n",
    "    return _theta\n",
    "\n",
    "  @property\n",
    "  def psi(self):\n",
    "    _psi = self.otherDrone.heading_angle - self.heading_angle\n",
    "\n",
    "    if (_psi > math.pi):\n",
    "      _psi -= 2*math.pi\n",
    "    elif (_psi<-math.pi):\n",
    "      _psi += 2*math.pi\n",
    "\n",
    "    return _psi\n",
    "    \n",
    "  @property\n",
    "  def state(self):\n",
    "    return np.array([\n",
    "        self.bank_angle, \n",
    "        self.rho,\n",
    "        self.theta,\n",
    "        self.psi,\n",
    "        self.otherDrone.bank_angle\n",
    "    ])[np.newaxis,...]\n",
    "\n",
    "  @property\n",
    "  def belief_map(self):\n",
    "    return self._transform_map(self._droneEnv.belief_map_channel.copy())\n",
    "  \n",
    "  @property\n",
    "  def time_elasped_map(self):\n",
    "    return self._transform_map(self._droneEnv.time_map_channel.copy(), 250.0)/250.0\n",
    "\n",
    "  def _transform_map(self, map, padding_value=0):\n",
    "    #matrix = rotate(map, angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value)\n",
    "    #return matrix[int(self.y)-50:int(self.y)+50, int(self.x)-50:int(self.x)+50]\n",
    "    #return shift_matrix(rotate(map, angle=np.rad2deg(self.heading_angle), cval=padding_value), self.x, self.y, padding_value)\n",
    "    return rotate(shift_matrix(map, self.x, self.y, padding_value), angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value)\n",
    "    #return rotate(map, angle=np.rad2deg(self.heading_angle), reshape=False, cval=padding_value )\n",
    "  @property\n",
    "  def observation(self):\n",
    "    return np.stack((self.time_elasped_map, self.belief_map), axis=0)[np.newaxis,...]\n",
    "    \n",
    "  def step(self, input):\n",
    "\n",
    "    self.x +=  VELOCITY*math.cos(self.heading_angle)\n",
    "    self.y +=  VELOCITY*math.sin(self.heading_angle)\n",
    "    self.trajectory.append((self.x, self.y))  \n",
    "    self.heading_angle += GRAVITY*np.tan(self.bank_angle)/(VELOCITY)\n",
    "\n",
    "    if (self.heading_angle>np.pi):\n",
    "      self.heading_angle-=2*np.pi\n",
    "    elif (self.heading_angle<-math.pi):\n",
    "      self.heading_angle+=2*np.pi\n",
    "\n",
    "    action =  5.0*np.pi/180.0 if input==1 else -5.0*np.pi/180.0\n",
    "    self.bank_angle += action\n",
    "    \n",
    "\n",
    "    if self.bank_angle >  50.0*np.pi/180.0 or self.bank_angle < -50.0*np.pi/180.0:\n",
    "      self.bank_angle -= action\n",
    "\n",
    "  @property\n",
    "  def reward(self):\n",
    "\n",
    "    return self._reward1()+self._reward2()+self._reward3()+self._reward4()\n",
    "      \n",
    "\n",
    "  def _reward1(self):\n",
    "    \n",
    "    fire_points = np.argwhere(self._droneEnv.belief_map_channel == 1)\n",
    "\n",
    "    if len(fire_points) == 0:\n",
    "      return -LAMBDA_1*141.42\n",
    "    else:\n",
    "      euclidean_distances = [euclidean_distance(self.x, self.y, fire_point[1], fire_point[0]) for fire_point in fire_points]\n",
    "      return -LAMBDA_1*min(euclidean_distances) \n",
    "\n",
    "\n",
    "  def _reward2(self):\n",
    "\n",
    "    min_drone_y = int(max(self.y-self._droneEnv.scan_radius,0))\n",
    "    max_drone_y = int(min(self.y+self._droneEnv.scan_radius, height))\n",
    "\n",
    "    min_drone_x = int(max(self.x-self._droneEnv.scan_radius,0))\n",
    "    max_drone_x = int(min(self.x+self._droneEnv.scan_radius, width))\n",
    "    \n",
    "    radar = self._droneEnv.belief_map_channel[min_drone_y:max_drone_y,min_drone_x:max_drone_x]\n",
    "    return -LAMBDA_2*(np.count_nonzero(radar==0))\n",
    "\n",
    "  def _reward3(self):\n",
    "    return -LAMBDA_3*(np.deg2rad(self.bank_angle)**2)\n",
    "\n",
    "  def _reward4(self):\n",
    "    return -LAMBDA_4*math.exp(-self.rho/C)\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=width)\n",
    "    ax.axis(ymin=0, ymax=height)\n",
    "    time_elasped_plot = ax.imshow(self.time_elasped_map*250.0, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot \n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "\n",
    "    ax.axis(xmin=0, xmax=width)\n",
    "    ax.axis(ymin=0, ymax=height)\n",
    "    belief_map_plot = ax.imshow(self.belief_map, cmap='gray_r', vmin=0, vmax=1)  \n",
    "    return belief_map_plot\n",
    "\n",
    "\n",
    "class DronesEnv:\n",
    "  def __init__(self, _height, _width, _dt, _dti, _scan_radius=10):\n",
    "    self._drones = [Drone(self, _dt, _dti), Drone(self, _dt, _dti)]\n",
    "    self._drones[0].otherDrone = self._drones[1]\n",
    "    self._drones[1].otherDrone = self._drones[0]\n",
    "    self._height = _height \n",
    "    self._width  = _width\n",
    "    self._scan_radius = _scan_radius\n",
    "\n",
    "  @property \n",
    "  def scan_radius(self):\n",
    "    return self._scan_radius\n",
    "\n",
    "  @scan_radius.setter\n",
    "  def scan_radius(self, _scan_radius):\n",
    "    self._scan_radius = _scan_radius\n",
    "        \n",
    "  def reset(self, fireMap=np.nan):\n",
    "\n",
    "\n",
    "    self._drones[0].reset()\n",
    "    self._drones[1].reset()\n",
    "\n",
    "    self._belief_map_channel = fireMap.copy()\n",
    "    #self._belief_map_channel = np.zeros(shape=(self._height, self._width))\n",
    "    self._time_elapsed_channel = np.full(shape=(self._height, self._width), fill_value=250)\n",
    "\n",
    "    self._drone_scan(self._drones[0], fireMap)\n",
    "    self._drone_scan(self._drones[1], fireMap)\n",
    "\n",
    "\n",
    "    \n",
    "  def _drone_scan(self, drone, fireMap=np.nan):\n",
    "    \n",
    "    reward = 0\n",
    "\n",
    "    min_drone_y = int(max(drone.y-self._scan_radius,0))\n",
    "    max_drone_y = int(min(drone.y+self._scan_radius, self._height))\n",
    "\n",
    "    min_drone_x = int(max(drone.x-self._scan_radius,0))\n",
    "    max_drone_x = int(min(drone.x+self._scan_radius, self._width))\n",
    "\n",
    "    if fireMap.any() != np.nan:\n",
    "      for y in range(min_drone_y,  max_drone_y):\n",
    "        for x in range(min_drone_x,  max_drone_x):\n",
    "          if fireMap[y,x] == 1 and self._belief_map_channel[y,x] == 0:\n",
    "            reward += 1\n",
    "          self._belief_map_channel[y,x] = fireMap[y,x]\n",
    "            \n",
    "    mask = np.ones(shape=(self._height, self._width), dtype=bool)\n",
    "    mask[:] = False\n",
    "    mask[min_drone_y:max_drone_y, min_drone_x:max_drone_x] = True\n",
    "\n",
    "    self._time_elapsed_channel[mask] = 0\n",
    "\n",
    "    self._time_elapsed_channel[~mask & (self._time_elapsed_channel < 250)] += 1\n",
    "\n",
    "    return reward\n",
    "\n",
    "  @property \n",
    "  def belief_map_channel(self):\n",
    "    return self._belief_map_channel\n",
    "\n",
    "  @belief_map_channel.setter\n",
    "  def belief_map_channel(self, _belief_map_channel):\n",
    "    self._belief_map_channel = _belief_map_channel\n",
    "\n",
    "  @property \n",
    "  def time_map_channel(self):\n",
    "    return self._time_elapsed_channel\n",
    "\n",
    "  @time_map_channel.setter\n",
    "  def time_map_channel(self, _time_elapsed_channel):\n",
    "    self._time_elapsed_channel = _time_elapsed_channel\n",
    "\n",
    "  @property\n",
    "  def drones(self):\n",
    "    return self._drones\n",
    "\n",
    "  def step(self, input, fireMap):\n",
    "    self._drones[0].step(input[0])\n",
    "    self._drones[1].step(input[1])\n",
    "    reward1 = self._drone_scan(self._drones[0], fireMap)\n",
    "    reward2 = self._drone_scan(self._drones[1], fireMap)\n",
    "    return reward1, reward2\n",
    "\n",
    "  def plot_time_elapsed(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=width)\n",
    "    ax.axis(ymin=0, ymax=height)\n",
    "    time_elasped_plot = ax.imshow(self._time_elapsed_channel, cmap='gray', vmin=0, vmax=250)\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    cbar = plt.colorbar(time_elasped_plot, cax=cax)\n",
    "    return time_elasped_plot\n",
    "\n",
    "  def plot_belief_map(self, fig, ax):\n",
    "    ax.axis(xmin=0, xmax=width)\n",
    "    ax.axis(ymin=0, ymax=height)\n",
    "    belief_map_plot = ax.imshow(self._belief_map_channel, cmap='gray_r', vmin=0, vmax=1)\n",
    "    return belief_map_plot   \n",
    "\n",
    "  def plot_drones(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self.drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self.drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x1 = self._drones[0].x + np.cos(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "    y1 = self._drones[0].y + np.sin(np.deg2rad(-90) + self._drones[0].heading_angle) * heading_line\n",
    "\n",
    "    heading_line = np.array([0, 50])\n",
    "\n",
    "    x2 = self._drones[1].x + np.cos(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    y2 = self._drones[1].y + np.sin(np.deg2rad(-90) + self._drones[1].heading_angle) * heading_line\n",
    "    \n",
    "    ax.plot(x1, y1, '--')\n",
    "    ax.plot(x2, y2, '--')\n",
    "\n",
    "  def plot_trajectory(self, fig, ax):\n",
    "      \n",
    "    ax.axis(xmin=0, xmax=self._width)\n",
    "    ax.axis(ymin=0, ymax=self._height)\n",
    "    ax.set_aspect(1)\n",
    "    ax.grid()\n",
    "\n",
    "    plane_marker_1 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_1._transform = plane_marker_1.get_transform().rotate(self._drones[0].heading_angle)\n",
    "\n",
    "    plane_marker_2 = matplotlib.markers.MarkerStyle(marker=plane_marker)\n",
    "    plane_marker_2._transform = plane_marker_2.get_transform().rotate(self._drones[1].heading_angle)\n",
    "\n",
    "    ax.scatter(self.drones[0].x, self.drones[0].y, marker=plane_marker_1, s=30**2)\n",
    "\n",
    "    ax.scatter(self.drones[1].x, self.drones[1].y, marker=plane_marker_2, s=30**2)\n",
    "\n",
    "    x1, y1 = zip(*self.drones[0].trajectory)\n",
    "    x2, y2 = zip(*self.drones[1].trajectory)\n",
    "\n",
    "    ax.plot(x1, y1, '.')\n",
    "    ax.plot(x2, y2, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfireEnv = ProbabilisticFireEnv(height, width)\\nobservation = fireEnv.reset()\\nfor _ in range(25):\\n    observation = fireEnv.step()\\ndronesEnv = DronesEnv(height, width) \\ndronesEnv.reset(observation)\\n\\n\\nfig, ax = plt.subplots(1, 5, figsize=(24,12))\\nfireEnv.plot_heat_map(fig, ax[0])\\ndronesEnv.plot_drones(fig, ax[1])\\ndronesEnv.plot_belief_map(fig, ax[2])\\ndronesEnv.drones[0].plot_belief_map(fig, ax[3])\\ndronesEnv.drones[0].plot_time_elapsed(fig, ax[4])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "observation = fireEnv.reset()\n",
    "for _ in range(25):\n",
    "    observation = fireEnv.step()\n",
    "dronesEnv = DronesEnv(height, width) \n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(24,12))\n",
    "fireEnv.plot_heat_map(fig, ax[0])\n",
    "dronesEnv.plot_drones(fig, ax[1])\n",
    "dronesEnv.plot_belief_map(fig, ax[2])\n",
    "dronesEnv.drones[0].plot_belief_map(fig, ax[3])\n",
    "dronesEnv.drones[0].plot_time_elapsed(fig, ax[4])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('belief_map', 'state_vector', 'action', 'next_belief_map', 'next_state_vector', 'reward'))\n",
    "class ReplayMemory:\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.capicity = capacity\n",
    "    self.memory = deque([],maxlen=self.capicity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    \"\"\"Save a transition\"\"\"\n",
    "    self.memory.append(Transition(*args))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "  def _get_conv_out(self, shape):\n",
    "    o = self.conv(torch.zeros(1, *shape))\n",
    "    return int(np.prod(o.size()))\n",
    "\n",
    "  def __init__(self, channels, height, width, outputs):\n",
    "    super(DQN, self).__init__()\n",
    "\n",
    "    self.fc1  = nn.Sequential(\n",
    "      nn.Linear(5, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 100),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "      nn.Conv2d(2, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(64, 64, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "  \n",
    "    conv_out_size = self._get_conv_out((channels,height,width))\n",
    "\n",
    "    self.fc2 = nn.Sequential(\n",
    "      nn.Linear(conv_out_size, 500),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(500, 100),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc3 = nn.Sequential(\n",
    "      nn.Linear(200, 200),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(200, 2),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, belief_map, state_vector):\n",
    "    fc1_out = self.fc1(state_vector)\n",
    "    conv_out = torch.flatten(self.conv(belief_map),1)\n",
    "    fc2_out = self.fc2(conv_out)\n",
    "    fc3_out = self.fc3(torch.cat((fc1_out, fc2_out), dim=1))\n",
    "    return fc3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.10\n",
    "EPS_DECAY = 300000\n",
    "INIT_SIZE = 20000\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "policy_file_path = f'./policy_weights.pt'\n",
    "target_file_path = f'./target_weights.pt'\n",
    "\n",
    "#policy_net.load_state_dict(torch.load(policy_file_path))\n",
    "#target_net.load_state_dict(torch.load('target_weights.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = ReplayMemory(70000)\n",
    "#memory.load()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(belief_map, state_vector, steps):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      output = policy_net(belief_map, state_vector).max(1)[1].view(1, 1)\n",
    "      return output\n",
    "  else:\n",
    "    return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "    \n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(belief_map_batch, state_vector_batch).gather(1, action_batch)\n",
    "    next_state_values = target_net(next_belief_map, next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights.pt'\n",
    "        target_file_path = f'./target_weights.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episodes completed\n",
      "loss None\n",
      "steps done 2070\n",
      "10 episodes completed\n",
      "loss None\n",
      "steps done 5740\n",
      "15 episodes completed\n",
      "loss None\n",
      "steps done 10100\n",
      "20 episodes completed\n",
      "loss None\n",
      "steps done 14340\n",
      "25 episodes completed\n",
      "loss None\n",
      "steps done 18380\n",
      "30 episodes completed\n",
      "loss 0.416800320148468\n",
      "steps done 22090\n",
      "35 episodes completed\n",
      "loss 0.5099551677703857\n",
      "steps done 26040\n",
      "40 episodes completed\n",
      "loss 0.6905213594436646\n",
      "steps done 29660\n",
      "update target\n",
      "45 episodes completed\n",
      "loss 0.4019929766654968\n",
      "steps done 32970\n",
      "50 episodes completed\n",
      "loss 0.5993580222129822\n",
      "steps done 37140\n",
      "update target\n",
      "55 episodes completed\n",
      "loss 0.8442859649658203\n",
      "steps done 40660\n",
      "60 episodes completed\n",
      "loss 0.6077636480331421\n",
      "steps done 45000\n",
      "65 episodes completed\n",
      "loss 0.5566778779029846\n",
      "steps done 48340\n",
      "update target\n",
      "70 episodes completed\n",
      "loss 0.9340360164642334\n",
      "steps done 52040\n",
      "75 episodes completed\n",
      "loss 0.9874288439750671\n",
      "steps done 56260\n",
      "80 episodes completed\n",
      "loss 0.6179695129394531\n",
      "steps done 59850\n",
      "update target\n",
      "85 episodes completed\n",
      "loss 0.4655192792415619\n",
      "steps done 63530\n",
      "90 episodes completed\n",
      "loss 0.6612869501113892\n",
      "steps done 67580\n",
      "update target\n",
      "95 episodes completed\n",
      "loss 0.9022221565246582\n",
      "steps done 71320\n",
      "100 episodes completed\n",
      "loss 0.6699409484863281\n",
      "steps done 75270\n",
      "105 episodes completed\n",
      "loss 0.8338444828987122\n",
      "steps done 78700\n",
      "update target\n",
      "110 episodes completed\n",
      "loss 1.5119208097457886\n",
      "steps done 82090\n",
      "115 episodes completed\n",
      "loss 1.4323467016220093\n",
      "steps done 85340\n",
      "120 episodes completed\n",
      "loss 1.1618860960006714\n",
      "steps done 88690\n",
      "update target\n",
      "125 episodes completed\n",
      "loss 0.6580636501312256\n",
      "steps done 92290\n",
      "130 episodes completed\n",
      "loss 1.3749747276306152\n",
      "steps done 96730\n",
      "update target\n",
      "135 episodes completed\n",
      "loss 1.1654248237609863\n",
      "steps done 101050\n",
      "140 episodes completed\n",
      "loss 1.5597524642944336\n",
      "steps done 105050\n",
      "145 episodes completed\n",
      "loss 0.8523249626159668\n",
      "steps done 108720\n",
      "update target\n",
      "150 episodes completed\n",
      "loss 1.7124905586242676\n",
      "steps done 112670\n",
      "155 episodes completed\n",
      "loss 1.75831937789917\n",
      "steps done 117050\n",
      "update target\n",
      "160 episodes completed\n",
      "loss 2.1386775970458984\n",
      "steps done 120910\n",
      "165 episodes completed\n",
      "loss 1.0702426433563232\n",
      "steps done 124380\n",
      "170 episodes completed\n",
      "loss 1.4177699089050293\n",
      "steps done 128380\n",
      "update target\n",
      "175 episodes completed\n",
      "loss 2.2813944816589355\n",
      "steps done 132180\n",
      "180 episodes completed\n",
      "loss 1.7058582305908203\n",
      "steps done 135790\n",
      "update target\n",
      "185 episodes completed\n",
      "loss 1.6952004432678223\n",
      "steps done 140740\n",
      "190 episodes completed\n",
      "loss 1.3508172035217285\n",
      "steps done 144600\n",
      "195 episodes completed\n",
      "loss 1.1458717584609985\n",
      "steps done 148530\n",
      "update target\n",
      "200 episodes completed\n",
      "loss 1.7157877683639526\n",
      "steps done 153230\n",
      "205 episodes completed\n",
      "loss 2.03774356842041\n",
      "steps done 157240\n",
      "update target\n",
      "210 episodes completed\n",
      "loss 2.882256507873535\n",
      "steps done 160440\n",
      "215 episodes completed\n",
      "loss 2.159501552581787\n",
      "steps done 164140\n",
      "220 episodes completed\n",
      "loss 2.4168262481689453\n",
      "steps done 167540\n",
      "update target\n",
      "225 episodes completed\n",
      "loss 1.7839078903198242\n",
      "steps done 171560\n",
      "230 episodes completed\n",
      "loss 1.6724278926849365\n",
      "steps done 175970\n",
      "235 episodes completed\n",
      "loss 1.3924907445907593\n",
      "steps done 179270\n",
      "update target\n",
      "240 episodes completed\n",
      "loss 1.4882638454437256\n",
      "steps done 182380\n",
      "245 episodes completed\n",
      "loss 1.3783341646194458\n",
      "steps done 186650\n",
      "update target\n",
      "250 episodes completed\n",
      "loss 1.622316598892212\n",
      "steps done 190190\n",
      "255 episodes completed\n",
      "loss 1.6559574604034424\n",
      "steps done 194020\n",
      "260 episodes completed\n",
      "loss 1.9847445487976074\n",
      "steps done 197410\n",
      "update target\n",
      "265 episodes completed\n",
      "loss 2.202755928039551\n",
      "steps done 200930\n",
      "270 episodes completed\n",
      "loss 2.651756763458252\n",
      "steps done 204480\n",
      "275 episodes completed\n",
      "loss 2.0001564025878906\n",
      "steps done 208440\n",
      "update target\n",
      "280 episodes completed\n",
      "loss 2.984083652496338\n",
      "steps done 212740\n",
      "285 episodes completed\n",
      "loss 3.0319766998291016\n",
      "steps done 216450\n",
      "update target\n",
      "290 episodes completed\n",
      "loss 1.6632766723632812\n",
      "steps done 220020\n",
      "295 episodes completed\n",
      "loss 2.8417651653289795\n",
      "steps done 223750\n",
      "300 episodes completed\n",
      "loss 2.201599597930908\n",
      "steps done 227160\n",
      "update target\n",
      "305 episodes completed\n",
      "loss 2.536261558532715\n",
      "steps done 230710\n",
      "310 episodes completed\n",
      "loss 2.117605686187744\n",
      "steps done 233770\n",
      "315 episodes completed\n",
      "loss 3.047667980194092\n",
      "steps done 238010\n",
      "update target\n",
      "320 episodes completed\n",
      "loss 2.094900608062744\n",
      "steps done 241290\n",
      "325 episodes completed\n",
      "loss 3.103227138519287\n",
      "steps done 245170\n",
      "330 episodes completed\n",
      "loss 3.685971736907959\n",
      "steps done 249210\n",
      "update target\n",
      "335 episodes completed\n",
      "loss 2.6296191215515137\n",
      "steps done 252860\n",
      "340 episodes completed\n",
      "loss 3.393251895904541\n",
      "steps done 256930\n",
      "update target\n",
      "345 episodes completed\n",
      "loss 3.6232619285583496\n",
      "steps done 260230\n",
      "350 episodes completed\n",
      "loss 2.3606886863708496\n",
      "steps done 264230\n",
      "355 episodes completed\n",
      "loss 3.421996593475342\n",
      "steps done 268760\n",
      "update target\n",
      "360 episodes completed\n",
      "loss 2.9655985832214355\n",
      "steps done 273420\n",
      "365 episodes completed\n",
      "loss 3.526641368865967\n",
      "steps done 277500\n",
      "update target\n",
      "370 episodes completed\n",
      "loss 3.5855226516723633\n",
      "steps done 280960\n",
      "375 episodes completed\n",
      "loss 3.436401605606079\n",
      "steps done 284850\n",
      "380 episodes completed\n",
      "loss 2.5725605487823486\n",
      "steps done 288790\n",
      "update target\n",
      "385 episodes completed\n",
      "loss 2.9739632606506348\n",
      "steps done 292420\n",
      "390 episodes completed\n",
      "loss 3.414626121520996\n",
      "steps done 295720\n",
      "update target\n",
      "395 episodes completed\n",
      "loss 3.3736190795898438\n",
      "steps done 300040\n",
      "400 episodes completed\n",
      "loss 2.7969250679016113\n",
      "steps done 303890\n",
      "405 episodes completed\n",
      "loss 2.7811636924743652\n",
      "steps done 308190\n",
      "update target\n",
      "410 episodes completed\n",
      "loss 2.7633960247039795\n",
      "steps done 311780\n",
      "415 episodes completed\n",
      "loss 3.740509271621704\n",
      "steps done 315700\n",
      "420 episodes completed\n",
      "loss 3.7387514114379883\n",
      "steps done 319640\n",
      "update target\n",
      "425 episodes completed\n",
      "loss 3.4181101322174072\n",
      "steps done 323330\n",
      "430 episodes completed\n",
      "loss 3.4518957138061523\n",
      "steps done 326450\n",
      "update target\n",
      "435 episodes completed\n",
      "loss 3.206165313720703\n",
      "steps done 331040\n",
      "440 episodes completed\n",
      "loss 4.691653728485107\n",
      "steps done 335060\n",
      "445 episodes completed\n",
      "loss 2.8897199630737305\n",
      "steps done 338760\n",
      "update target\n",
      "450 episodes completed\n",
      "loss 3.147798538208008\n",
      "steps done 342490\n",
      "455 episodes completed\n",
      "loss 4.221120834350586\n",
      "steps done 346610\n",
      "update target\n",
      "460 episodes completed\n",
      "loss 4.5055694580078125\n",
      "steps done 350010\n",
      "465 episodes completed\n",
      "loss 5.071967124938965\n",
      "steps done 353300\n",
      "470 episodes completed\n",
      "loss 3.592928886413574\n",
      "steps done 356820\n",
      "update target\n",
      "475 episodes completed\n",
      "loss 4.799070835113525\n",
      "steps done 360010\n",
      "480 episodes completed\n",
      "loss 4.122064590454102\n",
      "steps done 363780\n",
      "485 episodes completed\n",
      "loss 3.9587676525115967\n",
      "steps done 367420\n",
      "update target\n",
      "490 episodes completed\n",
      "loss 4.685276031494141\n",
      "steps done 371020\n",
      "495 episodes completed\n",
      "loss 4.0600996017456055\n",
      "steps done 374850\n",
      "500 episodes completed\n",
      "loss 5.2869954109191895\n",
      "steps done 378890\n",
      "update target\n",
      "505 episodes completed\n",
      "loss 3.9030280113220215\n",
      "steps done 382660\n",
      "510 episodes completed\n",
      "loss 4.919381141662598\n",
      "steps done 386630\n",
      "update target\n",
      "515 episodes completed\n",
      "loss 5.286300182342529\n",
      "steps done 390530\n",
      "520 episodes completed\n",
      "loss 3.48299241065979\n",
      "steps done 394140\n",
      "525 episodes completed\n",
      "loss 4.145345211029053\n",
      "steps done 398080\n",
      "update target\n",
      "530 episodes completed\n",
      "loss 6.665554046630859\n",
      "steps done 401710\n",
      "535 episodes completed\n",
      "loss 4.863513946533203\n",
      "steps done 405320\n",
      "540 episodes completed\n",
      "loss 4.129502296447754\n",
      "steps done 409310\n",
      "update target\n",
      "545 episodes completed\n",
      "loss 5.598413467407227\n",
      "steps done 413080\n",
      "550 episodes completed\n",
      "loss 4.176517486572266\n",
      "steps done 416570\n",
      "555 episodes completed\n",
      "loss 4.307526588439941\n",
      "steps done 419810\n",
      "update target\n",
      "560 episodes completed\n",
      "loss 4.701331615447998\n",
      "steps done 424050\n",
      "565 episodes completed\n",
      "loss 4.747384548187256\n",
      "steps done 428180\n",
      "update target\n",
      "570 episodes completed\n",
      "loss 4.539585113525391\n",
      "steps done 431770\n",
      "575 episodes completed\n",
      "loss 5.339590072631836\n",
      "steps done 435300\n",
      "580 episodes completed\n",
      "loss 5.632081985473633\n",
      "steps done 439720\n",
      "update target\n",
      "585 episodes completed\n",
      "loss 4.7251877784729\n",
      "steps done 443710\n",
      "590 episodes completed\n",
      "loss 3.466628074645996\n",
      "steps done 447510\n",
      "update target\n",
      "595 episodes completed\n",
      "loss 4.306459426879883\n",
      "steps done 451280\n",
      "600 episodes completed\n",
      "loss 5.3107404708862305\n",
      "steps done 455420\n",
      "605 episodes completed\n",
      "loss 5.74619197845459\n",
      "steps done 458830\n",
      "update target\n",
      "610 episodes completed\n",
      "loss 4.403507232666016\n",
      "steps done 461940\n",
      "615 episodes completed\n",
      "loss 4.85399055480957\n",
      "steps done 465340\n",
      "620 episodes completed\n",
      "loss 5.349699020385742\n",
      "steps done 469220\n",
      "update target\n",
      "625 episodes completed\n",
      "loss 5.340588569641113\n",
      "steps done 473950\n",
      "630 episodes completed\n",
      "loss 4.299092769622803\n",
      "steps done 477720\n",
      "update target\n",
      "635 episodes completed\n",
      "loss 5.084580898284912\n",
      "steps done 481610\n",
      "640 episodes completed\n",
      "loss 5.515045166015625\n",
      "steps done 485460\n",
      "645 episodes completed\n",
      "loss 4.484255790710449\n",
      "steps done 489430\n",
      "update target\n",
      "650 episodes completed\n",
      "loss 4.830146789550781\n",
      "steps done 493100\n",
      "655 episodes completed\n",
      "loss 3.6388919353485107\n",
      "steps done 496980\n",
      "update target\n",
      "660 episodes completed\n",
      "loss 5.980371952056885\n",
      "steps done 500590\n",
      "665 episodes completed\n",
      "loss 5.049415588378906\n",
      "steps done 504560\n",
      "670 episodes completed\n",
      "loss 5.159466743469238\n",
      "steps done 508270\n",
      "update target\n",
      "675 episodes completed\n",
      "loss 4.2074785232543945\n",
      "steps done 511630\n",
      "680 episodes completed\n",
      "loss 5.407467842102051\n",
      "steps done 515600\n",
      "685 episodes completed\n",
      "loss 5.020874500274658\n",
      "steps done 519200\n",
      "update target\n",
      "690 episodes completed\n",
      "loss 6.7310943603515625\n",
      "steps done 522820\n",
      "695 episodes completed\n",
      "loss 5.668339252471924\n",
      "steps done 527310\n",
      "update target\n",
      "700 episodes completed\n",
      "loss 5.685532093048096\n",
      "steps done 531160\n",
      "705 episodes completed\n",
      "loss 5.747835636138916\n",
      "steps done 535100\n",
      "710 episodes completed\n",
      "loss 5.444449424743652\n",
      "steps done 538690\n",
      "update target\n",
      "715 episodes completed\n",
      "loss 5.98076868057251\n",
      "steps done 542100\n",
      "720 episodes completed\n",
      "loss 5.3429646492004395\n",
      "steps done 545660\n",
      "725 episodes completed\n",
      "loss 6.398285865783691\n",
      "steps done 548750\n",
      "update target\n",
      "730 episodes completed\n",
      "loss 7.352637767791748\n",
      "steps done 552710\n",
      "735 episodes completed\n",
      "loss 7.30955696105957\n",
      "steps done 556310\n",
      "740 episodes completed\n",
      "loss 7.710259437561035\n",
      "steps done 559750\n",
      "update target\n",
      "745 episodes completed\n",
      "loss 5.465515613555908\n",
      "steps done 562950\n",
      "750 episodes completed\n",
      "loss 6.2656426429748535\n",
      "steps done 567380\n",
      "update target\n",
      "755 episodes completed\n",
      "loss 6.769174098968506\n",
      "steps done 571190\n",
      "760 episodes completed\n",
      "loss 7.664669036865234\n",
      "steps done 574760\n",
      "765 episodes completed\n",
      "loss 5.83119010925293\n",
      "steps done 579580\n",
      "update target\n",
      "770 episodes completed\n",
      "loss 5.973347187042236\n",
      "steps done 584070\n",
      "775 episodes completed\n",
      "loss 6.163992881774902\n",
      "steps done 588000\n",
      "update target\n",
      "780 episodes completed\n",
      "loss 6.877330780029297\n",
      "steps done 591640\n",
      "785 episodes completed\n",
      "loss 6.901975154876709\n",
      "steps done 595030\n",
      "790 episodes completed\n",
      "loss 6.187233924865723\n",
      "steps done 598040\n",
      "update target\n",
      "795 episodes completed\n",
      "loss 5.074288368225098\n",
      "steps done 601760\n",
      "800 episodes completed\n",
      "loss 5.546127796173096\n",
      "steps done 605870\n",
      "805 episodes completed\n",
      "loss 5.228025913238525\n",
      "steps done 609590\n",
      "update target\n",
      "810 episodes completed\n",
      "loss 8.271917343139648\n",
      "steps done 613180\n",
      "815 episodes completed\n",
      "loss 6.330388069152832\n",
      "steps done 617250\n",
      "update target\n",
      "820 episodes completed\n",
      "loss 5.286854267120361\n",
      "steps done 621280\n",
      "825 episodes completed\n",
      "loss 5.650961875915527\n",
      "steps done 625050\n",
      "830 episodes completed\n",
      "loss 4.4035964012146\n",
      "steps done 628790\n",
      "update target\n",
      "835 episodes completed\n",
      "loss 6.527239799499512\n",
      "steps done 633180\n",
      "840 episodes completed\n",
      "loss 6.615338325500488\n",
      "steps done 637610\n",
      "update target\n",
      "845 episodes completed\n",
      "loss 6.425317287445068\n",
      "steps done 641400\n",
      "850 episodes completed\n",
      "loss 6.167226314544678\n",
      "steps done 645830\n",
      "855 episodes completed\n",
      "loss 6.641658306121826\n",
      "steps done 649680\n",
      "update target\n",
      "860 episodes completed\n",
      "loss 6.765791893005371\n",
      "steps done 653240\n",
      "865 episodes completed\n",
      "loss 5.835256576538086\n",
      "steps done 656880\n",
      "update target\n",
      "870 episodes completed\n",
      "loss 5.588814735412598\n",
      "steps done 660780\n",
      "875 episodes completed\n",
      "loss 5.006743431091309\n",
      "steps done 664710\n",
      "880 episodes completed\n",
      "loss 5.0622687339782715\n",
      "steps done 668040\n",
      "update target\n",
      "885 episodes completed\n",
      "loss 5.635869979858398\n",
      "steps done 671560\n",
      "890 episodes completed\n",
      "loss 6.454589366912842\n",
      "steps done 675650\n",
      "895 episodes completed\n",
      "loss 4.914745330810547\n",
      "steps done 679710\n",
      "update target\n",
      "900 episodes completed\n",
      "loss 5.192465782165527\n",
      "steps done 683750\n",
      "905 episodes completed\n",
      "loss 4.720314025878906\n",
      "steps done 687670\n",
      "update target\n",
      "910 episodes completed\n",
      "loss 5.961001396179199\n",
      "steps done 692060\n",
      "915 episodes completed\n",
      "loss 4.561405181884766\n",
      "steps done 696150\n",
      "920 episodes completed\n",
      "loss 7.365084171295166\n",
      "steps done 699570\n",
      "update target\n",
      "925 episodes completed\n",
      "loss 7.674468994140625\n",
      "steps done 703710\n",
      "930 episodes completed\n",
      "loss 7.965915679931641\n",
      "steps done 707050\n",
      "update target\n",
      "935 episodes completed\n",
      "loss 7.526020050048828\n",
      "steps done 710830\n",
      "940 episodes completed\n",
      "loss 6.311504364013672\n",
      "steps done 714720\n",
      "945 episodes completed\n",
      "loss 5.831770420074463\n",
      "steps done 718370\n",
      "update target\n",
      "950 episodes completed\n",
      "loss 6.489923000335693\n",
      "steps done 721720\n",
      "955 episodes completed\n",
      "loss 8.451041221618652\n",
      "steps done 725410\n",
      "960 episodes completed\n",
      "loss 5.505675315856934\n",
      "steps done 729480\n",
      "update target\n",
      "965 episodes completed\n",
      "loss 6.631112098693848\n",
      "steps done 732730\n",
      "970 episodes completed\n",
      "loss 6.47326135635376\n",
      "steps done 737080\n",
      "update target\n",
      "975 episodes completed\n",
      "loss 7.233850955963135\n",
      "steps done 740930\n",
      "980 episodes completed\n",
      "loss 6.305643081665039\n",
      "steps done 744820\n",
      "985 episodes completed\n",
      "loss 6.069947719573975\n",
      "steps done 748580\n",
      "update target\n",
      "990 episodes completed\n",
      "loss 5.962411880493164\n",
      "steps done 751850\n",
      "995 episodes completed\n",
      "loss 7.0741472244262695\n",
      "steps done 755980\n",
      "1000 episodes completed\n",
      "loss 6.478638648986816\n",
      "steps done 759340\n",
      "update target\n",
      "1005 episodes completed\n",
      "loss 6.2474775314331055\n",
      "steps done 763550\n",
      "1010 episodes completed\n",
      "loss 5.970794677734375\n",
      "steps done 767590\n",
      "update target\n",
      "1015 episodes completed\n",
      "loss 5.832557678222656\n",
      "steps done 771620\n",
      "1020 episodes completed\n",
      "loss 6.062577247619629\n",
      "steps done 775910\n",
      "1025 episodes completed\n",
      "loss 6.70681095123291\n",
      "steps done 779290\n",
      "update target\n",
      "1030 episodes completed\n",
      "loss 6.232006072998047\n",
      "steps done 783300\n",
      "1035 episodes completed\n",
      "loss 5.850393295288086\n",
      "steps done 786770\n",
      "update target\n",
      "1040 episodes completed\n",
      "loss 6.729588508605957\n",
      "steps done 790340\n",
      "1045 episodes completed\n",
      "loss 5.15625\n",
      "steps done 794560\n",
      "1050 episodes completed\n",
      "loss 6.105312824249268\n",
      "steps done 797690\n",
      "update target\n",
      "1055 episodes completed\n",
      "loss 5.994880676269531\n",
      "steps done 800860\n",
      "1060 episodes completed\n",
      "loss 5.893401145935059\n",
      "steps done 804030\n",
      "1065 episodes completed\n",
      "loss 6.678810119628906\n",
      "steps done 807670\n",
      "update target\n",
      "1070 episodes completed\n",
      "loss 5.423521041870117\n",
      "steps done 811180\n",
      "1075 episodes completed\n",
      "loss 5.627904891967773\n",
      "steps done 815380\n",
      "1080 episodes completed\n",
      "loss 6.604527950286865\n",
      "steps done 819270\n",
      "update target\n",
      "1085 episodes completed\n",
      "loss 6.138151168823242\n",
      "steps done 823070\n",
      "1090 episodes completed\n",
      "loss 5.375782012939453\n",
      "steps done 826490\n",
      "update target\n",
      "1095 episodes completed\n",
      "loss 7.423421382904053\n",
      "steps done 831370\n",
      "1100 episodes completed\n",
      "loss 5.652058124542236\n",
      "steps done 835340\n",
      "1105 episodes completed\n",
      "loss 5.8190107345581055\n",
      "steps done 838850\n",
      "update target\n",
      "1110 episodes completed\n",
      "loss 6.988955497741699\n",
      "steps done 842880\n",
      "1115 episodes completed\n",
      "loss 4.871541976928711\n",
      "steps done 846350\n",
      "update target\n",
      "1120 episodes completed\n",
      "loss 5.870889663696289\n",
      "steps done 850280\n",
      "1125 episodes completed\n",
      "loss 5.875674247741699\n",
      "steps done 854010\n",
      "1130 episodes completed\n",
      "loss 6.071558952331543\n",
      "steps done 857740\n",
      "update target\n",
      "1135 episodes completed\n",
      "loss 6.823728561401367\n",
      "steps done 862430\n",
      "1140 episodes completed\n",
      "loss 5.659261703491211\n",
      "steps done 866560\n",
      "update target\n",
      "1145 episodes completed\n",
      "loss 5.587825775146484\n",
      "steps done 870430\n",
      "1150 episodes completed\n",
      "loss 5.839871406555176\n",
      "steps done 874280\n",
      "1155 episodes completed\n",
      "loss 6.153609275817871\n",
      "steps done 879080\n",
      "update target\n",
      "1160 episodes completed\n",
      "loss 5.779043197631836\n",
      "steps done 882680\n",
      "1165 episodes completed\n",
      "loss 5.30837869644165\n",
      "steps done 887090\n",
      "update target\n",
      "1170 episodes completed\n",
      "loss 6.280831336975098\n",
      "steps done 890460\n",
      "1175 episodes completed\n",
      "loss 5.276983261108398\n",
      "steps done 894050\n",
      "1180 episodes completed\n",
      "loss 5.595688343048096\n",
      "steps done 897650\n",
      "update target\n",
      "1185 episodes completed\n",
      "loss 6.9276862144470215\n",
      "steps done 901640\n",
      "1190 episodes completed\n",
      "loss 4.773324966430664\n",
      "steps done 905400\n",
      "1195 episodes completed\n",
      "loss 5.563682556152344\n",
      "steps done 909050\n",
      "update target\n",
      "1200 episodes completed\n",
      "loss 5.513548374176025\n",
      "steps done 912920\n",
      "1205 episodes completed\n",
      "loss 5.673167705535889\n",
      "steps done 916380\n",
      "update target\n",
      "1210 episodes completed\n",
      "loss 5.731171607971191\n",
      "steps done 920910\n",
      "1215 episodes completed\n",
      "loss 6.7070698738098145\n",
      "steps done 924320\n",
      "1220 episodes completed\n",
      "loss 5.399001598358154\n",
      "steps done 928410\n",
      "update target\n",
      "1225 episodes completed\n",
      "loss 6.183439254760742\n",
      "steps done 932190\n",
      "1230 episodes completed\n",
      "loss 6.115437030792236\n",
      "steps done 936730\n",
      "update target\n",
      "1235 episodes completed\n",
      "loss 5.5381622314453125\n",
      "steps done 940200\n",
      "1240 episodes completed\n",
      "loss 5.461451053619385\n",
      "steps done 944120\n",
      "1245 episodes completed\n",
      "loss 5.848186492919922\n",
      "steps done 948140\n",
      "update target\n",
      "1250 episodes completed\n",
      "loss 5.236142158508301\n",
      "steps done 952420\n",
      "1255 episodes completed\n",
      "loss 4.972313404083252\n",
      "steps done 955910\n",
      "1260 episodes completed\n",
      "loss 5.590888500213623\n",
      "steps done 959080\n",
      "update target\n",
      "1265 episodes completed\n",
      "loss 6.1728973388671875\n",
      "steps done 962690\n",
      "1270 episodes completed\n",
      "loss 5.123586654663086\n",
      "steps done 967080\n",
      "update target\n",
      "1275 episodes completed\n",
      "loss 6.406310081481934\n",
      "steps done 971080\n",
      "1280 episodes completed\n",
      "loss 4.477304458618164\n",
      "steps done 974310\n",
      "1285 episodes completed\n",
      "loss 5.418391704559326\n",
      "steps done 978120\n",
      "update target\n",
      "1290 episodes completed\n",
      "loss 5.713558197021484\n",
      "steps done 982060\n",
      "1295 episodes completed\n",
      "loss 5.613012790679932\n",
      "steps done 985740\n",
      "1300 episodes completed\n",
      "loss 5.768969535827637\n",
      "steps done 989270\n",
      "update target\n",
      "1305 episodes completed\n",
      "loss 5.247502326965332\n",
      "steps done 992820\n",
      "1310 episodes completed\n",
      "loss 5.692113399505615\n",
      "steps done 997590\n",
      "update target\n",
      "1315 episodes completed\n",
      "loss 5.682701110839844\n",
      "steps done 1001370\n",
      "1320 episodes completed\n",
      "loss 4.507255554199219\n",
      "steps done 1004820\n",
      "1325 episodes completed\n",
      "loss 4.706070423126221\n",
      "steps done 1008970\n",
      "update target\n",
      "1330 episodes completed\n",
      "loss 4.758526802062988\n",
      "steps done 1013240\n",
      "1335 episodes completed\n",
      "loss 4.655301094055176\n",
      "steps done 1016530\n",
      "update target\n",
      "1340 episodes completed\n",
      "loss 5.014604568481445\n",
      "steps done 1020190\n",
      "1345 episodes completed\n",
      "loss 4.72238302230835\n",
      "steps done 1023490\n",
      "1350 episodes completed\n",
      "loss 5.055915832519531\n",
      "steps done 1027130\n",
      "update target\n",
      "1355 episodes completed\n",
      "loss 5.508073806762695\n",
      "steps done 1030360\n",
      "1360 episodes completed\n",
      "loss 4.880436897277832\n",
      "steps done 1033780\n",
      "1365 episodes completed\n",
      "loss 5.550661563873291\n",
      "steps done 1037650\n",
      "update target\n",
      "1370 episodes completed\n",
      "loss 4.331892490386963\n",
      "steps done 1041090\n",
      "1375 episodes completed\n",
      "loss 4.4424004554748535\n",
      "steps done 1044730\n",
      "1380 episodes completed\n",
      "loss 5.315474987030029\n",
      "steps done 1048390\n",
      "update target\n",
      "1385 episodes completed\n",
      "loss 5.452494144439697\n",
      "steps done 1051820\n",
      "1390 episodes completed\n",
      "loss 5.316928863525391\n",
      "steps done 1055260\n",
      "1395 episodes completed\n",
      "loss 5.35951042175293\n",
      "steps done 1059270\n",
      "update target\n",
      "1400 episodes completed\n",
      "loss 4.333025932312012\n",
      "steps done 1062490\n",
      "1405 episodes completed\n",
      "loss 4.198411464691162\n",
      "steps done 1066500\n",
      "update target\n",
      "1410 episodes completed\n",
      "loss 4.755861759185791\n",
      "steps done 1070250\n",
      "1415 episodes completed\n",
      "loss 4.540679931640625\n",
      "steps done 1074910\n",
      "1420 episodes completed\n",
      "loss 4.0485382080078125\n",
      "steps done 1078350\n",
      "update target\n",
      "1425 episodes completed\n",
      "loss 5.075135707855225\n",
      "steps done 1082460\n",
      "1430 episodes completed\n",
      "loss 4.673412322998047\n",
      "steps done 1086030\n",
      "1435 episodes completed\n",
      "loss 4.031095504760742\n",
      "steps done 1089240\n",
      "update target\n",
      "1440 episodes completed\n",
      "loss 4.708191871643066\n",
      "steps done 1093030\n",
      "1445 episodes completed\n",
      "loss 4.61969518661499\n",
      "steps done 1096710\n",
      "update target\n",
      "1450 episodes completed\n",
      "loss 4.494635581970215\n",
      "steps done 1100390\n",
      "1455 episodes completed\n",
      "loss 5.265292644500732\n",
      "steps done 1103830\n",
      "1460 episodes completed\n",
      "loss 4.685232162475586\n",
      "steps done 1107620\n",
      "update target\n",
      "1465 episodes completed\n",
      "loss 5.366512298583984\n",
      "steps done 1111200\n",
      "1470 episodes completed\n",
      "loss 5.725600242614746\n",
      "steps done 1115050\n",
      "1475 episodes completed\n",
      "loss 5.223695278167725\n",
      "steps done 1118240\n",
      "update target\n",
      "1480 episodes completed\n",
      "loss 4.628920078277588\n",
      "steps done 1121860\n",
      "1485 episodes completed\n",
      "loss 4.6472368240356445\n",
      "steps done 1125710\n",
      "update target\n",
      "1490 episodes completed\n",
      "loss 5.478821754455566\n",
      "steps done 1130250\n",
      "1495 episodes completed\n",
      "loss 3.959315299987793\n",
      "steps done 1133650\n",
      "1500 episodes completed\n",
      "loss 4.726455211639404\n",
      "steps done 1137840\n",
      "update target\n",
      "1505 episodes completed\n",
      "loss 5.478329658508301\n",
      "steps done 1141570\n",
      "1510 episodes completed\n",
      "loss 3.8843326568603516\n",
      "steps done 1145140\n",
      "1515 episodes completed\n",
      "loss 5.237200736999512\n",
      "steps done 1149010\n",
      "update target\n",
      "1520 episodes completed\n",
      "loss 5.397235870361328\n",
      "steps done 1152980\n",
      "1525 episodes completed\n",
      "loss 5.110222816467285\n",
      "steps done 1157020\n",
      "update target\n",
      "1530 episodes completed\n",
      "loss 5.405010223388672\n",
      "steps done 1160530\n",
      "1535 episodes completed\n",
      "loss 4.923558235168457\n",
      "steps done 1163700\n",
      "1540 episodes completed\n",
      "loss 5.799984931945801\n",
      "steps done 1167030\n",
      "update target\n",
      "1545 episodes completed\n",
      "loss 5.189120292663574\n",
      "steps done 1170420\n",
      "1550 episodes completed\n",
      "loss 5.796424388885498\n",
      "steps done 1173600\n",
      "1555 episodes completed\n",
      "loss 5.749340534210205\n",
      "steps done 1176710\n",
      "update target\n",
      "1560 episodes completed\n",
      "loss 4.064780235290527\n",
      "steps done 1180710\n",
      "1565 episodes completed\n",
      "loss 5.695992469787598\n",
      "steps done 1184110\n",
      "1570 episodes completed\n",
      "loss 4.179470539093018\n",
      "steps done 1188360\n",
      "update target\n",
      "1575 episodes completed\n",
      "loss 4.722796440124512\n",
      "steps done 1191830\n",
      "1580 episodes completed\n",
      "loss 5.429018974304199\n",
      "steps done 1196210\n",
      "update target\n",
      "1585 episodes completed\n",
      "loss 5.3088836669921875\n",
      "steps done 1200630\n",
      "1590 episodes completed\n",
      "loss 5.221126556396484\n",
      "steps done 1204760\n",
      "1595 episodes completed\n",
      "loss 4.393357276916504\n",
      "steps done 1208620\n",
      "update target\n",
      "1600 episodes completed\n",
      "loss 4.469904899597168\n",
      "steps done 1212950\n",
      "1605 episodes completed\n",
      "loss 4.126954555511475\n",
      "steps done 1216980\n",
      "update target\n",
      "1610 episodes completed\n",
      "loss 4.971522331237793\n",
      "steps done 1220550\n",
      "1615 episodes completed\n",
      "loss 5.200117111206055\n",
      "steps done 1224280\n",
      "1620 episodes completed\n",
      "loss 4.20067834854126\n",
      "steps done 1227500\n",
      "update target\n",
      "1625 episodes completed\n",
      "loss 5.785938262939453\n",
      "steps done 1231250\n",
      "1630 episodes completed\n",
      "loss 6.184202671051025\n",
      "steps done 1234830\n",
      "1635 episodes completed\n",
      "loss 5.31341028213501\n",
      "steps done 1239030\n",
      "update target\n",
      "1640 episodes completed\n",
      "loss 5.0900468826293945\n",
      "steps done 1243520\n",
      "1645 episodes completed\n",
      "loss 4.581241130828857\n",
      "steps done 1247050\n",
      "update target\n",
      "1650 episodes completed\n",
      "loss 4.7542524337768555\n",
      "steps done 1251550\n",
      "1655 episodes completed\n",
      "loss 5.373971939086914\n",
      "steps done 1255490\n",
      "1660 episodes completed\n",
      "loss 4.401565074920654\n",
      "steps done 1259500\n",
      "update target\n",
      "1665 episodes completed\n",
      "loss 5.989429473876953\n",
      "steps done 1263350\n",
      "1670 episodes completed\n",
      "loss 4.663671493530273\n",
      "steps done 1266770\n",
      "update target\n",
      "1675 episodes completed\n",
      "loss 5.009271621704102\n",
      "steps done 1270320\n",
      "1680 episodes completed\n",
      "loss 5.449047088623047\n",
      "steps done 1274220\n",
      "1685 episodes completed\n",
      "loss 5.054704189300537\n",
      "steps done 1277720\n",
      "update target\n",
      "1690 episodes completed\n",
      "loss 5.3572845458984375\n",
      "steps done 1281240\n",
      "1695 episodes completed\n",
      "loss 4.334409713745117\n",
      "steps done 1284180\n",
      "1700 episodes completed\n",
      "loss 4.475191593170166\n",
      "steps done 1287970\n",
      "update target\n",
      "1705 episodes completed\n",
      "loss 5.743561267852783\n",
      "steps done 1291370\n",
      "1710 episodes completed\n",
      "loss 5.323616027832031\n",
      "steps done 1295300\n",
      "1715 episodes completed\n",
      "loss 4.863519668579102\n",
      "steps done 1298590\n",
      "update target\n",
      "1720 episodes completed\n",
      "loss 5.06846284866333\n",
      "steps done 1301600\n",
      "1725 episodes completed\n",
      "loss 5.736154079437256\n",
      "steps done 1304870\n",
      "1730 episodes completed\n",
      "loss 4.633687496185303\n",
      "steps done 1308660\n",
      "update target\n",
      "1735 episodes completed\n",
      "loss 5.400173664093018\n",
      "steps done 1311940\n",
      "1740 episodes completed\n",
      "loss 4.473206043243408\n",
      "steps done 1315750\n",
      "1745 episodes completed\n",
      "loss 4.486519813537598\n",
      "steps done 1318970\n",
      "update target\n",
      "1750 episodes completed\n",
      "loss 4.756541728973389\n",
      "steps done 1322720\n",
      "1755 episodes completed\n",
      "loss 4.9699482917785645\n",
      "steps done 1326290\n",
      "update target\n",
      "1760 episodes completed\n",
      "loss 3.8899331092834473\n",
      "steps done 1330560\n",
      "1765 episodes completed\n",
      "loss 4.715550422668457\n",
      "steps done 1333900\n",
      "1770 episodes completed\n",
      "loss 4.031968593597412\n",
      "steps done 1338080\n",
      "update target\n",
      "1775 episodes completed\n",
      "loss 4.486556053161621\n",
      "steps done 1341500\n",
      "1780 episodes completed\n",
      "loss 4.5096025466918945\n",
      "steps done 1345050\n",
      "1785 episodes completed\n",
      "loss 4.608570575714111\n",
      "steps done 1348970\n",
      "update target\n",
      "1790 episodes completed\n",
      "loss 4.589268684387207\n",
      "steps done 1352930\n",
      "1795 episodes completed\n",
      "loss 5.281394004821777\n",
      "steps done 1356040\n",
      "1800 episodes completed\n",
      "loss 4.17877197265625\n",
      "steps done 1359330\n",
      "update target\n",
      "1805 episodes completed\n",
      "loss 4.216279983520508\n",
      "steps done 1362860\n",
      "1810 episodes completed\n",
      "loss 4.723588943481445\n",
      "steps done 1366650\n",
      "update target\n",
      "1815 episodes completed\n",
      "loss 5.05640983581543\n",
      "steps done 1370450\n",
      "1820 episodes completed\n",
      "loss 4.856408596038818\n",
      "steps done 1373810\n",
      "1825 episodes completed\n",
      "loss 4.0858635902404785\n",
      "steps done 1377410\n",
      "update target\n",
      "1830 episodes completed\n",
      "loss 5.291009902954102\n",
      "steps done 1380680\n",
      "1835 episodes completed\n",
      "loss 4.054480075836182\n",
      "steps done 1384400\n",
      "1840 episodes completed\n",
      "loss 4.348025321960449\n",
      "steps done 1388380\n",
      "update target\n",
      "1845 episodes completed\n",
      "loss 5.317892074584961\n",
      "steps done 1391790\n",
      "1850 episodes completed\n",
      "loss 4.679502964019775\n",
      "steps done 1395880\n",
      "1855 episodes completed\n",
      "loss 5.673385143280029\n",
      "steps done 1399740\n",
      "update target\n",
      "1860 episodes completed\n",
      "loss 4.932720184326172\n",
      "steps done 1403530\n",
      "1865 episodes completed\n",
      "loss 4.256187438964844\n",
      "steps done 1407060\n",
      "update target\n",
      "1870 episodes completed\n",
      "loss 4.353756427764893\n",
      "steps done 1410220\n",
      "1875 episodes completed\n",
      "loss 4.448922634124756\n",
      "steps done 1414050\n",
      "1880 episodes completed\n",
      "loss 4.814248085021973\n",
      "steps done 1417090\n",
      "update target\n",
      "1885 episodes completed\n",
      "loss 5.302537441253662\n",
      "steps done 1421370\n",
      "1890 episodes completed\n",
      "loss 4.198331832885742\n",
      "steps done 1425510\n",
      "1895 episodes completed\n",
      "loss 4.4211883544921875\n",
      "steps done 1428880\n",
      "update target\n",
      "1900 episodes completed\n",
      "loss 4.082204818725586\n",
      "steps done 1432630\n",
      "1905 episodes completed\n",
      "loss 3.8506064414978027\n",
      "steps done 1436780\n",
      "update target\n",
      "1910 episodes completed\n",
      "loss 4.4211649894714355\n",
      "steps done 1440800\n",
      "1915 episodes completed\n",
      "loss 4.839537620544434\n",
      "steps done 1444530\n",
      "1920 episodes completed\n",
      "loss 4.765457630157471\n",
      "steps done 1448270\n",
      "update target\n",
      "1925 episodes completed\n",
      "loss 4.5324931144714355\n",
      "steps done 1452330\n",
      "1930 episodes completed\n",
      "loss 4.412827968597412\n",
      "steps done 1456360\n",
      "update target\n",
      "1935 episodes completed\n",
      "loss 5.207884311676025\n",
      "steps done 1460510\n",
      "1940 episodes completed\n",
      "loss 6.260285377502441\n",
      "steps done 1464420\n",
      "1945 episodes completed\n",
      "loss 5.499495029449463\n",
      "steps done 1467920\n",
      "update target\n",
      "1950 episodes completed\n",
      "loss 5.341540336608887\n",
      "steps done 1471210\n",
      "1955 episodes completed\n",
      "loss 4.3920578956604\n",
      "steps done 1474880\n",
      "1960 episodes completed\n",
      "loss 3.895327568054199\n",
      "steps done 1478500\n",
      "update target\n",
      "1965 episodes completed\n",
      "loss 4.325231552124023\n",
      "steps done 1482350\n",
      "1970 episodes completed\n",
      "loss 5.370433807373047\n",
      "steps done 1486620\n",
      "update target\n",
      "1975 episodes completed\n",
      "loss 4.69359016418457\n",
      "steps done 1490290\n",
      "1980 episodes completed\n",
      "loss 5.265070915222168\n",
      "steps done 1494480\n",
      "1985 episodes completed\n",
      "loss 4.741535663604736\n",
      "steps done 1498030\n",
      "update target\n",
      "1990 episodes completed\n",
      "loss 3.9756765365600586\n",
      "steps done 1501240\n",
      "1995 episodes completed\n",
      "loss 5.320531845092773\n",
      "steps done 1505260\n",
      "2000 episodes completed\n",
      "loss 4.964341163635254\n",
      "steps done 1508360\n",
      "update target\n",
      "2005 episodes completed\n",
      "loss 4.756484031677246\n",
      "steps done 1511730\n",
      "2010 episodes completed\n",
      "loss 4.091346740722656\n",
      "steps done 1515600\n",
      "2015 episodes completed\n",
      "loss 5.1207685470581055\n",
      "steps done 1518840\n",
      "update target\n",
      "2020 episodes completed\n",
      "loss 5.483211517333984\n",
      "steps done 1522330\n",
      "2025 episodes completed\n",
      "loss 5.408271789550781\n",
      "steps done 1526410\n",
      "2030 episodes completed\n",
      "loss 4.498984336853027\n",
      "steps done 1529630\n",
      "update target\n",
      "2035 episodes completed\n",
      "loss 4.770173072814941\n",
      "steps done 1533690\n",
      "2040 episodes completed\n",
      "loss 4.849337577819824\n",
      "steps done 1536770\n",
      "update target\n",
      "2045 episodes completed\n",
      "loss 5.854742050170898\n",
      "steps done 1540690\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://nhn2k267xr.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "observation = fireEnv.reset()\n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "  for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "    observation = fireEnv.step()\n",
    "\n",
    "    state_vector_1 = dronesEnv.drones[0].state\n",
    "    map_1 = dronesEnv.drones[0].observation\n",
    "    state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "    map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_2 = dronesEnv.drones[1].state\n",
    "    map_2 = dronesEnv.drones[1].observation\n",
    "    state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "    for i in range(int(DT/DTI)):\n",
    "      action1 = select_action(map_1, state_vector_1, steps)\n",
    "      action2 = select_action(map_2, state_vector_2, steps)\n",
    "      steps += 2\n",
    "      reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "      next_state_vector_1 = dronesEnv.drones[0].state\n",
    "      next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "      next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "      next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "      next_state_vector_2 = dronesEnv.drones[1].state\n",
    "      next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "      next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "      next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "      reward_1 = torch.tensor([reward_1], device=device)\n",
    "      reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "      memory.push(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1)\n",
    "      memory.push(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2)\n",
    "\n",
    "      state_vector_1 = next_state_vector_1\n",
    "      state_vector_2 = next_state_vector_2\n",
    "\n",
    "      map_1 = next_map_1\n",
    "      map_2 = next_map_2\n",
    "\n",
    "    if not fireEnv.fire_in_range(6):\n",
    "      observation = fireEnv.reset()\n",
    "      dronesEnv.reset(observation)\n",
    "\n",
    "      i_episode +=1\n",
    "      if (i_episode+1) % 5 == 0:\n",
    "        print(f'{i_episode+1} episodes completed')\n",
    "        print(f'loss {loss}')\n",
    "        print(f'steps done {steps}')\n",
    "      \n",
    "\n",
    "  if steps>=INIT_SIZE:\n",
    "    loss = optimize_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
