{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgpath2mpl\n",
      "  Downloading svgpath2mpl-1.0.0-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "Installing collected packages: svgpath2mpl\n",
      "Successfully installed svgpath2mpl-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from replay_memory import Transition\n",
    "from networks.drqn import DRQN\n",
    "from episode_buffer import EpisodeBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "BATCH_SIZE = 5\n",
    "GAMMA = 0.99\n",
    "INIT_SIZE = 5\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DRQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DRQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "episode_buffer = EpisodeBuffer()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "EPISODE_LENGTH = 128\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    loss = None\n",
    "    \n",
    "    global update_counter\n",
    "\n",
    "    episode_batch, epiosde_length = episode_buffer.sample(EPISODE_LENGTH)\n",
    "    print(epiosde_length)\n",
    "\n",
    "    update_counter += 1\n",
    "    batch = Transition(*zip(*episode_batch))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    hidden_policy = policy_net.init_hidden_state()\n",
    "    hidden_target = target_net.init_hidden_state()\n",
    "\n",
    "    policy_output, _ = policy_net(belief_map_batch, state_vector_batch, hidden_policy)\n",
    "    target_output, _ = target_net(next_belief_map, next_states, hidden_target)\n",
    "\n",
    "    state_action_values = policy_output.gather(1, action_batch)\n",
    "    next_state_values = target_output.max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights_drqn.pt'\n",
    "        target_file_path = f'./target_weights_drqn.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episodes completed\n",
      "loss None\n",
      "steps done 2090\n",
      "10 episodes completed\n",
      "loss None\n",
      "steps done 6230\n",
      "15 episodes completed\n",
      "loss None\n",
      "steps done 10180\n",
      "20 episodes completed\n",
      "loss None\n",
      "steps done 13480\n",
      "128\n",
      "128\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance/run_drqn.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m   map_2 \u001b[39m=\u001b[39m next_map_2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m i_episode\u001b[39m>\u001b[39m\u001b[39m=\u001b[39mINIT_SIZE:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m   loss \u001b[39m=\u001b[39m optimize_model()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fireEnv\u001b[39m.\u001b[39mfire_in_range(\u001b[39m6\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m   observation \u001b[39m=\u001b[39m fireEnv\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance/run_drqn.ipynb Cell 6\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mglobal\u001b[39;00m update_counter\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m episode_batch, epiosde_length \u001b[39m=\u001b[39m episode_buffer\u001b[39m.\u001b[39;49msample(EPISODE_LENGTH)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(epiosde_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_drqn.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m update_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/notebooks/episode_buffer.py:16\u001b[0m, in \u001b[0;36mEpisodeBuffer.sample\u001b[0;34m(self, episode_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m, episode_length):\n\u001b[0;32m---> 16\u001b[0m   episode_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisodes[random\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisodes))]\n\u001b[1;32m     18\u001b[0m   episode_start \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m30\u001b[39m)\n\u001b[1;32m     19\u001b[0m   episode_end  \u001b[39m=\u001b[39m  \u001b[39mmin\u001b[39m(episode_start\u001b[39m+\u001b[39mepisode_length, \u001b[39mlen\u001b[39m(episode_batch))\n",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "observation = fireEnv.reset()\n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "episode_memory_1 = []\n",
    "episode_memory_2 = []\n",
    "\n",
    "hidden_1 = policy_net.init_hidden_state()\n",
    "hidden_2 = policy_net.init_hidden_state()\n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "  for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "    observation = fireEnv.step()\n",
    "\n",
    "    state_vector_1 = dronesEnv.drones[0].state\n",
    "    map_1 = dronesEnv.drones[0].observation\n",
    "    state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "    map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_2 = dronesEnv.drones[1].state\n",
    "    map_2 = dronesEnv.drones[1].observation\n",
    "    state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "    for i in range(int(DT/DTI)):\n",
    "\n",
    "      action1, hidden_1 = policy_net.select_action(map_1, state_vector_1, steps, hidden=hidden_1)\n",
    "      action2, hidden_2 = policy_net.select_action(map_2, state_vector_2, steps, hidden=hidden_2)\n",
    "      steps += 2\n",
    "      reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "      next_state_vector_1 = dronesEnv.drones[0].state\n",
    "      next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "      next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "      next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "      next_state_vector_2 = dronesEnv.drones[1].state\n",
    "      next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "      next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "      next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "      reward_1 = torch.tensor([reward_1], device=device)\n",
    "      reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "      episode_memory_1.append(Transition(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1))\n",
    "      episode_memory_2.append(Transition(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2))\n",
    "\n",
    "      state_vector_1 = next_state_vector_1\n",
    "      state_vector_2 = next_state_vector_2\n",
    "\n",
    "      map_1 = next_map_1\n",
    "      map_2 = next_map_2\n",
    "\n",
    "    if i_episode>=INIT_SIZE:\n",
    "      loss = optimize_model()\n",
    "      \n",
    "    if not fireEnv.fire_in_range(6):\n",
    "      observation = fireEnv.reset()\n",
    "      dronesEnv.reset(observation)\n",
    "      episode_buffer.push(episode_memory_1.copy())\n",
    "      episode_buffer.push(episode_memory_2.copy())\n",
    "      episode_memory_1 = []\n",
    "      episode_memory_2 = []\n",
    "      hidden_1 = policy_net.init_hidden_state()\n",
    "      hidden_2 = policy_net.init_hidden_state()\n",
    "      i_episode +=1\n",
    "      \n",
    "      if (i_episode+1) % 5 == 0:\n",
    "        print(f'{i_episode+1} episodes completed')\n",
    "        print(f'loss {loss}')\n",
    "        print(f'steps done {steps}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
