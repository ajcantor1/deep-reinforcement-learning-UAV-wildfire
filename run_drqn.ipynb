{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgpath2mpl\n",
      "  Downloading svgpath2mpl-1.0.0-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "Installing collected packages: svgpath2mpl\n",
      "Successfully installed svgpath2mpl-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from replay_memory import Transition\n",
    "from networks.drqn import DRQN\n",
    "from episode_buffer import EpisodeBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "BATCH_SIZE = 5\n",
    "GAMMA = 0.99\n",
    "INIT_SIZE = 20\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DRQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DRQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "episode_buffer = EpisodeBuffer()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "EPISODE_LENGTH = 128\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    loss = None\n",
    "    \n",
    "    global update_counter\n",
    "\n",
    "    episode_batch, epiosde_length = episode_buffer.sample(EPISODE_LENGTH)\n",
    "    print(epiosde_length)\n",
    "\n",
    "    update_counter += 1\n",
    "    batch = Transition(*zip(*episode_batch))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    hidden_policy = policy_net.init_hidden_state()\n",
    "    hidden_target = target_net.init_hidden_state()\n",
    "\n",
    "    policy_output, _ = policy_net(belief_map_batch, state_vector_batch, hidden_policy)\n",
    "    target_output, _ = target_net(next_belief_map, next_states, hidden_target)\n",
    "\n",
    "    state_action_values = policy_output.gather(1, action_batch)\n",
    "    next_state_values = target_output.max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights_drqn.pt'\n",
    "        target_file_path = f'./target_weights_drqn.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episodes completed\n",
      "loss None\n",
      "steps done 2260\n",
      "10 episodes completed\n",
      "loss None\n",
      "steps done 5880\n",
      "15 episodes completed\n",
      "loss 0.0028986050747334957\n",
      "steps done 9940\n",
      "20 episodes completed\n",
      "loss 0.0035093410406261683\n",
      "steps done 13900\n",
      "25 episodes completed\n",
      "loss 0.005467869807034731\n",
      "steps done 17620\n",
      "30 episodes completed\n",
      "loss 0.008082836866378784\n",
      "steps done 21310\n",
      "35 episodes completed\n",
      "loss 0.026911139488220215\n",
      "steps done 24600\n",
      "40 episodes completed\n",
      "loss 1.2059624195098877\n",
      "steps done 28690\n",
      "45 episodes completed\n",
      "loss 1.1862528324127197\n",
      "steps done 32230\n",
      "50 episodes completed\n",
      "loss 1.192994475364685\n",
      "steps done 36240\n",
      "55 episodes completed\n",
      "loss 0.009061483666300774\n",
      "steps done 40440\n",
      "60 episodes completed\n",
      "loss 0.08432638645172119\n",
      "steps done 44140\n",
      "65 episodes completed\n",
      "loss 0.6622826457023621\n",
      "steps done 47090\n",
      "70 episodes completed\n",
      "loss 0.3353615701198578\n",
      "steps done 50330\n",
      "75 episodes completed\n",
      "loss 5.6673707149457186e-05\n",
      "steps done 54410\n",
      "80 episodes completed\n",
      "loss 0.010501058772206306\n",
      "steps done 58050\n",
      "85 episodes completed\n",
      "loss 0.9742815494537354\n",
      "steps done 61710\n",
      "90 episodes completed\n",
      "loss 1.1660640239715576\n",
      "steps done 65390\n",
      "95 episodes completed\n",
      "loss 0.0008339259075000882\n",
      "steps done 69020\n",
      "100 episodes completed\n",
      "loss 2.5462746620178223\n",
      "steps done 72950\n",
      "105 episodes completed\n",
      "loss 0.7766090631484985\n",
      "steps done 76690\n",
      "110 episodes completed\n",
      "loss 1.6743364334106445\n",
      "steps done 81010\n",
      "115 episodes completed\n",
      "loss 0.0001832641428336501\n",
      "steps done 85130\n",
      "120 episodes completed\n",
      "loss 0.00011639480362646282\n",
      "steps done 88890\n",
      "125 episodes completed\n",
      "loss 0.10774379223585129\n",
      "steps done 92140\n",
      "130 episodes completed\n",
      "loss 1.7552443742752075\n",
      "steps done 95800\n",
      "135 episodes completed\n",
      "loss 0.6695718169212341\n",
      "steps done 99800\n",
      "140 episodes completed\n",
      "loss 0.00012335953942965716\n",
      "steps done 103990\n",
      "145 episodes completed\n",
      "loss 0.0008850247832015157\n",
      "steps done 107900\n",
      "150 episodes completed\n",
      "loss 0.0003068852238357067\n",
      "steps done 111600\n",
      "155 episodes completed\n",
      "loss 0.007444075308740139\n",
      "steps done 116000\n",
      "160 episodes completed\n",
      "loss 0.00042569846846163273\n",
      "steps done 119620\n",
      "165 episodes completed\n",
      "loss 0.007923208177089691\n",
      "steps done 123550\n",
      "170 episodes completed\n",
      "loss 0.008161945268511772\n",
      "steps done 127320\n",
      "175 episodes completed\n",
      "loss 0.01164248213171959\n",
      "steps done 130580\n",
      "180 episodes completed\n",
      "loss 0.0004533775500021875\n",
      "steps done 134790\n",
      "185 episodes completed\n",
      "loss 1.2198193073272705\n",
      "steps done 138180\n",
      "190 episodes completed\n",
      "loss 0.00033557217102497816\n",
      "steps done 141820\n",
      "195 episodes completed\n",
      "loss 0.00026265421183779836\n",
      "steps done 145570\n",
      "200 episodes completed\n",
      "loss 0.8177887797355652\n",
      "steps done 149480\n",
      "205 episodes completed\n",
      "loss 0.00015741694369353354\n",
      "steps done 153590\n",
      "210 episodes completed\n",
      "loss 0.0007625038269907236\n",
      "steps done 157360\n",
      "update target\n",
      "215 episodes completed\n",
      "loss 0.0016579203074797988\n",
      "steps done 161100\n",
      "220 episodes completed\n",
      "loss 0.0017361636273562908\n",
      "steps done 164770\n",
      "225 episodes completed\n",
      "loss 2.1262717247009277\n",
      "steps done 168450\n",
      "230 episodes completed\n",
      "loss 0.20800258219242096\n",
      "steps done 173370\n",
      "235 episodes completed\n",
      "loss 0.0006778041133657098\n",
      "steps done 176810\n",
      "240 episodes completed\n",
      "loss 0.0012448809575289488\n",
      "steps done 181090\n",
      "245 episodes completed\n",
      "loss 0.0013677321840077639\n",
      "steps done 184930\n",
      "250 episodes completed\n",
      "loss 9.17567522265017e-05\n",
      "steps done 188310\n",
      "255 episodes completed\n",
      "loss 0.002633144846186042\n",
      "steps done 192440\n",
      "260 episodes completed\n",
      "loss 1.087293267250061\n",
      "steps done 196580\n",
      "265 episodes completed\n",
      "loss 0.0003347109886817634\n",
      "steps done 199920\n",
      "270 episodes completed\n",
      "loss 0.9998207092285156\n",
      "steps done 203670\n",
      "275 episodes completed\n",
      "loss 0.5917775630950928\n",
      "steps done 206950\n",
      "280 episodes completed\n",
      "loss 9.32373950490728e-05\n",
      "steps done 210330\n",
      "285 episodes completed\n",
      "loss 1.340127944946289\n",
      "steps done 213940\n",
      "290 episodes completed\n",
      "loss 0.11677953600883484\n",
      "steps done 217650\n",
      "295 episodes completed\n",
      "loss 9.16522549232468e-05\n",
      "steps done 222030\n",
      "300 episodes completed\n",
      "loss 0.00012267794227227569\n",
      "steps done 226400\n",
      "305 episodes completed\n",
      "loss 0.036032989621162415\n",
      "steps done 231040\n",
      "310 episodes completed\n",
      "loss 1.9206676483154297\n",
      "steps done 234920\n",
      "315 episodes completed\n",
      "loss 2.227952480316162\n",
      "steps done 238560\n",
      "320 episodes completed\n",
      "loss 0.41967010498046875\n",
      "steps done 242180\n",
      "325 episodes completed\n",
      "loss 1.950266718864441\n",
      "steps done 245900\n",
      "330 episodes completed\n",
      "loss 2.1451218128204346\n",
      "steps done 249780\n",
      "335 episodes completed\n",
      "loss 0.0012609302066266537\n",
      "steps done 254530\n",
      "340 episodes completed\n",
      "loss 0.001625596429221332\n",
      "steps done 258640\n",
      "345 episodes completed\n",
      "loss 1.5716265439987183\n",
      "steps done 262210\n",
      "350 episodes completed\n",
      "loss 0.0003771514166146517\n",
      "steps done 266050\n",
      "355 episodes completed\n",
      "loss 1.6976454257965088\n",
      "steps done 269740\n",
      "360 episodes completed\n",
      "loss 1.218332052230835\n",
      "steps done 273260\n",
      "365 episodes completed\n",
      "loss 0.0009601732017472386\n",
      "steps done 276830\n",
      "370 episodes completed\n",
      "loss 0.6239717602729797\n",
      "steps done 280650\n",
      "375 episodes completed\n",
      "loss 1.4254205226898193\n",
      "steps done 284000\n",
      "380 episodes completed\n",
      "loss 0.7555331587791443\n",
      "steps done 288070\n",
      "385 episodes completed\n",
      "loss 0.8615660667419434\n",
      "steps done 291610\n",
      "390 episodes completed\n",
      "loss 0.8465982675552368\n",
      "steps done 295660\n",
      "395 episodes completed\n",
      "loss 0.0005199622828513384\n",
      "steps done 299710\n",
      "400 episodes completed\n",
      "loss 0.3159262537956238\n",
      "steps done 303530\n",
      "405 episodes completed\n",
      "loss 1.1780928373336792\n",
      "steps done 307350\n",
      "410 episodes completed\n",
      "loss 1.9982552528381348\n",
      "steps done 311150\n",
      "update target\n",
      "415 episodes completed\n",
      "loss 0.003777076257392764\n",
      "steps done 315150\n",
      "420 episodes completed\n",
      "loss 1.760286808013916\n",
      "steps done 318720\n",
      "425 episodes completed\n",
      "loss 0.00416798098012805\n",
      "steps done 322490\n",
      "430 episodes completed\n",
      "loss 1.9612467288970947\n",
      "steps done 326160\n",
      "435 episodes completed\n",
      "loss 1.628909945487976\n",
      "steps done 328990\n",
      "440 episodes completed\n",
      "loss 0.1076575368642807\n",
      "steps done 333980\n",
      "445 episodes completed\n",
      "loss 0.10375423729419708\n",
      "steps done 337790\n",
      "450 episodes completed\n",
      "loss 0.12281150370836258\n",
      "steps done 340970\n",
      "455 episodes completed\n",
      "loss 0.011861978098750114\n",
      "steps done 344880\n",
      "460 episodes completed\n",
      "loss 2.2402400970458984\n",
      "steps done 348380\n",
      "465 episodes completed\n",
      "loss 1.262589693069458\n",
      "steps done 352410\n",
      "470 episodes completed\n",
      "loss 0.5385116338729858\n",
      "steps done 356540\n",
      "475 episodes completed\n",
      "loss 0.0013838130980730057\n",
      "steps done 359960\n",
      "480 episodes completed\n",
      "loss 0.0021870562341064215\n",
      "steps done 364560\n",
      "485 episodes completed\n",
      "loss 0.0004339900042396039\n",
      "steps done 368150\n",
      "490 episodes completed\n",
      "loss 0.00914568267762661\n",
      "steps done 371920\n",
      "495 episodes completed\n",
      "loss 1.3023581504821777\n",
      "steps done 375830\n",
      "500 episodes completed\n",
      "loss 1.7857365608215332\n",
      "steps done 379490\n",
      "505 episodes completed\n",
      "loss 0.09508711099624634\n",
      "steps done 384290\n",
      "510 episodes completed\n",
      "loss 0.36839839816093445\n",
      "steps done 388770\n",
      "515 episodes completed\n",
      "loss 0.0009364644065499306\n",
      "steps done 393050\n",
      "520 episodes completed\n",
      "loss 0.00022456212900578976\n",
      "steps done 397070\n",
      "525 episodes completed\n",
      "loss 0.0010905108647421002\n",
      "steps done 400810\n",
      "530 episodes completed\n",
      "loss 0.04513579607009888\n",
      "steps done 404780\n",
      "535 episodes completed\n",
      "loss 0.0004022135981358588\n",
      "steps done 408290\n",
      "540 episodes completed\n",
      "loss 1.2437970638275146\n",
      "steps done 411710\n",
      "545 episodes completed\n",
      "loss 0.7812693119049072\n",
      "steps done 415220\n",
      "550 episodes completed\n",
      "loss 0.17783069610595703\n",
      "steps done 418590\n",
      "555 episodes completed\n",
      "loss 0.006891052704304457\n",
      "steps done 422180\n",
      "560 episodes completed\n",
      "loss 0.3674321174621582\n",
      "steps done 424910\n",
      "565 episodes completed\n",
      "loss 0.0003152063291054219\n",
      "steps done 428910\n",
      "570 episodes completed\n",
      "loss 0.46437978744506836\n",
      "steps done 433020\n",
      "575 episodes completed\n",
      "loss 0.00018562546756584197\n",
      "steps done 436760\n",
      "580 episodes completed\n",
      "loss 0.0002720144693739712\n",
      "steps done 440800\n",
      "585 episodes completed\n",
      "loss 0.0003323379496578127\n",
      "steps done 444400\n",
      "590 episodes completed\n",
      "loss 0.003183006774634123\n",
      "steps done 448270\n",
      "595 episodes completed\n",
      "loss 1.316156268119812\n",
      "steps done 452210\n",
      "600 episodes completed\n",
      "loss 0.001351686310954392\n",
      "steps done 456360\n",
      "605 episodes completed\n",
      "loss 0.0003350779297761619\n",
      "steps done 459980\n",
      "610 episodes completed\n",
      "loss 0.004991026129573584\n",
      "steps done 463150\n",
      "update target\n",
      "615 episodes completed\n",
      "loss 0.1862863302230835\n",
      "steps done 466860\n",
      "620 episodes completed\n",
      "loss 0.030111216008663177\n",
      "steps done 470770\n",
      "625 episodes completed\n",
      "loss 0.12046107649803162\n",
      "steps done 474210\n",
      "630 episodes completed\n",
      "loss 0.009406199678778648\n",
      "steps done 477880\n",
      "635 episodes completed\n",
      "loss 0.06477770209312439\n",
      "steps done 481490\n",
      "640 episodes completed\n",
      "loss 0.21684682369232178\n",
      "steps done 484930\n",
      "645 episodes completed\n",
      "loss 0.0015184220392256975\n",
      "steps done 489200\n",
      "650 episodes completed\n",
      "loss 0.23940011858940125\n",
      "steps done 493330\n",
      "655 episodes completed\n",
      "loss 0.01685970090329647\n",
      "steps done 497120\n",
      "660 episodes completed\n",
      "loss 0.2978299856185913\n",
      "steps done 501580\n",
      "665 episodes completed\n",
      "loss 0.08009964972734451\n",
      "steps done 504540\n",
      "670 episodes completed\n",
      "loss 0.07961088418960571\n",
      "steps done 508840\n",
      "675 episodes completed\n",
      "loss 1.6853439807891846\n",
      "steps done 513220\n",
      "680 episodes completed\n",
      "loss 0.0026550255715847015\n",
      "steps done 516570\n",
      "685 episodes completed\n",
      "loss 2.894019842147827\n",
      "steps done 520400\n",
      "690 episodes completed\n",
      "loss 0.46083033084869385\n",
      "steps done 523480\n",
      "695 episodes completed\n",
      "loss 2.3150017261505127\n",
      "steps done 527460\n",
      "700 episodes completed\n",
      "loss 0.34715044498443604\n",
      "steps done 530910\n",
      "705 episodes completed\n",
      "loss 0.0030063583981245756\n",
      "steps done 534410\n",
      "710 episodes completed\n",
      "loss 0.017092309892177582\n",
      "steps done 538160\n",
      "715 episodes completed\n",
      "loss 0.19479992985725403\n",
      "steps done 542570\n",
      "720 episodes completed\n",
      "loss 0.8691332340240479\n",
      "steps done 545870\n",
      "725 episodes completed\n",
      "loss 1.3240091800689697\n",
      "steps done 549520\n",
      "730 episodes completed\n",
      "loss 0.000982892932370305\n",
      "steps done 553130\n",
      "735 episodes completed\n",
      "loss 0.6875004172325134\n",
      "steps done 556880\n",
      "740 episodes completed\n",
      "loss 0.5622607469558716\n",
      "steps done 561380\n",
      "745 episodes completed\n",
      "loss 0.0012219364289194345\n",
      "steps done 565280\n",
      "750 episodes completed\n",
      "loss 0.605439305305481\n",
      "steps done 568810\n",
      "755 episodes completed\n",
      "loss 0.8308389186859131\n",
      "steps done 572460\n",
      "760 episodes completed\n",
      "loss 0.0008585898904129863\n",
      "steps done 576320\n",
      "765 episodes completed\n",
      "loss 0.05644558370113373\n",
      "steps done 580210\n",
      "770 episodes completed\n",
      "loss 0.0018103562761098146\n",
      "steps done 583550\n",
      "775 episodes completed\n",
      "loss 0.00047512497985735536\n",
      "steps done 587100\n",
      "780 episodes completed\n",
      "loss 0.3118312954902649\n",
      "steps done 591010\n",
      "785 episodes completed\n",
      "loss 0.4101051986217499\n",
      "steps done 594880\n",
      "790 episodes completed\n",
      "loss 0.09505534172058105\n",
      "steps done 598770\n",
      "795 episodes completed\n",
      "loss 0.290640264749527\n",
      "steps done 602830\n",
      "800 episodes completed\n",
      "loss 0.0011227995855733752\n",
      "steps done 606640\n",
      "805 episodes completed\n",
      "loss 0.41214942932128906\n",
      "steps done 609620\n",
      "810 episodes completed\n",
      "loss 0.26652097702026367\n",
      "steps done 612810\n",
      "update target\n",
      "815 episodes completed\n",
      "loss 0.0010293945670127869\n",
      "steps done 616640\n",
      "820 episodes completed\n",
      "loss 0.3343300223350525\n",
      "steps done 620590\n",
      "825 episodes completed\n",
      "loss 0.05769405886530876\n",
      "steps done 624860\n",
      "830 episodes completed\n",
      "loss 1.0887494087219238\n",
      "steps done 628020\n",
      "835 episodes completed\n",
      "loss 0.0014936438528820872\n",
      "steps done 632500\n",
      "840 episodes completed\n",
      "loss 0.008713599294424057\n",
      "steps done 636140\n",
      "845 episodes completed\n",
      "loss 0.5856760740280151\n",
      "steps done 640000\n",
      "850 episodes completed\n",
      "loss 0.001046476187184453\n",
      "steps done 643440\n",
      "855 episodes completed\n",
      "loss 0.06484052538871765\n",
      "steps done 647090\n",
      "860 episodes completed\n",
      "loss 0.001488636713474989\n",
      "steps done 650380\n",
      "865 episodes completed\n",
      "loss 0.734755277633667\n",
      "steps done 654140\n",
      "870 episodes completed\n",
      "loss 0.5878900289535522\n",
      "steps done 657720\n",
      "875 episodes completed\n",
      "loss 0.10454206168651581\n",
      "steps done 661820\n",
      "880 episodes completed\n",
      "loss 0.004101598635315895\n",
      "steps done 665590\n",
      "885 episodes completed\n",
      "loss 0.00023916510690469295\n",
      "steps done 668740\n",
      "890 episodes completed\n",
      "loss 1.3534715175628662\n",
      "steps done 672510\n",
      "895 episodes completed\n",
      "loss 5.333599983714521e-05\n",
      "steps done 675550\n",
      "900 episodes completed\n",
      "loss 0.0026411456055939198\n",
      "steps done 679250\n",
      "905 episodes completed\n",
      "loss 0.1430492252111435\n",
      "steps done 683060\n",
      "910 episodes completed\n",
      "loss 1.482426643371582\n",
      "steps done 687780\n",
      "915 episodes completed\n",
      "loss 0.03596968948841095\n",
      "steps done 690940\n",
      "920 episodes completed\n",
      "loss 0.0899234190583229\n",
      "steps done 694380\n",
      "925 episodes completed\n",
      "loss 0.00041234822128899395\n",
      "steps done 698180\n",
      "930 episodes completed\n",
      "loss 0.0035541222896426916\n",
      "steps done 702300\n",
      "935 episodes completed\n",
      "loss 1.0614566802978516\n",
      "steps done 705960\n",
      "940 episodes completed\n",
      "loss 0.0023984797298908234\n",
      "steps done 709220\n",
      "945 episodes completed\n",
      "loss 0.8426988124847412\n",
      "steps done 712880\n",
      "950 episodes completed\n",
      "loss 0.29356393218040466\n",
      "steps done 717340\n",
      "955 episodes completed\n",
      "loss 0.00019354085088707507\n",
      "steps done 721770\n",
      "960 episodes completed\n",
      "loss 1.2287800312042236\n",
      "steps done 725830\n",
      "965 episodes completed\n",
      "loss 0.0753781720995903\n",
      "steps done 730040\n",
      "970 episodes completed\n",
      "loss 0.00040494161657989025\n",
      "steps done 734100\n",
      "975 episodes completed\n",
      "loss 0.0008262471528723836\n",
      "steps done 738420\n",
      "980 episodes completed\n",
      "loss 0.02733580209314823\n",
      "steps done 742390\n",
      "985 episodes completed\n",
      "loss 0.02361721731722355\n",
      "steps done 745830\n",
      "990 episodes completed\n",
      "loss 0.00036021758569404483\n",
      "steps done 749900\n",
      "995 episodes completed\n",
      "loss 0.019178785383701324\n",
      "steps done 753590\n",
      "1000 episodes completed\n",
      "loss 0.38166913390159607\n",
      "steps done 756880\n",
      "1005 episodes completed\n",
      "loss 1.299514889717102\n",
      "steps done 760680\n",
      "1010 episodes completed\n",
      "loss 0.028115130960941315\n",
      "steps done 765070\n",
      "update target\n",
      "1015 episodes completed\n",
      "loss 0.006298665422946215\n",
      "steps done 769050\n",
      "1020 episodes completed\n",
      "loss 0.6665141582489014\n",
      "steps done 773140\n",
      "1025 episodes completed\n",
      "loss 0.0032464158721268177\n",
      "steps done 777000\n",
      "1030 episodes completed\n",
      "loss 0.11271319538354874\n",
      "steps done 780420\n",
      "1035 episodes completed\n",
      "loss 8.746292587602511e-05\n",
      "steps done 784770\n",
      "1040 episodes completed\n",
      "loss 0.20543748140335083\n",
      "steps done 788440\n",
      "1045 episodes completed\n",
      "loss 0.22697752714157104\n",
      "steps done 792090\n",
      "1050 episodes completed\n",
      "loss 0.00041348327067680657\n",
      "steps done 795940\n",
      "1055 episodes completed\n",
      "loss 0.001124499598518014\n",
      "steps done 799070\n",
      "1060 episodes completed\n",
      "loss 0.0013748351484537125\n",
      "steps done 803200\n",
      "1065 episodes completed\n",
      "loss 0.0015039807185530663\n",
      "steps done 807170\n",
      "1070 episodes completed\n",
      "loss 0.0032740666065365076\n",
      "steps done 811050\n",
      "1075 episodes completed\n",
      "loss 0.005190026946365833\n",
      "steps done 814750\n",
      "1080 episodes completed\n",
      "loss 0.006833907216787338\n",
      "steps done 818940\n",
      "1085 episodes completed\n",
      "loss 0.00039960903814062476\n",
      "steps done 823400\n",
      "1090 episodes completed\n",
      "loss 1.2921977043151855\n",
      "steps done 827380\n",
      "1095 episodes completed\n",
      "loss 0.5949652194976807\n",
      "steps done 830900\n",
      "1100 episodes completed\n",
      "loss 0.6677743792533875\n",
      "steps done 835490\n",
      "1105 episodes completed\n",
      "loss 0.005547159817069769\n",
      "steps done 840140\n",
      "1110 episodes completed\n",
      "loss 0.00021264725364744663\n",
      "steps done 844110\n",
      "1115 episodes completed\n",
      "loss 1.298668622970581\n",
      "steps done 847630\n",
      "1120 episodes completed\n",
      "loss 0.17400513589382172\n",
      "steps done 851790\n",
      "1125 episodes completed\n",
      "loss 0.02917102724313736\n",
      "steps done 855910\n",
      "1130 episodes completed\n",
      "loss 0.0033361080568283796\n",
      "steps done 859980\n",
      "1135 episodes completed\n",
      "loss 0.007980894297361374\n",
      "steps done 863090\n",
      "1140 episodes completed\n",
      "loss 0.0013530164724215865\n",
      "steps done 866870\n",
      "1145 episodes completed\n",
      "loss 0.7920064926147461\n",
      "steps done 869810\n",
      "1150 episodes completed\n",
      "loss 2.2731659412384033\n",
      "steps done 873550\n",
      "1155 episodes completed\n",
      "loss 0.0015462171286344528\n",
      "steps done 877150\n",
      "1160 episodes completed\n",
      "loss 1.3460909128189087\n",
      "steps done 880660\n",
      "1165 episodes completed\n",
      "loss 0.4668003022670746\n",
      "steps done 884140\n",
      "1170 episodes completed\n",
      "loss 0.09131836146116257\n",
      "steps done 887900\n",
      "1175 episodes completed\n",
      "loss 0.029979297891259193\n",
      "steps done 891890\n",
      "1180 episodes completed\n",
      "loss 2.088009834289551\n",
      "steps done 895670\n",
      "1185 episodes completed\n",
      "loss 0.5398238301277161\n",
      "steps done 899530\n",
      "1190 episodes completed\n",
      "loss 0.0018652176950126886\n",
      "steps done 903410\n",
      "1195 episodes completed\n",
      "loss 0.5548223853111267\n",
      "steps done 907000\n",
      "1200 episodes completed\n",
      "loss 0.7565100193023682\n",
      "steps done 910120\n",
      "1205 episodes completed\n",
      "loss 0.7816459536552429\n",
      "steps done 914350\n",
      "1210 episodes completed\n",
      "loss 0.053736548870801926\n",
      "steps done 918650\n",
      "update target\n",
      "1215 episodes completed\n",
      "loss 0.8954881429672241\n",
      "steps done 922300\n",
      "1220 episodes completed\n",
      "loss 0.6217301487922668\n",
      "steps done 926220\n",
      "1225 episodes completed\n",
      "loss 1.2436457872390747\n",
      "steps done 930160\n",
      "1230 episodes completed\n",
      "loss 1.0799124240875244\n",
      "steps done 933810\n",
      "1235 episodes completed\n",
      "loss 0.15214470028877258\n",
      "steps done 937650\n",
      "1240 episodes completed\n",
      "loss 0.0012396704405546188\n",
      "steps done 941290\n",
      "1245 episodes completed\n",
      "loss 0.0011071572080254555\n",
      "steps done 945220\n",
      "1250 episodes completed\n",
      "loss 0.00403812387958169\n",
      "steps done 948910\n",
      "1255 episodes completed\n",
      "loss 0.03781174123287201\n",
      "steps done 952950\n",
      "1260 episodes completed\n",
      "loss 0.0016685186419636011\n",
      "steps done 957520\n",
      "1265 episodes completed\n",
      "loss 0.9084317088127136\n",
      "steps done 961580\n",
      "1270 episodes completed\n",
      "loss 0.03580131381750107\n",
      "steps done 965130\n",
      "1275 episodes completed\n",
      "loss 0.3843866288661957\n",
      "steps done 968610\n",
      "1280 episodes completed\n",
      "loss 0.0004424238868523389\n",
      "steps done 972420\n",
      "1285 episodes completed\n",
      "loss 0.0012600187910720706\n",
      "steps done 975800\n",
      "1290 episodes completed\n",
      "loss 2.630077362060547\n",
      "steps done 979450\n",
      "1295 episodes completed\n",
      "loss 0.01683756709098816\n",
      "steps done 982660\n",
      "1300 episodes completed\n",
      "loss 1.138059377670288\n",
      "steps done 986380\n",
      "1305 episodes completed\n",
      "loss 0.02357776090502739\n",
      "steps done 990150\n",
      "1310 episodes completed\n",
      "loss 0.3841801583766937\n",
      "steps done 993970\n",
      "1315 episodes completed\n",
      "loss 0.0005194994155317545\n",
      "steps done 997860\n",
      "1320 episodes completed\n",
      "loss 0.928689181804657\n",
      "steps done 1001270\n",
      "1325 episodes completed\n",
      "loss 1.2172253131866455\n",
      "steps done 1004680\n",
      "1330 episodes completed\n",
      "loss 0.0007878366159275174\n",
      "steps done 1008020\n",
      "1335 episodes completed\n",
      "loss 0.7999023199081421\n",
      "steps done 1011900\n",
      "1340 episodes completed\n",
      "loss 0.040575869381427765\n",
      "steps done 1014980\n",
      "1345 episodes completed\n",
      "loss 0.0369865745306015\n",
      "steps done 1018820\n",
      "1350 episodes completed\n",
      "loss 0.0022681783884763718\n",
      "steps done 1023280\n",
      "1355 episodes completed\n",
      "loss 0.9817323088645935\n",
      "steps done 1027790\n",
      "1360 episodes completed\n",
      "loss 0.000661964004393667\n",
      "steps done 1031430\n",
      "1365 episodes completed\n",
      "loss 0.00033127854112535715\n",
      "steps done 1035560\n",
      "1370 episodes completed\n",
      "loss 0.014696158468723297\n",
      "steps done 1038940\n",
      "1375 episodes completed\n",
      "loss 0.579664945602417\n",
      "steps done 1043320\n",
      "1380 episodes completed\n",
      "loss 0.783288300037384\n",
      "steps done 1046840\n",
      "1385 episodes completed\n",
      "loss 0.0010131362359970808\n",
      "steps done 1050470\n",
      "1390 episodes completed\n",
      "loss 0.3321946859359741\n",
      "steps done 1054370\n",
      "1395 episodes completed\n",
      "loss 0.0010128063149750233\n",
      "steps done 1057910\n",
      "1400 episodes completed\n",
      "loss 0.4530038833618164\n",
      "steps done 1061170\n",
      "1405 episodes completed\n",
      "loss 0.35108494758605957\n",
      "steps done 1064560\n",
      "1410 episodes completed\n",
      "loss 0.4974502921104431\n",
      "steps done 1069040\n",
      "update target\n",
      "1415 episodes completed\n",
      "loss 1.8522107601165771\n",
      "steps done 1072860\n",
      "1420 episodes completed\n",
      "loss 0.0025164084509015083\n",
      "steps done 1076720\n",
      "1425 episodes completed\n",
      "loss 0.005600752308964729\n",
      "steps done 1079650\n",
      "1430 episodes completed\n",
      "loss 0.9391407370567322\n",
      "steps done 1083110\n",
      "1435 episodes completed\n",
      "loss 0.008864969946444035\n",
      "steps done 1086730\n",
      "1440 episodes completed\n",
      "loss 1.0845048427581787\n",
      "steps done 1090550\n",
      "1445 episodes completed\n",
      "loss 1.058203101158142\n",
      "steps done 1094350\n",
      "1450 episodes completed\n",
      "loss 3.3984861373901367\n",
      "steps done 1098110\n",
      "1455 episodes completed\n",
      "loss 0.021301880478858948\n",
      "steps done 1101910\n",
      "1460 episodes completed\n",
      "loss 1.2671537399291992\n",
      "steps done 1105670\n",
      "1465 episodes completed\n",
      "loss 0.001582722645252943\n",
      "steps done 1109300\n",
      "1470 episodes completed\n",
      "loss 1.9560680389404297\n",
      "steps done 1113240\n",
      "1475 episodes completed\n",
      "loss 0.006477466318756342\n",
      "steps done 1116290\n",
      "1480 episodes completed\n",
      "loss 0.6657320857048035\n",
      "steps done 1119810\n",
      "1485 episodes completed\n",
      "loss 1.617288589477539\n",
      "steps done 1123270\n",
      "1490 episodes completed\n",
      "loss 0.0010750927031040192\n",
      "steps done 1126850\n",
      "1495 episodes completed\n",
      "loss 0.00012226664694026113\n",
      "steps done 1130350\n",
      "1500 episodes completed\n",
      "loss 0.00016407070506829768\n",
      "steps done 1134360\n",
      "1505 episodes completed\n",
      "loss 1.5163319110870361\n",
      "steps done 1138050\n",
      "1510 episodes completed\n",
      "loss 0.0076537542045116425\n",
      "steps done 1141980\n",
      "1515 episodes completed\n",
      "loss 0.014732515439391136\n",
      "steps done 1146310\n",
      "1520 episodes completed\n",
      "loss 0.004137897863984108\n",
      "steps done 1149540\n",
      "1525 episodes completed\n",
      "loss 0.009979254566133022\n",
      "steps done 1152820\n",
      "1530 episodes completed\n",
      "loss 1.0431036949157715\n",
      "steps done 1156560\n",
      "1535 episodes completed\n",
      "loss 1.7372219562530518\n",
      "steps done 1159730\n",
      "1540 episodes completed\n",
      "loss 0.16139578819274902\n",
      "steps done 1163640\n",
      "1545 episodes completed\n",
      "loss 1.0754849910736084\n",
      "steps done 1167360\n",
      "1550 episodes completed\n",
      "loss 0.1504913568496704\n",
      "steps done 1170630\n",
      "1555 episodes completed\n",
      "loss 0.00041448549018241465\n",
      "steps done 1174140\n",
      "1560 episodes completed\n",
      "loss 1.0775415897369385\n",
      "steps done 1177900\n",
      "1565 episodes completed\n",
      "loss 1.146265983581543\n",
      "steps done 1181170\n",
      "1570 episodes completed\n",
      "loss 1.087703824043274\n",
      "steps done 1185550\n",
      "1575 episodes completed\n",
      "loss 0.0037953401915729046\n",
      "steps done 1189280\n",
      "1580 episodes completed\n",
      "loss 0.5946125984191895\n",
      "steps done 1193030\n",
      "1585 episodes completed\n",
      "loss 0.5066399574279785\n",
      "steps done 1196850\n",
      "1590 episodes completed\n",
      "loss 0.0005255279829725623\n",
      "steps done 1200760\n",
      "1595 episodes completed\n",
      "loss 0.004732729867100716\n",
      "steps done 1204740\n",
      "1600 episodes completed\n",
      "loss 0.00538845406845212\n",
      "steps done 1208580\n",
      "1605 episodes completed\n",
      "loss 0.25120675563812256\n",
      "steps done 1212530\n",
      "1610 episodes completed\n",
      "loss 0.0009006096515804529\n",
      "steps done 1216890\n",
      "update target\n",
      "1615 episodes completed\n",
      "loss 1.6687984466552734\n",
      "steps done 1219970\n",
      "1620 episodes completed\n",
      "loss 0.021005485206842422\n",
      "steps done 1223750\n",
      "1625 episodes completed\n",
      "loss 0.001062785624526441\n",
      "steps done 1227290\n",
      "1630 episodes completed\n",
      "loss 0.0008178757270798087\n",
      "steps done 1231330\n",
      "1635 episodes completed\n",
      "loss 0.008046712726354599\n",
      "steps done 1234750\n",
      "1640 episodes completed\n",
      "loss 0.0031491038389503956\n",
      "steps done 1238400\n",
      "1645 episodes completed\n",
      "loss 0.05452035740017891\n",
      "steps done 1241190\n",
      "1650 episodes completed\n",
      "loss 1.5763301849365234\n",
      "steps done 1244950\n",
      "1655 episodes completed\n",
      "loss 0.2628241181373596\n",
      "steps done 1248390\n",
      "1660 episodes completed\n",
      "loss 0.4792119860649109\n",
      "steps done 1252550\n",
      "1665 episodes completed\n",
      "loss 0.2814616560935974\n",
      "steps done 1256130\n",
      "1670 episodes completed\n",
      "loss 1.1072280406951904\n",
      "steps done 1259950\n",
      "1675 episodes completed\n",
      "loss 0.45458367466926575\n",
      "steps done 1264170\n",
      "1680 episodes completed\n",
      "loss 2.4697513580322266\n",
      "steps done 1267980\n",
      "1685 episodes completed\n",
      "loss 1.6100574731826782\n",
      "steps done 1272410\n",
      "1690 episodes completed\n",
      "loss 0.7638274431228638\n",
      "steps done 1276310\n",
      "1695 episodes completed\n",
      "loss 0.0008756894967518747\n",
      "steps done 1280260\n",
      "1700 episodes completed\n",
      "loss 0.057583037763834\n",
      "steps done 1283430\n",
      "1705 episodes completed\n",
      "loss 0.06288889795541763\n",
      "steps done 1286820\n",
      "1710 episodes completed\n",
      "loss 0.7926875948905945\n",
      "steps done 1291130\n",
      "1715 episodes completed\n",
      "loss 0.07375332713127136\n",
      "steps done 1295070\n",
      "1720 episodes completed\n",
      "loss 0.09296288341283798\n",
      "steps done 1298990\n",
      "1725 episodes completed\n",
      "loss 0.0034937472082674503\n",
      "steps done 1302890\n",
      "1730 episodes completed\n",
      "loss 0.2839662432670593\n",
      "steps done 1306450\n",
      "1735 episodes completed\n",
      "loss 0.001546410727314651\n",
      "steps done 1309730\n",
      "1740 episodes completed\n",
      "loss 0.876759946346283\n",
      "steps done 1313830\n",
      "1745 episodes completed\n",
      "loss 0.3891218304634094\n",
      "steps done 1318080\n",
      "1750 episodes completed\n",
      "loss 0.0013843367341905832\n",
      "steps done 1322090\n",
      "1755 episodes completed\n",
      "loss 0.0006867029005661607\n",
      "steps done 1325060\n",
      "1760 episodes completed\n",
      "loss 1.7463897466659546\n",
      "steps done 1328790\n",
      "1765 episodes completed\n",
      "loss 0.0015533606056123972\n",
      "steps done 1333740\n",
      "1770 episodes completed\n",
      "loss 0.0061531029641628265\n",
      "steps done 1337730\n",
      "1775 episodes completed\n",
      "loss 0.9496340155601501\n",
      "steps done 1341550\n",
      "1780 episodes completed\n",
      "loss 2.8847837448120117\n",
      "steps done 1345750\n",
      "1785 episodes completed\n",
      "loss 2.1919987201690674\n",
      "steps done 1349600\n",
      "1790 episodes completed\n",
      "loss 0.001558044576086104\n",
      "steps done 1354300\n",
      "1795 episodes completed\n",
      "loss 0.0010464218212291598\n",
      "steps done 1358220\n",
      "1800 episodes completed\n",
      "loss 0.39349663257598877\n",
      "steps done 1361630\n",
      "1805 episodes completed\n",
      "loss 0.2604747414588928\n",
      "steps done 1365100\n",
      "1810 episodes completed\n",
      "loss 0.0014013670152053237\n",
      "steps done 1368980\n",
      "update target\n",
      "1815 episodes completed\n",
      "loss 1.3609873056411743\n",
      "steps done 1372460\n",
      "1820 episodes completed\n",
      "loss 1.6373846530914307\n",
      "steps done 1376340\n",
      "1825 episodes completed\n",
      "loss 0.0004568669246509671\n",
      "steps done 1379960\n",
      "1830 episodes completed\n",
      "loss 0.9558125734329224\n",
      "steps done 1383370\n",
      "1835 episodes completed\n",
      "loss 0.039848215878009796\n",
      "steps done 1386890\n",
      "1840 episodes completed\n",
      "loss 1.5039362907409668\n",
      "steps done 1390130\n",
      "1845 episodes completed\n",
      "loss 2.375847578048706\n",
      "steps done 1394030\n",
      "1850 episodes completed\n",
      "loss 0.8255324959754944\n",
      "steps done 1396930\n",
      "1855 episodes completed\n",
      "loss 0.004123765043914318\n",
      "steps done 1400510\n",
      "1860 episodes completed\n",
      "loss 0.002815488027408719\n",
      "steps done 1403740\n",
      "1865 episodes completed\n",
      "loss 0.038594093173742294\n",
      "steps done 1407690\n",
      "1870 episodes completed\n",
      "loss 0.03771340101957321\n",
      "steps done 1411730\n",
      "1875 episodes completed\n",
      "loss 0.8487039804458618\n",
      "steps done 1415510\n",
      "1880 episodes completed\n",
      "loss 1.139367699623108\n",
      "steps done 1419260\n",
      "1885 episodes completed\n",
      "loss 1.5221524238586426\n",
      "steps done 1422660\n",
      "1890 episodes completed\n",
      "loss 1.4691424369812012\n",
      "steps done 1426680\n",
      "1895 episodes completed\n",
      "loss 1.7919940948486328\n",
      "steps done 1430250\n",
      "1900 episodes completed\n",
      "loss 1.4794726371765137\n",
      "steps done 1434250\n",
      "1905 episodes completed\n",
      "loss 0.18665443360805511\n",
      "steps done 1437780\n",
      "1910 episodes completed\n",
      "loss 0.048164959996938705\n",
      "steps done 1441760\n",
      "1915 episodes completed\n",
      "loss 0.023638565093278885\n",
      "steps done 1445580\n",
      "1920 episodes completed\n",
      "loss 0.0016766127664595842\n",
      "steps done 1449190\n",
      "1925 episodes completed\n",
      "loss 0.0018536923453211784\n",
      "steps done 1453060\n",
      "1930 episodes completed\n",
      "loss 1.849277377128601\n",
      "steps done 1457310\n",
      "1935 episodes completed\n",
      "loss 0.0007124209660105407\n",
      "steps done 1461490\n",
      "1940 episodes completed\n",
      "loss 0.011582927778363228\n",
      "steps done 1465630\n",
      "1945 episodes completed\n",
      "loss 1.0594894886016846\n",
      "steps done 1469020\n",
      "1950 episodes completed\n",
      "loss 0.07813601195812225\n",
      "steps done 1472900\n",
      "1955 episodes completed\n",
      "loss 0.07511139661073685\n",
      "steps done 1476090\n",
      "1960 episodes completed\n",
      "loss 0.11494226008653641\n",
      "steps done 1479790\n",
      "1965 episodes completed\n",
      "loss 0.9907984733581543\n",
      "steps done 1483600\n",
      "1970 episodes completed\n",
      "loss 0.0013444798532873392\n",
      "steps done 1487490\n",
      "1975 episodes completed\n",
      "loss 0.004505047108978033\n",
      "steps done 1490760\n",
      "1980 episodes completed\n",
      "loss 1.4590851068496704\n",
      "steps done 1494730\n",
      "1985 episodes completed\n",
      "loss 0.0009287201683036983\n",
      "steps done 1498110\n",
      "1990 episodes completed\n",
      "loss 0.9073693752288818\n",
      "steps done 1501920\n",
      "1995 episodes completed\n",
      "loss 0.0038954047486186028\n",
      "steps done 1506470\n",
      "2000 episodes completed\n",
      "loss 1.2304301261901855\n",
      "steps done 1509610\n",
      "2005 episodes completed\n",
      "loss 0.022194083780050278\n",
      "steps done 1513000\n",
      "2010 episodes completed\n",
      "loss 0.002950157504528761\n",
      "steps done 1516350\n",
      "update target\n",
      "2015 episodes completed\n",
      "loss 1.2172021865844727\n",
      "steps done 1520540\n",
      "2020 episodes completed\n",
      "loss 0.017928801476955414\n",
      "steps done 1524890\n",
      "2025 episodes completed\n",
      "loss 0.5355303287506104\n",
      "steps done 1528790\n",
      "2030 episodes completed\n",
      "loss 1.308681845664978\n",
      "steps done 1533050\n",
      "2035 episodes completed\n",
      "loss 0.001308204373344779\n",
      "steps done 1536730\n",
      "2040 episodes completed\n",
      "loss 0.0022338652051985264\n",
      "steps done 1540730\n",
      "2045 episodes completed\n",
      "loss 0.6445435881614685\n",
      "steps done 1544340\n",
      "2050 episodes completed\n",
      "loss 0.025454888120293617\n",
      "steps done 1548110\n",
      "2055 episodes completed\n",
      "loss 1.339370608329773\n",
      "steps done 1552690\n",
      "2060 episodes completed\n",
      "loss 9.218964260071516e-05\n",
      "steps done 1556770\n",
      "2065 episodes completed\n",
      "loss 0.000309133087284863\n",
      "steps done 1560510\n",
      "2070 episodes completed\n",
      "loss 0.02769281342625618\n",
      "steps done 1563690\n",
      "2075 episodes completed\n",
      "loss 0.19886933267116547\n",
      "steps done 1567950\n",
      "2080 episodes completed\n",
      "loss 1.7513751983642578\n",
      "steps done 1572120\n",
      "2085 episodes completed\n",
      "loss 1.195128083229065\n",
      "steps done 1575990\n",
      "2090 episodes completed\n",
      "loss 1.9732825756072998\n",
      "steps done 1580370\n",
      "2095 episodes completed\n",
      "loss 0.04551929980516434\n",
      "steps done 1584980\n",
      "2100 episodes completed\n",
      "loss 0.007803850807249546\n",
      "steps done 1589370\n",
      "2105 episodes completed\n",
      "loss 0.006741513032466173\n",
      "steps done 1592840\n",
      "2110 episodes completed\n",
      "loss 1.5608375072479248\n",
      "steps done 1596530\n",
      "2115 episodes completed\n",
      "loss 1.5441842079162598\n",
      "steps done 1599980\n",
      "2120 episodes completed\n",
      "loss 0.0015905261971056461\n",
      "steps done 1603590\n",
      "2125 episodes completed\n",
      "loss 0.04510899633169174\n",
      "steps done 1607170\n",
      "2130 episodes completed\n",
      "loss 1.185551643371582\n",
      "steps done 1610760\n",
      "2135 episodes completed\n",
      "loss 0.01053845975548029\n",
      "steps done 1614180\n",
      "2140 episodes completed\n",
      "loss 0.051673755049705505\n",
      "steps done 1618360\n",
      "2145 episodes completed\n",
      "loss 0.0017022809479385614\n",
      "steps done 1621840\n",
      "2150 episodes completed\n",
      "loss 0.007045106962323189\n",
      "steps done 1625680\n",
      "2155 episodes completed\n",
      "loss 0.0009259876096621156\n",
      "steps done 1630020\n",
      "2160 episodes completed\n",
      "loss 0.0015548693481832743\n",
      "steps done 1633860\n",
      "2165 episodes completed\n",
      "loss 0.003307717852294445\n",
      "steps done 1637510\n",
      "2170 episodes completed\n",
      "loss 0.9461097717285156\n",
      "steps done 1641300\n",
      "2175 episodes completed\n",
      "loss 0.0016169887967407703\n",
      "steps done 1644940\n",
      "2180 episodes completed\n",
      "loss 0.008954696357250214\n",
      "steps done 1648910\n",
      "2185 episodes completed\n",
      "loss 1.1932460069656372\n",
      "steps done 1653420\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://nwcn8ey3iw.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "observation = fireEnv.reset()\n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "episode_memory_1 = []\n",
    "episode_memory_2 = []\n",
    "\n",
    "hidden_1 = policy_net.init_hidden_state()\n",
    "hidden_2 = policy_net.init_hidden_state()\n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "  for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "    observation = fireEnv.step()\n",
    "\n",
    "    state_vector_1 = dronesEnv.drones[0].state\n",
    "    map_1 = dronesEnv.drones[0].observation\n",
    "    state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "    map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_2 = dronesEnv.drones[1].state\n",
    "    map_2 = dronesEnv.drones[1].observation\n",
    "    state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "    for i in range(int(DT/DTI)):\n",
    "\n",
    "      action1, hidden_1 = policy_net.select_action(map_1, state_vector_1, steps, hidden=hidden_1)\n",
    "      action2, hidden_2 = policy_net.select_action(map_2, state_vector_2, steps, hidden=hidden_2)\n",
    "      steps += 2\n",
    "      reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "      next_state_vector_1 = dronesEnv.drones[0].state\n",
    "      next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "      next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "      next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "      next_state_vector_2 = dronesEnv.drones[1].state\n",
    "      next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "      next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "      next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "      reward_1 = torch.tensor([reward_1], device=device)\n",
    "      reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "      episode_memory_1.append(Transition(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1))\n",
    "      episode_memory_2.append(Transition(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2))\n",
    "\n",
    "      state_vector_1 = next_state_vector_1\n",
    "      state_vector_2 = next_state_vector_2\n",
    "\n",
    "      map_1 = next_map_1\n",
    "      map_2 = next_map_2\n",
    "\n",
    "    if i_episode>=INIT_SIZE:\n",
    "      loss = optimize_model()\n",
    "      \n",
    "    if not fireEnv.fire_in_range(6):\n",
    "      observation = fireEnv.reset()\n",
    "      dronesEnv.reset(observation)\n",
    "      episode_buffer.push(episode_memory_1.copy())\n",
    "      episode_buffer.push(episode_memory_2.copy())\n",
    "      episode_memory_1 = []\n",
    "      episode_memory_2 = []\n",
    "      hidden_1 = policy_net.init_hidden_state()\n",
    "      hidden_2 = policy_net.init_hidden_state()\n",
    "      i_episode +=1\n",
    "      \n",
    "      if (i_episode+1) % 5 == 0:\n",
    "        print(f'{i_episode+1} episodes completed')\n",
    "        print(f'loss {loss}')\n",
    "        print(f'steps done {steps}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
