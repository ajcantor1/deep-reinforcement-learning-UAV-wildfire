{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgpath2mpl\n",
      "  Downloading svgpath2mpl-1.0.0-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "Installing collected packages: svgpath2mpl\n",
      "Successfully installed svgpath2mpl-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from replay_memory import Transition\n",
    "from networks.drqn import DRQN\n",
    "from episode_buffer import EpisodeBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "BATCH_SIZE = 5\n",
    "GAMMA = 0.99\n",
    "INIT_SIZE = 5\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DRQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DRQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "episode_buffer = EpisodeBuffer()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "EPISODE_LENGTH = 128\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    loss = None\n",
    "    \n",
    "    global update_counter\n",
    "\n",
    "    episode_batch, epiosde_length = episode_buffer.sample(EPISODE_LENGTH)\n",
    "\n",
    "    update_counter += 1\n",
    "    batch = Transition(*zip(*episode_batch))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    hidden_policy = policy_net.init_hidden_state()\n",
    "    hidden_target = target_net.init_hidden_state()\n",
    "\n",
    "    policy_output, _ = policy_net(belief_map_batch, state_vector_batch, hidden_policy)\n",
    "    target_output, _ = target_net(next_belief_map, next_states, hidden_target)\n",
    "\n",
    "    state_action_values = policy_output.gather(1, action_batch)\n",
    "    next_state_values = target_output.max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights_drqn.pt'\n",
    "        target_file_path = f'./target_weights_drqn.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episodes completed\n",
      "loss None\n",
      "steps done 2070\n",
      "10 episodes completed\n",
      "loss 0.0020916499197483063\n",
      "steps done 6010\n",
      "15 episodes completed\n",
      "loss 0.17738160490989685\n",
      "steps done 10290\n",
      "update target\n",
      "20 episodes completed\n",
      "loss 1.1363341808319092\n",
      "steps done 14060\n",
      "25 episodes completed\n",
      "loss 0.00010166229185415432\n",
      "steps done 17820\n",
      "30 episodes completed\n",
      "loss 0.18423466384410858\n",
      "steps done 21220\n",
      "update target\n",
      "35 episodes completed\n",
      "loss 0.0004083088133484125\n",
      "steps done 25030\n",
      "40 episodes completed\n",
      "loss 1.2048654556274414\n",
      "steps done 28910\n",
      "update target\n",
      "45 episodes completed\n",
      "loss 2.273698091506958\n",
      "steps done 33010\n",
      "50 episodes completed\n",
      "loss 1.174837350845337\n",
      "steps done 35880\n",
      "55 episodes completed\n",
      "loss 0.00740866269916296\n",
      "steps done 39610\n",
      "update target\n",
      "60 episodes completed\n",
      "loss 0.00013605505228042603\n",
      "steps done 43600\n",
      "65 episodes completed\n",
      "loss 0.002227463060989976\n",
      "steps done 47050\n",
      "70 episodes completed\n",
      "loss 0.7172749042510986\n",
      "steps done 51410\n",
      "update target\n",
      "75 episodes completed\n",
      "loss 0.017545830458402634\n",
      "steps done 55640\n",
      "80 episodes completed\n",
      "loss 0.04088179022073746\n",
      "steps done 59280\n",
      "update target\n",
      "85 episodes completed\n",
      "loss 0.3340660035610199\n",
      "steps done 63310\n",
      "90 episodes completed\n",
      "loss 0.004794785752892494\n",
      "steps done 68040\n",
      "95 episodes completed\n",
      "loss 0.06768403202295303\n",
      "steps done 72490\n",
      "update target\n",
      "100 episodes completed\n",
      "loss 0.09769104421138763\n",
      "steps done 75780\n",
      "105 episodes completed\n",
      "loss 0.21089819073677063\n",
      "steps done 79510\n",
      "110 episodes completed\n",
      "loss 0.09245656430721283\n",
      "steps done 82750\n",
      "update target\n",
      "115 episodes completed\n",
      "loss 0.120160311460495\n",
      "steps done 86280\n",
      "120 episodes completed\n",
      "loss 0.32072263956069946\n",
      "steps done 90630\n",
      "update target\n",
      "125 episodes completed\n",
      "loss 1.8091633319854736\n",
      "steps done 95360\n",
      "130 episodes completed\n",
      "loss 1.1808384656906128\n",
      "steps done 99390\n",
      "update target\n",
      "135 episodes completed\n",
      "loss 0.015246352180838585\n",
      "steps done 103160\n",
      "140 episodes completed\n",
      "loss 2.6792688369750977\n",
      "steps done 107710\n",
      "145 episodes completed\n",
      "loss 0.10669393092393875\n",
      "steps done 111640\n",
      "update target\n",
      "150 episodes completed\n",
      "loss 0.006893345154821873\n",
      "steps done 115440\n",
      "155 episodes completed\n",
      "loss 0.0023664594627916813\n",
      "steps done 119030\n",
      "160 episodes completed\n",
      "loss 0.010479754768311977\n",
      "steps done 122610\n",
      "update target\n",
      "165 episodes completed\n",
      "loss 0.002564895898103714\n",
      "steps done 125880\n",
      "170 episodes completed\n",
      "loss 0.28084611892700195\n",
      "steps done 129130\n",
      "175 episodes completed\n",
      "loss 2.2630510330200195\n",
      "steps done 132380\n",
      "update target\n",
      "180 episodes completed\n",
      "loss 0.005590498447418213\n",
      "steps done 135930\n",
      "185 episodes completed\n",
      "loss 0.005495819263160229\n",
      "steps done 140060\n",
      "update target\n",
      "190 episodes completed\n",
      "loss 0.8459336161613464\n",
      "steps done 143670\n",
      "195 episodes completed\n",
      "loss 2.047595977783203\n",
      "steps done 147190\n",
      "200 episodes completed\n",
      "loss 0.8570429086685181\n",
      "steps done 151420\n",
      "update target\n",
      "205 episodes completed\n",
      "loss 2.0915462970733643\n",
      "steps done 154850\n",
      "210 episodes completed\n",
      "loss 0.0024099363945424557\n",
      "steps done 158750\n",
      "215 episodes completed\n",
      "loss 0.20753206312656403\n",
      "steps done 161790\n",
      "update target\n",
      "220 episodes completed\n",
      "loss 0.01842574030160904\n",
      "steps done 165830\n",
      "225 episodes completed\n",
      "loss 0.020560644567012787\n",
      "steps done 170230\n",
      "update target\n",
      "230 episodes completed\n",
      "loss 2.3627967834472656\n",
      "steps done 174140\n",
      "235 episodes completed\n",
      "loss 0.039903171360492706\n",
      "steps done 178360\n",
      "240 episodes completed\n",
      "loss 1.9237812757492065\n",
      "steps done 182070\n",
      "update target\n",
      "245 episodes completed\n",
      "loss 0.601677656173706\n",
      "steps done 185450\n",
      "250 episodes completed\n",
      "loss 0.4321441352367401\n",
      "steps done 189320\n",
      "255 episodes completed\n",
      "loss 1.675799012184143\n",
      "steps done 192620\n",
      "update target\n",
      "260 episodes completed\n",
      "loss 0.027043264359235764\n",
      "steps done 196060\n",
      "265 episodes completed\n",
      "loss 0.00756462849676609\n",
      "steps done 199160\n",
      "update target\n",
      "270 episodes completed\n",
      "loss 0.6178315281867981\n",
      "steps done 202860\n",
      "275 episodes completed\n",
      "loss 0.08409766852855682\n",
      "steps done 206650\n",
      "280 episodes completed\n",
      "loss 0.002643425250425935\n",
      "steps done 209940\n",
      "update target\n",
      "285 episodes completed\n",
      "loss 0.07305227965116501\n",
      "steps done 214340\n",
      "290 episodes completed\n",
      "loss 0.020866530016064644\n",
      "steps done 218070\n",
      "update target\n",
      "295 episodes completed\n",
      "loss 3.777283191680908\n",
      "steps done 222860\n",
      "300 episodes completed\n",
      "loss 0.0036580823361873627\n",
      "steps done 226370\n",
      "305 episodes completed\n",
      "loss 0.017307989299297333\n",
      "steps done 229910\n",
      "update target\n",
      "310 episodes completed\n",
      "loss 0.005069916136562824\n",
      "steps done 233250\n",
      "315 episodes completed\n",
      "loss 0.004781888797879219\n",
      "steps done 236840\n",
      "320 episodes completed\n",
      "loss 2.5095813274383545\n",
      "steps done 240530\n",
      "update target\n",
      "325 episodes completed\n",
      "loss 2.366119384765625\n",
      "steps done 244780\n",
      "330 episodes completed\n",
      "loss 5.20302677154541\n",
      "steps done 248360\n",
      "335 episodes completed\n",
      "loss 2.3774728775024414\n",
      "steps done 252050\n",
      "update target\n",
      "340 episodes completed\n",
      "loss 0.017093300819396973\n",
      "steps done 255900\n",
      "345 episodes completed\n",
      "loss 3.3077101707458496\n",
      "steps done 259490\n",
      "update target\n",
      "350 episodes completed\n",
      "loss 0.018995821475982666\n",
      "steps done 262810\n",
      "355 episodes completed\n",
      "loss 4.113509654998779\n",
      "steps done 266300\n",
      "360 episodes completed\n",
      "loss 2.136319398880005\n",
      "steps done 269910\n",
      "update target\n",
      "365 episodes completed\n",
      "loss 0.04079073667526245\n",
      "steps done 273670\n",
      "370 episodes completed\n",
      "loss 4.2240471839904785\n",
      "steps done 277110\n",
      "375 episodes completed\n",
      "loss 7.217329502105713\n",
      "steps done 280810\n",
      "update target\n",
      "380 episodes completed\n",
      "loss 3.80224609375\n",
      "steps done 284340\n",
      "385 episodes completed\n",
      "loss 3.5250966548919678\n",
      "steps done 288310\n",
      "390 episodes completed\n",
      "loss 0.48416048288345337\n",
      "steps done 292470\n",
      "update target\n",
      "395 episodes completed\n",
      "loss 2.505645990371704\n",
      "steps done 296150\n",
      "400 episodes completed\n",
      "loss 3.318953037261963\n",
      "steps done 299780\n",
      "update target\n",
      "405 episodes completed\n",
      "loss 4.389963626861572\n",
      "steps done 303820\n",
      "410 episodes completed\n",
      "loss 3.853875160217285\n",
      "steps done 307820\n",
      "415 episodes completed\n",
      "loss 6.6005401611328125\n",
      "steps done 311480\n",
      "update target\n",
      "420 episodes completed\n",
      "loss 6.720041751861572\n",
      "steps done 315340\n",
      "425 episodes completed\n",
      "loss 0.1402832716703415\n",
      "steps done 319150\n",
      "update target\n",
      "430 episodes completed\n",
      "loss 4.204523086547852\n",
      "steps done 322880\n",
      "435 episodes completed\n",
      "loss 5.440315246582031\n",
      "steps done 326990\n",
      "440 episodes completed\n",
      "loss 3.930910110473633\n",
      "steps done 330860\n",
      "update target\n",
      "445 episodes completed\n",
      "loss 4.192898273468018\n",
      "steps done 334690\n",
      "450 episodes completed\n",
      "loss 0.6892091631889343\n",
      "steps done 338180\n",
      "455 episodes completed\n",
      "loss 0.0011175461113452911\n",
      "steps done 341810\n",
      "update target\n",
      "460 episodes completed\n",
      "loss 5.997430801391602\n",
      "steps done 345720\n",
      "465 episodes completed\n",
      "loss 3.63750958442688\n",
      "steps done 349940\n",
      "update target\n",
      "470 episodes completed\n",
      "loss 7.18153190612793\n",
      "steps done 353180\n",
      "475 episodes completed\n",
      "loss 4.21629524230957\n",
      "steps done 356680\n",
      "480 episodes completed\n",
      "loss 4.30465841293335\n",
      "steps done 360190\n",
      "update target\n",
      "485 episodes completed\n",
      "loss 5.130557060241699\n",
      "steps done 363760\n",
      "490 episodes completed\n",
      "loss 1.3682405948638916\n",
      "steps done 367630\n",
      "495 episodes completed\n",
      "loss 2.8767831325531006\n",
      "steps done 371170\n",
      "update target\n",
      "500 episodes completed\n",
      "loss 0.3436262905597687\n",
      "steps done 374810\n",
      "505 episodes completed\n",
      "loss 3.8562018871307373\n",
      "steps done 379210\n",
      "update target\n",
      "510 episodes completed\n",
      "loss 7.305886268615723\n",
      "steps done 383130\n",
      "515 episodes completed\n",
      "loss 3.946925163269043\n",
      "steps done 386400\n",
      "520 episodes completed\n",
      "loss 2.9977545738220215\n",
      "steps done 390080\n",
      "update target\n",
      "525 episodes completed\n",
      "loss 2.587576389312744\n",
      "steps done 393920\n",
      "530 episodes completed\n",
      "loss 3.2137551307678223\n",
      "steps done 398430\n",
      "update target\n",
      "535 episodes completed\n",
      "loss 4.507757186889648\n",
      "steps done 402820\n",
      "540 episodes completed\n",
      "loss 4.624767303466797\n",
      "steps done 406660\n",
      "545 episodes completed\n",
      "loss 0.7308669686317444\n",
      "steps done 409720\n",
      "update target\n",
      "550 episodes completed\n",
      "loss 5.2517876625061035\n",
      "steps done 413060\n",
      "555 episodes completed\n",
      "loss 4.454856872558594\n",
      "steps done 416330\n",
      "560 episodes completed\n",
      "loss 5.894013404846191\n",
      "steps done 419730\n",
      "update target\n",
      "565 episodes completed\n",
      "loss 4.776337623596191\n",
      "steps done 423960\n",
      "570 episodes completed\n",
      "loss 3.505910634994507\n",
      "steps done 428130\n",
      "575 episodes completed\n",
      "loss 4.095903396606445\n",
      "steps done 432030\n",
      "update target\n",
      "580 episodes completed\n",
      "loss 4.011126518249512\n",
      "steps done 436080\n",
      "585 episodes completed\n",
      "loss 3.868414878845215\n",
      "steps done 439820\n",
      "update target\n",
      "590 episodes completed\n",
      "loss 4.210821628570557\n",
      "steps done 444100\n",
      "595 episodes completed\n",
      "loss 4.358309745788574\n",
      "steps done 447460\n",
      "600 episodes completed\n",
      "loss 0.4348500370979309\n",
      "steps done 450510\n",
      "update target\n",
      "605 episodes completed\n",
      "loss 0.8521337509155273\n",
      "steps done 454320\n",
      "610 episodes completed\n",
      "loss 3.0770537853240967\n",
      "steps done 457370\n",
      "615 episodes completed\n",
      "loss 5.078736305236816\n",
      "steps done 461370\n",
      "update target\n",
      "620 episodes completed\n",
      "loss 1.8095009326934814\n",
      "steps done 465300\n",
      "625 episodes completed\n",
      "loss 7.043907642364502\n",
      "steps done 469640\n",
      "update target\n",
      "630 episodes completed\n",
      "loss 0.13312366604804993\n",
      "steps done 473930\n",
      "635 episodes completed\n",
      "loss 3.7859232425689697\n",
      "steps done 478160\n",
      "640 episodes completed\n",
      "loss 3.5947253704071045\n",
      "steps done 482050\n",
      "update target\n",
      "645 episodes completed\n",
      "loss 0.04710827395319939\n",
      "steps done 485600\n",
      "650 episodes completed\n",
      "loss 8.297989845275879\n",
      "steps done 491220\n",
      "update target\n",
      "655 episodes completed\n",
      "loss 1.4746677875518799\n",
      "steps done 495760\n",
      "660 episodes completed\n",
      "loss 0.6173809766769409\n",
      "steps done 499870\n",
      "update target\n",
      "665 episodes completed\n",
      "loss 0.37997737526893616\n",
      "steps done 503850\n",
      "670 episodes completed\n",
      "loss 1.7287788391113281\n",
      "steps done 507550\n",
      "675 episodes completed\n",
      "loss 4.765058517456055\n",
      "steps done 511500\n",
      "update target\n",
      "680 episodes completed\n",
      "loss 0.016067376360297203\n",
      "steps done 515370\n",
      "685 episodes completed\n",
      "loss 1.7801486253738403\n",
      "steps done 518650\n",
      "690 episodes completed\n",
      "loss 0.604405403137207\n",
      "steps done 522770\n",
      "update target\n",
      "695 episodes completed\n",
      "loss 2.666566848754883\n",
      "steps done 526650\n",
      "700 episodes completed\n",
      "loss 4.937647342681885\n",
      "steps done 530020\n",
      "update target\n",
      "705 episodes completed\n",
      "loss 0.010908704251050949\n",
      "steps done 533840\n",
      "710 episodes completed\n",
      "loss 4.313520908355713\n",
      "steps done 537180\n",
      "715 episodes completed\n",
      "loss 2.874239444732666\n",
      "steps done 540330\n",
      "update target\n",
      "720 episodes completed\n",
      "loss 2.918313980102539\n",
      "steps done 544790\n",
      "725 episodes completed\n",
      "loss 4.226642608642578\n",
      "steps done 548010\n",
      "730 episodes completed\n",
      "loss 6.681987762451172\n",
      "steps done 551950\n",
      "update target\n",
      "735 episodes completed\n",
      "loss 0.28924649953842163\n",
      "steps done 556320\n",
      "740 episodes completed\n",
      "loss 3.8348898887634277\n",
      "steps done 560640\n",
      "update target\n",
      "745 episodes completed\n",
      "loss 0.03953950107097626\n",
      "steps done 564610\n",
      "750 episodes completed\n",
      "loss 0.45585763454437256\n",
      "steps done 568390\n",
      "755 episodes completed\n",
      "loss 0.055849332362413406\n",
      "steps done 571790\n",
      "update target\n",
      "760 episodes completed\n",
      "loss 0.2626951038837433\n",
      "steps done 575620\n",
      "765 episodes completed\n",
      "loss 1.784487009048462\n",
      "steps done 579010\n",
      "770 episodes completed\n",
      "loss 4.907705307006836\n",
      "steps done 582630\n",
      "update target\n",
      "775 episodes completed\n",
      "loss 6.012516498565674\n",
      "steps done 586550\n",
      "780 episodes completed\n",
      "loss 3.096479892730713\n",
      "steps done 589890\n",
      "update target\n",
      "785 episodes completed\n",
      "loss 3.161226749420166\n",
      "steps done 593840\n",
      "790 episodes completed\n",
      "loss 4.359156131744385\n",
      "steps done 597910\n",
      "795 episodes completed\n",
      "loss 5.020784854888916\n",
      "steps done 601440\n",
      "update target\n",
      "800 episodes completed\n",
      "loss 6.158181667327881\n",
      "steps done 605390\n",
      "805 episodes completed\n",
      "loss 0.009028438478708267\n",
      "steps done 609040\n",
      "810 episodes completed\n",
      "loss 5.870959758758545\n",
      "steps done 612530\n",
      "update target\n",
      "815 episodes completed\n",
      "loss 2.7022626399993896\n",
      "steps done 616250\n",
      "820 episodes completed\n",
      "loss 4.522726058959961\n",
      "steps done 620320\n",
      "update target\n",
      "825 episodes completed\n",
      "loss 0.050387751311063766\n",
      "steps done 624420\n",
      "830 episodes completed\n",
      "loss 1.4163240194320679\n",
      "steps done 628430\n",
      "835 episodes completed\n",
      "loss 4.283568382263184\n",
      "steps done 632330\n",
      "update target\n",
      "840 episodes completed\n",
      "loss 5.095099449157715\n",
      "steps done 636680\n",
      "845 episodes completed\n",
      "loss 2.859370470046997\n",
      "steps done 640370\n",
      "update target\n",
      "850 episodes completed\n",
      "loss 2.9789047241210938\n",
      "steps done 644150\n",
      "855 episodes completed\n",
      "loss 3.9659061431884766\n",
      "steps done 648240\n",
      "860 episodes completed\n",
      "loss 4.743143081665039\n",
      "steps done 652330\n",
      "update target\n",
      "865 episodes completed\n",
      "loss 5.52717399597168\n",
      "steps done 656650\n",
      "870 episodes completed\n",
      "loss 4.910238265991211\n",
      "steps done 660330\n",
      "update target\n",
      "875 episodes completed\n",
      "loss 0.01902182400226593\n",
      "steps done 664480\n",
      "880 episodes completed\n",
      "loss 0.008901005610823631\n",
      "steps done 668410\n",
      "update target\n",
      "885 episodes completed\n",
      "loss 6.688835144042969\n",
      "steps done 672910\n",
      "890 episodes completed\n",
      "loss 0.08756076544523239\n",
      "steps done 676660\n",
      "895 episodes completed\n",
      "loss 0.012715897522866726\n",
      "steps done 679950\n",
      "update target\n",
      "900 episodes completed\n",
      "loss 5.6932148933410645\n",
      "steps done 683790\n",
      "905 episodes completed\n",
      "loss 0.6566213965415955\n",
      "steps done 688190\n",
      "910 episodes completed\n",
      "loss 0.07939749956130981\n",
      "steps done 692310\n",
      "update target\n",
      "915 episodes completed\n",
      "loss 6.304864406585693\n",
      "steps done 696130\n",
      "920 episodes completed\n",
      "loss 3.287290096282959\n",
      "steps done 700460\n",
      "update target\n",
      "925 episodes completed\n",
      "loss 4.224639892578125\n",
      "steps done 704160\n",
      "930 episodes completed\n",
      "loss 5.658958435058594\n",
      "steps done 707580\n",
      "935 episodes completed\n",
      "loss 3.5098767280578613\n",
      "steps done 711260\n",
      "update target\n",
      "940 episodes completed\n",
      "loss 0.006654706783592701\n",
      "steps done 714710\n",
      "945 episodes completed\n",
      "loss 5.253765106201172\n",
      "steps done 718700\n",
      "950 episodes completed\n",
      "loss 4.893531799316406\n",
      "steps done 722370\n",
      "update target\n",
      "955 episodes completed\n",
      "loss 4.912128925323486\n",
      "steps done 726200\n",
      "960 episodes completed\n",
      "loss 3.9136154651641846\n",
      "steps done 729790\n",
      "update target\n",
      "965 episodes completed\n",
      "loss 6.233093738555908\n",
      "steps done 734520\n",
      "970 episodes completed\n",
      "loss 3.727780342102051\n",
      "steps done 737950\n",
      "975 episodes completed\n",
      "loss 3.8768725395202637\n",
      "steps done 741920\n",
      "update target\n",
      "980 episodes completed\n",
      "loss 4.760586738586426\n",
      "steps done 745820\n",
      "985 episodes completed\n",
      "loss 3.988894462585449\n",
      "steps done 749740\n",
      "update target\n",
      "990 episodes completed\n",
      "loss 0.02072450891137123\n",
      "steps done 753590\n",
      "995 episodes completed\n",
      "loss 6.271548271179199\n",
      "steps done 757420\n",
      "1000 episodes completed\n",
      "loss 7.297749996185303\n",
      "steps done 760850\n",
      "update target\n",
      "1005 episodes completed\n",
      "loss 4.628089904785156\n",
      "steps done 764980\n",
      "1010 episodes completed\n",
      "loss 4.575197219848633\n",
      "steps done 768430\n",
      "1015 episodes completed\n",
      "loss 3.1011128425598145\n",
      "steps done 772030\n",
      "update target\n",
      "1020 episodes completed\n",
      "loss 7.660871505737305\n",
      "steps done 775400\n",
      "1025 episodes completed\n",
      "loss 3.161104440689087\n",
      "steps done 779160\n",
      "update target\n",
      "1030 episodes completed\n",
      "loss 3.8667099475860596\n",
      "steps done 783100\n",
      "1035 episodes completed\n",
      "loss 3.887111186981201\n",
      "steps done 787140\n",
      "1040 episodes completed\n",
      "loss 4.232517242431641\n",
      "steps done 791030\n",
      "update target\n",
      "1045 episodes completed\n",
      "loss 4.2851057052612305\n",
      "steps done 794660\n",
      "1050 episodes completed\n",
      "loss 3.335038185119629\n",
      "steps done 798100\n",
      "1055 episodes completed\n",
      "loss 2.73250412940979\n",
      "steps done 801720\n",
      "update target\n",
      "1060 episodes completed\n",
      "loss 2.8671414852142334\n",
      "steps done 805700\n",
      "1065 episodes completed\n",
      "loss 0.8733670711517334\n",
      "steps done 809430\n",
      "1070 episodes completed\n",
      "loss 3.4591469764709473\n",
      "steps done 812610\n",
      "update target\n",
      "1075 episodes completed\n",
      "loss 6.883347034454346\n",
      "steps done 816590\n",
      "1080 episodes completed\n",
      "loss 3.8518476486206055\n",
      "steps done 820140\n",
      "update target\n",
      "1085 episodes completed\n",
      "loss 2.8498175144195557\n",
      "steps done 823770\n",
      "1090 episodes completed\n",
      "loss 3.3464462757110596\n",
      "steps done 828190\n",
      "1095 episodes completed\n",
      "loss 0.22385016083717346\n",
      "steps done 832110\n",
      "update target\n",
      "1100 episodes completed\n",
      "loss 4.740096569061279\n",
      "steps done 836000\n",
      "1105 episodes completed\n",
      "loss 5.602895259857178\n",
      "steps done 839320\n",
      "1110 episodes completed\n",
      "loss 2.4107260704040527\n",
      "steps done 842460\n",
      "update target\n",
      "1115 episodes completed\n",
      "loss 4.220235824584961\n",
      "steps done 847120\n",
      "1120 episodes completed\n",
      "loss 2.570110559463501\n",
      "steps done 850850\n",
      "update target\n",
      "1125 episodes completed\n",
      "loss 2.48455810546875\n",
      "steps done 854160\n",
      "1130 episodes completed\n",
      "loss 3.3903112411499023\n",
      "steps done 858420\n",
      "1135 episodes completed\n",
      "loss 5.957934379577637\n",
      "steps done 861870\n",
      "update target\n",
      "1140 episodes completed\n",
      "loss 6.465164661407471\n",
      "steps done 866380\n",
      "1145 episodes completed\n",
      "loss 7.494966506958008\n",
      "steps done 870340\n",
      "update target\n",
      "1150 episodes completed\n",
      "loss 2.956057548522949\n",
      "steps done 873950\n",
      "1155 episodes completed\n",
      "loss 5.651404857635498\n",
      "steps done 878430\n",
      "1160 episodes completed\n",
      "loss 8.917495727539062\n",
      "steps done 881930\n",
      "update target\n",
      "1165 episodes completed\n",
      "loss 5.382304668426514\n",
      "steps done 885570\n",
      "1170 episodes completed\n",
      "loss 1.8729040622711182\n",
      "steps done 889310\n",
      "update target\n",
      "1175 episodes completed\n",
      "loss 3.4456992149353027\n",
      "steps done 893260\n",
      "1180 episodes completed\n",
      "loss 3.001875400543213\n",
      "steps done 896560\n",
      "1185 episodes completed\n",
      "loss 3.774843692779541\n",
      "steps done 900090\n",
      "update target\n",
      "1190 episodes completed\n",
      "loss 4.067873477935791\n",
      "steps done 904370\n",
      "1195 episodes completed\n",
      "loss 4.186811923980713\n",
      "steps done 907630\n",
      "1200 episodes completed\n",
      "loss 4.926032543182373\n",
      "steps done 910650\n",
      "update target\n",
      "1205 episodes completed\n",
      "loss 3.9925737380981445\n",
      "steps done 914190\n",
      "1210 episodes completed\n",
      "loss 3.309079170227051\n",
      "steps done 918530\n",
      "1215 episodes completed\n",
      "loss 4.169755935668945\n",
      "steps done 922500\n",
      "update target\n",
      "1220 episodes completed\n",
      "loss 5.4507951736450195\n",
      "steps done 926540\n",
      "1225 episodes completed\n",
      "loss 4.588043212890625\n",
      "steps done 930470\n",
      "update target\n",
      "1230 episodes completed\n",
      "loss 1.8057265281677246\n",
      "steps done 934340\n",
      "1235 episodes completed\n",
      "loss 4.562366485595703\n",
      "steps done 937730\n",
      "1240 episodes completed\n",
      "loss 3.4548330307006836\n",
      "steps done 941020\n",
      "update target\n",
      "1245 episodes completed\n",
      "loss 4.487987518310547\n",
      "steps done 944840\n",
      "1250 episodes completed\n",
      "loss 5.195998191833496\n",
      "steps done 948560\n",
      "1255 episodes completed\n",
      "loss 4.129608154296875\n",
      "steps done 952290\n",
      "update target\n",
      "1260 episodes completed\n",
      "loss 1.2503708600997925\n",
      "steps done 955430\n",
      "1265 episodes completed\n",
      "loss 9.074853897094727\n",
      "steps done 958620\n",
      "update target\n",
      "1270 episodes completed\n",
      "loss 4.301279067993164\n",
      "steps done 962870\n",
      "1275 episodes completed\n",
      "loss 4.418889045715332\n",
      "steps done 966270\n",
      "1280 episodes completed\n",
      "loss 3.0275943279266357\n",
      "steps done 970150\n",
      "update target\n",
      "1285 episodes completed\n",
      "loss 3.2122273445129395\n",
      "steps done 973670\n",
      "1290 episodes completed\n",
      "loss 5.798316955566406\n",
      "steps done 976850\n",
      "1295 episodes completed\n",
      "loss 3.5988259315490723\n",
      "steps done 980960\n",
      "update target\n",
      "1300 episodes completed\n",
      "loss 5.258791446685791\n",
      "steps done 985280\n",
      "1305 episodes completed\n",
      "loss 4.800044536590576\n",
      "steps done 988730\n",
      "1310 episodes completed\n",
      "loss 2.396615982055664\n",
      "steps done 992350\n",
      "update target\n",
      "1315 episodes completed\n",
      "loss 2.7592461109161377\n",
      "steps done 995930\n",
      "1320 episodes completed\n",
      "loss 4.792372703552246\n",
      "steps done 999490\n",
      "update target\n",
      "1325 episodes completed\n",
      "loss 6.681912899017334\n",
      "steps done 1003780\n",
      "1330 episodes completed\n",
      "loss 2.967848777770996\n",
      "steps done 1007300\n",
      "1335 episodes completed\n",
      "loss 5.237452507019043\n",
      "steps done 1011500\n",
      "update target\n",
      "1340 episodes completed\n",
      "loss 2.504836082458496\n",
      "steps done 1014610\n",
      "1345 episodes completed\n",
      "loss 2.829730987548828\n",
      "steps done 1019710\n",
      "update target\n",
      "1350 episodes completed\n",
      "loss 3.160029888153076\n",
      "steps done 1023170\n",
      "1355 episodes completed\n",
      "loss 10.150869369506836\n",
      "steps done 1026520\n",
      "1360 episodes completed\n",
      "loss 4.027974605560303\n",
      "steps done 1030630\n",
      "update target\n",
      "1365 episodes completed\n",
      "loss 7.087075233459473\n",
      "steps done 1033990\n",
      "1370 episodes completed\n",
      "loss 2.318901300430298\n",
      "steps done 1037380\n",
      "1375 episodes completed\n",
      "loss 4.753657341003418\n",
      "steps done 1040970\n",
      "update target\n",
      "1380 episodes completed\n",
      "loss 6.190370082855225\n",
      "steps done 1045230\n",
      "1385 episodes completed\n",
      "loss 4.284949779510498\n",
      "steps done 1048690\n",
      "1390 episodes completed\n",
      "loss 6.605805397033691\n",
      "steps done 1052190\n",
      "update target\n",
      "1395 episodes completed\n",
      "loss 5.953264236450195\n",
      "steps done 1055960\n",
      "1400 episodes completed\n",
      "loss 4.52042818069458\n",
      "steps done 1059100\n",
      "update target\n",
      "1405 episodes completed\n",
      "loss 9.407073974609375\n",
      "steps done 1063160\n",
      "1410 episodes completed\n",
      "loss 5.4288740158081055\n",
      "steps done 1067440\n",
      "1415 episodes completed\n",
      "loss 1.8840378522872925\n",
      "steps done 1071860\n",
      "update target\n",
      "1420 episodes completed\n",
      "loss 3.118271827697754\n",
      "steps done 1075840\n",
      "1425 episodes completed\n",
      "loss 4.730956554412842\n",
      "steps done 1079510\n",
      "update target\n",
      "1430 episodes completed\n",
      "loss 4.421971797943115\n",
      "steps done 1083660\n",
      "1435 episodes completed\n",
      "loss 11.646011352539062\n",
      "steps done 1087400\n",
      "1440 episodes completed\n",
      "loss 2.8084893226623535\n",
      "steps done 1091060\n",
      "update target\n",
      "1445 episodes completed\n",
      "loss 5.642260551452637\n",
      "steps done 1095010\n",
      "1450 episodes completed\n",
      "loss 5.758710861206055\n",
      "steps done 1098570\n",
      "1455 episodes completed\n",
      "loss 5.735356330871582\n",
      "steps done 1102760\n",
      "update target\n",
      "1460 episodes completed\n",
      "loss 5.2876877784729\n",
      "steps done 1106610\n",
      "1465 episodes completed\n",
      "loss 4.888976097106934\n",
      "steps done 1110380\n",
      "update target\n",
      "1470 episodes completed\n",
      "loss 9.804134368896484\n",
      "steps done 1114240\n",
      "1475 episodes completed\n",
      "loss 2.4694948196411133\n",
      "steps done 1118250\n",
      "1480 episodes completed\n",
      "loss 3.4851233959198\n",
      "steps done 1122110\n",
      "update target\n",
      "1485 episodes completed\n",
      "loss 4.1270575523376465\n",
      "steps done 1125320\n",
      "1490 episodes completed\n",
      "loss 6.51772928237915\n",
      "steps done 1129080\n",
      "1495 episodes completed\n",
      "loss 6.420326232910156\n",
      "steps done 1132680\n",
      "update target\n",
      "1500 episodes completed\n",
      "loss 6.438791751861572\n",
      "steps done 1136550\n",
      "1505 episodes completed\n",
      "loss 3.9387664794921875\n",
      "steps done 1140300\n",
      "update target\n",
      "1510 episodes completed\n",
      "loss 4.311709880828857\n",
      "steps done 1143950\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://nvkfapipi4.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "observation = fireEnv.reset()\n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "episode_memory_1 = []\n",
    "episode_memory_2 = []\n",
    "\n",
    "hidden_1 = policy_net.init_hidden_state()\n",
    "hidden_2 = policy_net.init_hidden_state()\n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "  for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "    observation = fireEnv.step()\n",
    "\n",
    "    state_vector_1 = dronesEnv.drones[0].state\n",
    "    map_1 = dronesEnv.drones[0].observation\n",
    "    state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "    map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_2 = dronesEnv.drones[1].state\n",
    "    map_2 = dronesEnv.drones[1].observation\n",
    "    state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "    for i in range(int(DT/DTI)):\n",
    "\n",
    "      action1, hidden_1 = policy_net.select_action(map_1, state_vector_1, steps, hidden=hidden_1)\n",
    "      action2, hidden_2 = policy_net.select_action(map_2, state_vector_2, steps, hidden=hidden_2)\n",
    "      steps += 2\n",
    "      reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "      next_state_vector_1 = dronesEnv.drones[0].state\n",
    "      next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "      next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "      next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "      next_state_vector_2 = dronesEnv.drones[1].state\n",
    "      next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "      next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "      next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "      reward_1 = torch.tensor([reward_1], device=device)\n",
    "      reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "      episode_memory_1.append(Transition(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1))\n",
    "      episode_memory_2.append(Transition(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2))\n",
    "\n",
    "      state_vector_1 = next_state_vector_1\n",
    "      state_vector_2 = next_state_vector_2\n",
    "\n",
    "      map_1 = next_map_1\n",
    "      map_2 = next_map_2\n",
    "\n",
    "    if i_episode>=INIT_SIZE:\n",
    "      loss = optimize_model()\n",
    "\n",
    "    if not fireEnv.fire_in_range(6):\n",
    "      observation = fireEnv.reset()\n",
    "      dronesEnv.reset(observation)\n",
    "      episode_buffer.push(episode_memory_1.copy())\n",
    "      episode_buffer.push(episode_memory_2.copy())\n",
    "      episode_memory_1 = []\n",
    "      episode_memory_2 = []\n",
    "      hidden_1 = policy_net.init_hidden_state()\n",
    "      hidden_2 = policy_net.init_hidden_state()\n",
    "      i_episode +=1\n",
    "      \n",
    "      if (i_episode+1) % 5 == 0:\n",
    "        print(f'{i_episode+1} episodes completed')\n",
    "        print(f'loss {loss}')\n",
    "        print(f'steps done {steps}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
