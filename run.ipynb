{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgpath2mpl\n",
      "  Downloading svgpath2mpl-1.0.0-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "Installing collected packages: svgpath2mpl\n",
      "Successfully installed svgpath2mpl-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from replay_memory import ReplayMemory, Transition\n",
    "from models import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 200000\n",
    "INIT_SIZE = 20000\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "policy_file_path = f'./policy_weights.pt'\n",
    "target_file_path = f'./target_weights.pt'\n",
    "\n",
    "#policy_net.load_state_dict(torch.load(policy_file_path))\n",
    "#target_net.load_state_dict(torch.load('target_weights.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = ReplayMemory(70000)\n",
    "#memory.load()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(belief_map, state_vector, steps):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      output = policy_net(belief_map, state_vector).max(1)[1].view(1, 1)\n",
    "      return output\n",
    "  else:\n",
    "    return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "    \n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(belief_map_batch, state_vector_batch).gather(1, action_batch)\n",
    "    next_state_values = target_net(next_belief_map, next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights2.pt'\n",
    "        target_file_path = f'./target_weights2.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episodes completed\n",
      "loss None\n",
      "steps done 3170\n",
      "10 episodes completed\n",
      "loss None\n",
      "steps done 8410\n",
      "15 episodes completed\n",
      "loss None\n",
      "steps done 13890\n",
      "20 episodes completed\n",
      "loss None\n",
      "steps done 18970\n",
      "25 episodes completed\n",
      "loss 0.4194021224975586\n",
      "steps done 24290\n",
      "30 episodes completed\n",
      "loss 0.6819818019866943\n",
      "steps done 29530\n",
      "update target\n",
      "35 episodes completed\n",
      "loss 0.2566223442554474\n",
      "steps done 35040\n",
      "update target\n",
      "40 episodes completed\n",
      "loss 0.5716414451599121\n",
      "steps done 40360\n",
      "45 episodes completed\n",
      "loss 0.3863264322280884\n",
      "steps done 45910\n",
      "update target\n",
      "50 episodes completed\n",
      "loss 0.9412010312080383\n",
      "steps done 51720\n",
      "55 episodes completed\n",
      "loss 0.34435802698135376\n",
      "steps done 57310\n",
      "update target\n",
      "60 episodes completed\n",
      "loss 0.48565080761909485\n",
      "steps done 62810\n",
      "65 episodes completed\n",
      "loss 0.37440213561058044\n",
      "steps done 68080\n",
      "update target\n",
      "70 episodes completed\n",
      "loss 0.9166136384010315\n",
      "steps done 73010\n",
      "75 episodes completed\n",
      "loss 0.526171863079071\n",
      "steps done 78410\n",
      "update target\n",
      "80 episodes completed\n",
      "loss 1.2326380014419556\n",
      "steps done 83370\n",
      "85 episodes completed\n",
      "loss 0.3811759352684021\n",
      "steps done 88450\n",
      "update target\n",
      "90 episodes completed\n",
      "loss 0.7989904880523682\n",
      "steps done 93720\n",
      "95 episodes completed\n",
      "loss 0.39500632882118225\n",
      "steps done 99220\n",
      "update target\n",
      "100 episodes completed\n",
      "loss 0.5418843030929565\n",
      "steps done 104740\n",
      "105 episodes completed\n",
      "loss 0.5741668343544006\n",
      "steps done 109740\n",
      "update target\n",
      "110 episodes completed\n",
      "loss 0.6812609434127808\n",
      "steps done 114970\n",
      "115 episodes completed\n",
      "loss 0.9185386896133423\n",
      "steps done 119860\n",
      "update target\n",
      "120 episodes completed\n",
      "loss 0.5856112241744995\n",
      "steps done 125300\n",
      "update target\n",
      "125 episodes completed\n",
      "loss 0.8346196413040161\n",
      "steps done 130710\n",
      "130 episodes completed\n",
      "loss 1.1532386541366577\n",
      "steps done 135690\n",
      "update target\n",
      "135 episodes completed\n",
      "loss 1.2415919303894043\n",
      "steps done 141090\n",
      "140 episodes completed\n",
      "loss 1.5367259979248047\n",
      "steps done 146530\n",
      "update target\n",
      "145 episodes completed\n",
      "loss 0.6755968332290649\n",
      "steps done 151850\n",
      "150 episodes completed\n",
      "loss 0.8418007493019104\n",
      "steps done 157540\n",
      "update target\n",
      "155 episodes completed\n",
      "loss 1.29157292842865\n",
      "steps done 162650\n",
      "160 episodes completed\n",
      "loss 1.0622479915618896\n",
      "steps done 167920\n",
      "update target\n",
      "165 episodes completed\n",
      "loss 0.5439260601997375\n",
      "steps done 173130\n",
      "170 episodes completed\n",
      "loss 0.5476170182228088\n",
      "steps done 178490\n",
      "update target\n",
      "175 episodes completed\n",
      "loss 1.8347700834274292\n",
      "steps done 183720\n",
      "180 episodes completed\n",
      "loss 1.083657145500183\n",
      "steps done 189210\n",
      "update target\n",
      "185 episodes completed\n",
      "loss 0.9005234837532043\n",
      "steps done 194440\n",
      "190 episodes completed\n",
      "loss 1.183925747871399\n",
      "steps done 199780\n",
      "update target\n",
      "195 episodes completed\n",
      "loss 1.059016227722168\n",
      "steps done 205510\n",
      "update target\n",
      "200 episodes completed\n",
      "loss 1.1400471925735474\n",
      "steps done 210610\n",
      "205 episodes completed\n",
      "loss 1.2833635807037354\n",
      "steps done 215710\n",
      "update target\n",
      "210 episodes completed\n",
      "loss 1.707848310470581\n",
      "steps done 221320\n",
      "215 episodes completed\n",
      "loss 1.3874574899673462\n",
      "steps done 226210\n",
      "update target\n",
      "220 episodes completed\n",
      "loss 1.102759838104248\n",
      "steps done 231330\n",
      "225 episodes completed\n",
      "loss 1.9586236476898193\n",
      "steps done 236450\n",
      "update target\n",
      "230 episodes completed\n",
      "loss 2.5674729347229004\n",
      "steps done 241820\n",
      "235 episodes completed\n",
      "loss 1.6393934488296509\n",
      "steps done 247040\n",
      "update target\n",
      "240 episodes completed\n",
      "loss 1.7441372871398926\n",
      "steps done 251910\n",
      "245 episodes completed\n",
      "loss 1.616355299949646\n",
      "steps done 257550\n",
      "update target\n",
      "250 episodes completed\n",
      "loss 1.0064282417297363\n",
      "steps done 262450\n",
      "255 episodes completed\n",
      "loss 1.558839201927185\n",
      "steps done 267780\n",
      "update target\n",
      "260 episodes completed\n",
      "loss 1.3647511005401611\n",
      "steps done 273070\n",
      "265 episodes completed\n",
      "loss 1.2346601486206055\n",
      "steps done 278480\n",
      "update target\n",
      "270 episodes completed\n",
      "loss 1.824984073638916\n",
      "steps done 284000\n",
      "275 episodes completed\n",
      "loss 1.860458254814148\n",
      "steps done 289270\n",
      "update target\n",
      "280 episodes completed\n",
      "loss 1.9677989482879639\n",
      "steps done 294730\n",
      "285 episodes completed\n",
      "loss 1.9731483459472656\n",
      "steps done 299740\n",
      "update target\n",
      "290 episodes completed\n",
      "loss 2.2213284969329834\n",
      "steps done 305380\n",
      "update target\n",
      "295 episodes completed\n",
      "loss 2.058826446533203\n",
      "steps done 310530\n",
      "300 episodes completed\n",
      "loss 1.3712074756622314\n",
      "steps done 315990\n",
      "update target\n",
      "305 episodes completed\n",
      "loss 2.9112071990966797\n",
      "steps done 321280\n",
      "310 episodes completed\n",
      "loss 2.1248836517333984\n",
      "steps done 326300\n",
      "update target\n",
      "315 episodes completed\n",
      "loss 3.2862601280212402\n",
      "steps done 331890\n",
      "320 episodes completed\n",
      "loss 1.9974185228347778\n",
      "steps done 337180\n",
      "update target\n",
      "325 episodes completed\n",
      "loss 3.190337657928467\n",
      "steps done 342400\n",
      "330 episodes completed\n",
      "loss 2.1069388389587402\n",
      "steps done 347830\n",
      "update target\n",
      "335 episodes completed\n",
      "loss 2.8762729167938232\n",
      "steps done 352800\n",
      "340 episodes completed\n",
      "loss 2.7593533992767334\n",
      "steps done 357810\n",
      "update target\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://n2u0jxqivt.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "observation = fireEnv.reset()\n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "  for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "    observation = fireEnv.step()\n",
    "\n",
    "    state_vector_1 = dronesEnv.drones[0].state\n",
    "    map_1 = dronesEnv.drones[0].observation\n",
    "    state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "    map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_2 = dronesEnv.drones[1].state\n",
    "    map_2 = dronesEnv.drones[1].observation\n",
    "    state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "    for i in range(int(DT/DTI)):\n",
    "      action1 = select_action(map_1, state_vector_1, steps)\n",
    "      action2 = select_action(map_2, state_vector_2, steps)\n",
    "      steps += 2\n",
    "      reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "      next_state_vector_1 = dronesEnv.drones[0].state\n",
    "      next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "      next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "      next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "      next_state_vector_2 = dronesEnv.drones[1].state\n",
    "      next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "      next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "      next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "      reward_1 = torch.tensor([reward_1], device=device)\n",
    "      reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "      memory.push(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1)\n",
    "      memory.push(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2)\n",
    "\n",
    "      state_vector_1 = next_state_vector_1\n",
    "      state_vector_2 = next_state_vector_2\n",
    "\n",
    "      map_1 = next_map_1\n",
    "      map_2 = next_map_2\n",
    "\n",
    "    if not fireEnv.fire_in_range(6):\n",
    "      observation = fireEnv.reset()\n",
    "      dronesEnv.reset(observation)\n",
    "\n",
    "      i_episode +=1\n",
    "      if (i_episode+1) % 5 == 0:\n",
    "        print(f'{i_episode+1} episodes completed')\n",
    "        print(f'loss {loss}')\n",
    "        print(f'steps done {steps}')\n",
    "      \n",
    "\n",
    "  if steps>=INIT_SIZE:\n",
    "    loss = optimize_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
