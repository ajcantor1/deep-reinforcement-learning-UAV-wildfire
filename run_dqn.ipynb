{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgpath2mpl in /home/jeremy/.local/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/jeremy/.local/lib/python3.8/site-packages (from svgpath2mpl) (3.4.3)\n",
      "Requirement already satisfied: numpy in /home/jeremy/.local/lib/python3.8/site-packages (from svgpath2mpl) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib->svgpath2mpl) (2.7.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jeremy/.local/lib/python3.8/site-packages (from matplotlib->svgpath2mpl) (2.4.7)\n",
      "Requirement already satisfied: six in /home/jeremy/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->svgpath2mpl) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "#from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "#from drone_env import DronesEnv\n",
    "from env.wildfire_gym import WildFireGym\n",
    "from replay_memory import ReplayMemory, Transition\n",
    "from networks.dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 200000\n",
    "INIT_SIZE = 20000\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(device, channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "policy_file_path = f'./policy_weights.pt'\n",
    "target_file_path = f'./target_weights.pt'\n",
    "\n",
    "#policy_net.load_state_dict(torch.load(policy_file_path))\n",
    "#target_net.load_state_dict(torch.load('target_weights.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = ReplayMemory(70000)\n",
    "#memory.load()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(belief_map, state_vector, steps):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      output = policy_net(belief_map, state_vector).max(1)[1].view(1, 1)\n",
    "      return output\n",
    "  else:\n",
    "    return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "    \n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(belief_map_batch, state_vector_batch).gather(1, action_batch)\n",
    "    next_state_values = target_net(next_belief_map, next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights2.pt'\n",
    "        target_file_path = f'./target_weights2.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 completed\n",
      "episode 2 completed\n",
      "episode 3 completed\n",
      "episode 4 completed\n",
      "episode 5 completed\n",
      "episode 6 completed\n",
      "episode 7 completed\n",
      "episode 8 completed\n",
      "episode 9 completed\n",
      "episode 10 completed\n",
      "episode 11 completed\n",
      "episode 12 completed\n",
      "episode 13 completed\n",
      "episode 14 completed\n",
      "episode 15 completed\n",
      "episode 16 completed\n",
      "episode 17 completed\n",
      "episode 18 completed\n",
      "episode 19 completed\n",
      "episode 20 completed\n",
      "episode 21 completed\n",
      "episode 22 completed\n",
      "episode 23 completed\n",
      "episode 24 completed\n",
      "episode 25 completed\n",
      "episode 26 completed\n",
      "episode 27 completed\n",
      "episode 28 completed\n",
      "episode 29 completed\n",
      "episode 30 completed\n",
      "episode 31 completed\n",
      "episode 32 completed\n",
      "episode 33 completed\n",
      "episode 34 completed\n",
      "episode 35 completed\n",
      "episode 36 completed\n",
      "episode 37 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance/run_dqn.ipynb Cell 7\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepisode \u001b[39m\u001b[39m{\u001b[39;00mi_episode\u001b[39m}\u001b[39;00m\u001b[39m completed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m i_episode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m observation \u001b[39m=\u001b[39m wildFireGym\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m map_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(observation[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbelief_map\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m map_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(observation[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbelief_map\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "File \u001b[0;32m~/Desktop/projects/notebooks/Distributed Wildfire Surveillance/env/wildfire_gym.py:27\u001b[0m, in \u001b[0;36mWildFireGym.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     seed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfireEnv\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdronesEnv\u001b[39m.\u001b[39mreset(seed)\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/notebooks/Distributed Wildfire Surveillance/env/abstract_fire_env.py:51\u001b[0m, in \u001b[0;36mAbstractFireEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m seed\n",
      "File \u001b[0;32m~/Desktop/projects/notebooks/Distributed Wildfire Surveillance/env/abstract_fire_env.py:37\u001b[0m, in \u001b[0;36mAbstractFireEnv.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 37\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_observation()\n\u001b[1;32m     38\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation\n",
      "File \u001b[0;32m~/Desktop/projects/notebooks/Distributed Wildfire Surveillance/env/probabilistic_fire_env.py:38\u001b[0m, in \u001b[0;36mProbabilisticFireEnv.next_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m pnm \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m neighboring_cell \u001b[39min\u001b[39;00m neighboring_cells:\n\u001b[0;32m---> 38\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation[neighboring_cell] \u001b[39m==\u001b[39;49m \u001b[39m1\u001b[39;49m:\n\u001b[1;32m     39\u001b[0m     dnmkl \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([a\u001b[39m-\u001b[39mb \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(neighboring_cell, (row,col))])\n\u001b[1;32m     40\u001b[0m     norm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dnmkl\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "wildFireGym = WildFireGym()\n",
    "observation = wildFireGym.reset()\n",
    "steps = 0 \n",
    "\n",
    "map_1 = torch.tensor(observation[0]['belief_map'], device=device, dtype=torch.float)\n",
    "map_2 = torch.tensor(observation[1]['belief_map'], device=device, dtype=torch.float)\n",
    "\n",
    "state_vector_1 = torch.tensor(observation[0]['state_vector'], device=device, dtype=torch.float)\n",
    "state_vector_2 = torch.tensor(observation[1]['state_vector'], device=device, dtype=torch.float)\n",
    "\n",
    "while True:\n",
    "\n",
    "  action1 = select_action(map_1, state_vector_1, steps)\n",
    "  action2 = select_action(map_2, state_vector_2, steps)\n",
    "\n",
    "  next_observation, rewards, done, _ = wildFireGym.step([action1, action2])\n",
    "\n",
    "  reward_1 = torch.tensor([rewards[0]], device=device)\n",
    "  reward_2 = torch.tensor([rewards[1]], device=device)  \n",
    "\n",
    "  next_map_1 = torch.tensor(next_observation[0]['belief_map'], device=device, dtype=torch.float)\n",
    "  next_map_2 = torch.tensor(next_observation[1]['belief_map'], device=device, dtype=torch.float)\n",
    "\n",
    "  next_state_vector_1 = torch.tensor(next_observation[0]['state_vector'], device=device, dtype=torch.float)\n",
    "  next_state_vector_2 = torch.tensor(next_observation[1]['state_vector'], device=device, dtype=torch.float)\n",
    "\n",
    "  memory.push(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1)\n",
    "  memory.push(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2)\n",
    "\n",
    "  observation = next_observation\n",
    "\n",
    "  map_1 = next_map_1\n",
    "  map_2 = next_map_2\n",
    "\n",
    "  state_vector_1 = next_state_vector_1\n",
    "  state_vector_2 = next_state_vector_2\n",
    "\n",
    "  steps += 1\n",
    "  \n",
    "  if done:\n",
    "    print(f'episode {i_episode} completed')\n",
    "    i_episode += 1\n",
    "    observation = wildFireGym.reset()\n",
    "\n",
    "    map_1 = torch.tensor(observation[0]['belief_map'], device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(observation[1]['belief_map'], device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_1 = torch.tensor(observation[0]['state_vector'], device=device, dtype=torch.float)\n",
    "    state_vector_2 = torch.tensor(observation[1]['state_vector'], device=device, dtype=torch.float)\n",
    "\n",
    "  if steps>=INIT_SIZE:\n",
    "    loss = optimize_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
