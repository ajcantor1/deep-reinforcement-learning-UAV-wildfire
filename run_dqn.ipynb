{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgpath2mpl in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from replay_memory import ReplayMemory, Transition\n",
    "from models.dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 100\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 200000\n",
    "INIT_SIZE = 20000\n",
    "TARGET_UPDATE = 1000\n",
    "SAVE_POLICY = 100\n",
    "EPISODE_LENGTH = 250\n",
    "TRAIN_FREQ  = 10   # Number of samples to generate between trainings (Should be multiple of 10)\n",
    "PRINT_FREQ  = 100  # Frequency of printing (Should be a multiple of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_actions = 2\n",
    "screen_height = screen_width = 100\n",
    "channels = 2\n",
    "policy_net = DQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(channels, screen_height, screen_width, n_actions).to(device)\n",
    "steps = 0\n",
    "policy_file_path = f'./policy_weights.pt'\n",
    "target_file_path = f'./target_weights.pt'\n",
    "\n",
    "#policy_net.load_state_dict(torch.load(policy_file_path))\n",
    "#target_net.load_state_dict(torch.load('target_weights.pt'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = ReplayMemory(70000)\n",
    "#memory.load()\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "update_counter = 0\n",
    "optimizer = Adamax(policy_net.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(belief_map, state_vector, steps):\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      output = policy_net(belief_map, state_vector).max(1)[1].view(1, 1)\n",
    "      return output\n",
    "  else:\n",
    "    return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    global update_counter\n",
    "    update_counter += 1\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    next_states = torch.cat(batch.next_state_vector)\n",
    "    next_belief_map = torch.cat(batch.next_belief_map)\n",
    "\n",
    "    belief_map_batch = torch.cat(batch.belief_map)\n",
    "    state_vector_batch = torch.cat(batch.state_vector)\n",
    "    \n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(belief_map_batch, state_vector_batch).gather(1, action_batch)\n",
    "    next_state_values = target_net(next_belief_map, next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if update_counter % TARGET_UPDATE == 0:\n",
    "        policy_file_path = f'./policy_weights2.pt'\n",
    "        target_file_path = f'./target_weights2.pt'\n",
    "        torch.save(policy_net.state_dict(), policy_file_path)\n",
    "        torch.save(target_net.state_dict(), target_file_path)\n",
    "        print('update target')\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episodes completed\n",
      "loss None\n",
      "steps done 2170\n",
      "10 episodes completed\n",
      "loss None\n",
      "steps done 5590\n",
      "15 episodes completed\n",
      "loss None\n",
      "steps done 8790\n",
      "20 episodes completed\n",
      "loss None\n",
      "steps done 12890\n",
      "25 episodes completed\n",
      "loss None\n",
      "steps done 16930\n",
      "30 episodes completed\n",
      "loss 0.6781970262527466\n",
      "steps done 21310\n",
      "35 episodes completed\n",
      "loss 0.7727193832397461\n",
      "steps done 25060\n",
      "40 episodes completed\n",
      "loss 0.6906485557556152\n",
      "steps done 28910\n",
      "update target\n",
      "45 episodes completed\n",
      "loss 0.7573515176773071\n",
      "steps done 32250\n",
      "50 episodes completed\n",
      "loss 0.3136163353919983\n",
      "steps done 36260\n",
      "55 episodes completed\n",
      "loss 0.18435485661029816\n",
      "steps done 39530\n",
      "update target\n",
      "60 episodes completed\n",
      "loss 0.7223389148712158\n",
      "steps done 43000\n",
      "65 episodes completed\n",
      "loss 0.456302672624588\n",
      "steps done 46230\n",
      "70 episodes completed\n",
      "loss 0.6229659914970398\n",
      "steps done 49650\n",
      "update target\n",
      "75 episodes completed\n",
      "loss 0.6317541599273682\n",
      "steps done 53140\n",
      "80 episodes completed\n",
      "loss 0.5283146500587463\n",
      "steps done 56660\n",
      "85 episodes completed\n",
      "loss 0.4986885190010071\n",
      "steps done 59650\n",
      "update target\n",
      "90 episodes completed\n",
      "loss 0.3001697361469269\n",
      "steps done 63720\n",
      "95 episodes completed\n",
      "loss 0.7901657819747925\n",
      "steps done 67730\n",
      "update target\n",
      "100 episodes completed\n",
      "loss 0.7546328902244568\n",
      "steps done 70990\n",
      "105 episodes completed\n",
      "loss 0.5232391953468323\n",
      "steps done 74970\n",
      "110 episodes completed\n",
      "loss 0.9452009201049805\n",
      "steps done 78550\n",
      "update target\n",
      "115 episodes completed\n",
      "loss 1.289641261100769\n",
      "steps done 82070\n",
      "120 episodes completed\n",
      "loss 1.1335569620132446\n",
      "steps done 86140\n",
      "update target\n",
      "125 episodes completed\n",
      "loss 1.0756020545959473\n",
      "steps done 90230\n",
      "130 episodes completed\n",
      "loss 0.9998044967651367\n",
      "steps done 94270\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance/run_dqn.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m   map_2 \u001b[39m=\u001b[39m next_map_2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fireEnv\u001b[39m.\u001b[39mfire_in_range(\u001b[39m6\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m   observation \u001b[39m=\u001b[39m fireEnv\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m   dronesEnv\u001b[39m.\u001b[39mreset(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_dqn.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m   i_episode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/notebooks/abstract_fire_env.py:51\u001b[0m, in \u001b[0;36mAbstractFireEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m seed\n",
      "File \u001b[0;32m/notebooks/abstract_fire_env.py:37\u001b[0m, in \u001b[0;36mAbstractFireEnv.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 37\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_observation()\n\u001b[1;32m     38\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation\n",
      "File \u001b[0;32m/notebooks/probabilistic_fire_env.py:35\u001b[0m, in \u001b[0;36mProbabilisticFireEnv.next_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation[row,col] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation[row,col] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuel[row, col] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m   neighboring_cells \u001b[39m=\u001b[39m getNeighbors((row, col))\n\u001b[1;32m     36\u001b[0m   pnm \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     37\u001b[0m   \u001b[39mfor\u001b[39;00m neighboring_cell \u001b[39min\u001b[39;00m neighboring_cells:\n",
      "File \u001b[0;32m/notebooks/probabilistic_fire_env.py:18\u001b[0m, in \u001b[0;36mgetNeighbors\u001b[0;34m(point)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(min_y, max_y): \n\u001b[1;32m     17\u001b[0m   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(min_x, max_x):\n\u001b[0;32m---> 18\u001b[0m     neighbors\u001b[39m.\u001b[39;49mappend((y, x))\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m neighbors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "loss = None\n",
    "i_episode = 1\n",
    "\n",
    "observation = fireEnv.reset()\n",
    "dronesEnv.reset(observation)\n",
    "\n",
    "while True:\n",
    "  # Initialize the environment and state\n",
    "  #env.reset()\n",
    "  for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "    observation = fireEnv.step()\n",
    "\n",
    "    state_vector_1 = dronesEnv.drones[0].state\n",
    "    map_1 = dronesEnv.drones[0].observation\n",
    "    state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "    map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "    state_vector_2 = dronesEnv.drones[1].state\n",
    "    map_2 = dronesEnv.drones[1].observation\n",
    "    state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "    map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "    for i in range(int(DT/DTI)):\n",
    "      action1 = select_action(map_1, state_vector_1, steps)\n",
    "      action2 = select_action(map_2, state_vector_2, steps)\n",
    "      steps += 2\n",
    "      reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "      next_state_vector_1 = dronesEnv.drones[0].state\n",
    "      next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "      next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "      next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "      next_state_vector_2 = dronesEnv.drones[1].state\n",
    "      next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "      next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "      next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "      reward_1 = torch.tensor([reward_1], device=device)\n",
    "      reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "      memory.push(map_1, state_vector_1, action1, next_map_1, next_state_vector_1, reward_1)\n",
    "      memory.push(map_2, state_vector_2, action2, next_map_2, next_state_vector_2, reward_2)\n",
    "\n",
    "      state_vector_1 = next_state_vector_1\n",
    "      state_vector_2 = next_state_vector_2\n",
    "\n",
    "      map_1 = next_map_1\n",
    "      map_2 = next_map_2\n",
    "\n",
    "    if not fireEnv.fire_in_range(6):\n",
    "      observation = fireEnv.reset()\n",
    "      dronesEnv.reset(observation)\n",
    "\n",
    "      i_episode +=1\n",
    "      if (i_episode+1) % 5 == 0:\n",
    "        print(f'{i_episode+1} episodes completed')\n",
    "        print(f'loss {loss}')\n",
    "        print(f'steps done {steps}')\n",
    "      \n",
    "\n",
    "  if steps>=INIT_SIZE:\n",
    "    loss = optimize_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
