{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from networks.ppo_net import PPONet\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "n_actions = 2\n",
    "height = width = 100\n",
    "channels = 2\n",
    "EPISODES_PER_BATCH = 5\n",
    "TRAIN_FREQ  = 10\n",
    "SAVE_FREQ = 10\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "actor = PPONet(device,  channels, height, width, n_actions).to(device)\n",
    "critic = PPONet(device, channels, height, width, 1).to(device)\n",
    "\n",
    "optimizer_actor = torch.optim.Adam(params=actor.parameters(), lr=0.001)\n",
    "optimizer_critic = torch.optim.Adam(params=critic.parameters(), lr=0.001)\n",
    "\n",
    "cov_var = torch.full(size=(n_actions,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(belief_map, state_vector, hidden):\n",
    "    mean, new_hidden = actor(belief_map, state_vector, hidden)\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.detach().numpy(), log_prob.detach(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('belief_map', 'state_vector', 'action', 'log_probability' ,'reward'))\n",
    "\n",
    "class EpisodeTransitions:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._transitions = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        self._transitions.append(Transition(*args))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self._transitions[index]\n",
    "\n",
    "    @property \n",
    "    def transitions(self):\n",
    "        return self._transitions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._transitions)+1\n",
    "\n",
    "class EpisodeMemoryBuffer:\n",
    "\n",
    "    def __init__(self, capacity=10):\n",
    "        self.capicity = capacity\n",
    "        self.memory = []\n",
    "        \n",
    "    def push(self, episode):\n",
    "        self.memory.append(episode)\n",
    "    \n",
    "    @property\n",
    "    def get_batch(self):\n",
    "        transitions = []\n",
    "        for episode in self.memory:\n",
    "            transitions.extend(episode.transistions)\n",
    "        \n",
    "        return transitions\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(episode) for episode in self.memory])\n",
    "\n",
    "    @property\n",
    "    def reward_to_go(self):\n",
    "        _reward_to_go = []\n",
    "        for episode in reversed(self.memory):\n",
    "\n",
    "            discounted_reward = 0\n",
    "\n",
    "            for transition in episode.transitions:\n",
    "                discounted_reward = transition.reward.item() + discounted_reward * GAMMA\n",
    "                _reward_to_go = [discounted_reward] + _reward_to_go\n",
    "        \n",
    "        return torch.Tensor(_reward_to_go, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "\n",
    "def rollout():\n",
    "\n",
    "    memory_buffer = EpisodeMemoryBuffer\n",
    "\n",
    "    episode_transitions_1 = EpisodeTransitions()\n",
    "    episode_transitions_2 = EpisodeTransitions()\n",
    "\n",
    "    episode_i = 0\n",
    "\n",
    "    observation = fireEnv.reset()\n",
    "    dronesEnv.reset(observation)\n",
    "\n",
    "    episode_length = 0 \n",
    "\n",
    "    while episode_i < EPISODES_PER_BATCH:\n",
    "\n",
    "        for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "            observation = fireEnv.step()\n",
    "\n",
    "            state_vector_1 = dronesEnv.drones[0].state\n",
    "            map_1 = dronesEnv.drones[0].observation\n",
    "            state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "            map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "            state_vector_2 = dronesEnv.drones[1].state\n",
    "            map_2 = dronesEnv.drones[1].observation\n",
    "            state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "            map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "            for i in range(int(DT/DTI)):\n",
    "\n",
    "                episode_length += 1\n",
    "                action1, log_probability_1, hidden_1 = get_action(map_1, state_vector_1, hidden_1)\n",
    "                action2, log_probability_2, hidden_2 = get_action(map_2, state_vector_2, hidden_2)\n",
    "                reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "                next_state_vector_1 = dronesEnv.drones[0].state\n",
    "                next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "                next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "                next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "                next_state_vector_2 = dronesEnv.drones[1].state\n",
    "                next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "                next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "                next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "                reward_1 = torch.tensor([reward_1], device=device)\n",
    "                reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "                episode_transitions_1.push(map_1, state_vector_1, action1, log_probability_1, reward_1)\n",
    "                episode_transitions_2.push(map_2, state_vector_2, action2, log_probability_2, reward_2)\n",
    "\n",
    "                state_vector_1 = next_state_vector_1\n",
    "                state_vector_2 = next_state_vector_2\n",
    "\n",
    "                map_1 = next_map_1\n",
    "                map_2 = next_map_2\n",
    "\n",
    "            if not fireEnv.fire_in_range(6):\n",
    "                memory_buffer.push(episode_transitions_1)\n",
    "                memory_buffer.push(episode_transitions_2)\n",
    "                episode_i += 1\n",
    "                observation = fireEnv.reset()\n",
    "                dronesEnv.reset(observation)\n",
    "            \n",
    "    return memory_buffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(belief_maps, state_vectors, actions):\n",
    "    \n",
    "    values, _ = critic(belief_maps, state_vectors).squeeze()\n",
    "\n",
    "\n",
    "    mean = actor(belief_maps, state_vectors)\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "    log_probs = dist.log_prob(actions)\n",
    "\n",
    "\n",
    "    return values, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, total_timesteps):\n",
    "    t_so_far = 0 # Timesteps simulated so far\n",
    "    i_so_far = 0 # Iterations ran so far\n",
    "\n",
    "    while t_so_far < total_timesteps:  \n",
    "        memory_buffer = rollout()\n",
    "\n",
    "        t_so_far += len(memory_buffer)\n",
    "        i_so_far += 1\n",
    "\n",
    "        batch = Transition(*zip(*memory_buffer.get_batch))\n",
    "\n",
    "        belief_map_batch = torch.cat(batch.belief_map)\n",
    "        state_vector_batch = torch.cat(batch.state_vector)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        log_probs_batch = torch.cat(batch.log_probability)\n",
    "        \n",
    "        reward_to_go = memory_buffer.reward_to_go\n",
    "        values, _ = evaluate(belief_map_batch , state_vector_batch, action_batch)\n",
    "        A_k =  reward_to_go - values.detach()  \n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "        for _ in range(5):   \n",
    "            values, current_log_probs  = evaluate(belief_map_batch , state_vector_batch, action_batch)\n",
    "            ratios = torch.exp(current_log_probs - log_probs_batch)\n",
    "            surr1 = ratios * A_k\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            critic_loss = nn.MSELoss()(values, reward_to_go)\n",
    "\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            optimizer_actor.step()\n",
    "\n",
    "            optimizer_critic.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            optimizer_critic.step()\n",
    "\n",
    "        if i_so_far % SAVE_FREQ == 0:\n",
    "            torch.save(self.actor.state_dict(), './ppo_actor.pth')\n",
    "            torch.save(self.critic.state_dict(), './ppo_critic.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
