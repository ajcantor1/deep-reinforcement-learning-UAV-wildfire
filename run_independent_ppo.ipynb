{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgpath2mpl in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (3.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from svgpath2mpl) (1.23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->svgpath2mpl) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->svgpath2mpl) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install svgpath2mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adamax\n",
    "import random\n",
    "import math \n",
    "from svgpath2mpl import parse_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import rotate, shift\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from probabilistic_fire_env import ProbabilisticFireEnv\n",
    "from drone_env import DronesEnv\n",
    "from networks.ppo_net import PPONet\n",
    "from torch.distributions import MultivariateNormal, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT          = 0.5  # Time between wildfire updates            \n",
    "DTI         = 0.1  # Time between aircraft decisions\n",
    "n_actions = 2\n",
    "height = width = 100\n",
    "channels = 2\n",
    "EPISODES_PER_BATCH = 1\n",
    "TRAIN_FREQ  = 10\n",
    "SAVE_FREQ = 10\n",
    "GAMMA = 0.95\n",
    "CLIP  = 0.2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "actor = PPONet(device,  channels, height, width, n_actions).to(device)\n",
    "critic = PPONet(device, channels, height, width, 1).to(device)\n",
    "\n",
    "optimizer_actor = torch.optim.Adam(params=actor.parameters(), lr=0.001)\n",
    "optimizer_critic = torch.optim.Adam(params=critic.parameters(), lr=0.001)\n",
    "\n",
    "cov_var = torch.full(size=(n_actions,), fill_value=0.5, device=device)\n",
    "cov_mat = torch.diag(cov_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(belief_map, state_vector, hidden):\n",
    "    action_probs, new_hidden = actor(belief_map, state_vector, hidden)\n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action, log_prob, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('belief_map', 'state_vector', 'action', 'log_probability' ,'reward'))\n",
    "\n",
    "class EpisodeTransitions:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._transitions = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        self._transitions.append(Transition(*args))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self._transitions[index]\n",
    "\n",
    "    @property \n",
    "    def transitions(self):\n",
    "        return self._transitions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._transitions)+1\n",
    "\n",
    "class EpisodeMemoryBuffer:\n",
    "\n",
    "    def __init__(self, capacity=10):\n",
    "        self.capicity = capacity\n",
    "        self.memory = []\n",
    "        \n",
    "    def push(self, episodeTransitions):\n",
    "        self.memory.append(episodeTransitions)\n",
    "    \n",
    "    @property\n",
    "    def get_batches(self):\n",
    "        batches = []\n",
    "        for episode in self.memory:\n",
    "            batches.extend([episode.transitions[ i : i + BATCH_SIZE] for i in range(0, len(episode.transitions), BATCH_SIZE)])\n",
    "            if len(batches[-1]) < BATCH_SIZE:\n",
    "                batches.pop()\n",
    "        random.shuffle(batches)\n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(episode) for episode in self.memory])\n",
    "\n",
    "\n",
    "def compute_reward_to_go(rewards):\n",
    "    _reward_to_go = []\n",
    "\n",
    "    discounted_reward = 0\n",
    "\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_reward = float(reward.item()) + discounted_reward * GAMMA\n",
    "        _reward_to_go.insert(0, discounted_reward)\n",
    "            \n",
    "    return torch.tensor(_reward_to_go, device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireEnv = ProbabilisticFireEnv(height, width)\n",
    "dronesEnv = DronesEnv(height, width, DT, DTI) \n",
    "\n",
    "def rollout():\n",
    "\n",
    "    memory_buffer = EpisodeMemoryBuffer()\n",
    "\n",
    "    episode_transitions_1 = EpisodeTransitions()\n",
    "    episode_transitions_2 = EpisodeTransitions()\n",
    "\n",
    "    episode_i = 0\n",
    "\n",
    "    observation = fireEnv.reset()\n",
    "    dronesEnv.reset(observation)\n",
    "\n",
    "    episode_length = 0 \n",
    "    hidden_1 = None\n",
    "    hidden_2 = None\n",
    "    while episode_i < EPISODES_PER_BATCH:\n",
    "\n",
    "        for j in range(TRAIN_FREQ//int(2*DT/DTI)):\n",
    "\n",
    "            observation = fireEnv.step()\n",
    "\n",
    "            state_vector_1 = dronesEnv.drones[0].state\n",
    "            map_1 = dronesEnv.drones[0].observation\n",
    "            state_vector_1 = torch.tensor(state_vector_1, device=device, dtype=torch.float)\n",
    "            map_1 = torch.tensor(map_1, device=device, dtype=torch.float)\n",
    "\n",
    "            state_vector_2 = dronesEnv.drones[1].state\n",
    "            map_2 = dronesEnv.drones[1].observation\n",
    "            state_vector_2 = torch.tensor(state_vector_2, device=device, dtype=torch.float)\n",
    "            map_2 = torch.tensor(map_2, device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "            for i in range(int(DT/DTI)):\n",
    "\n",
    "                episode_length += 1\n",
    "                action1, log_probability_1, hidden_1 = get_action(map_1, state_vector_1, hidden_1)\n",
    "                action2, log_probability_2, hidden_2 = get_action(map_2, state_vector_2, hidden_2)\n",
    "                reward_1, reward_2 = dronesEnv.step([action1.item(), action2.item()], observation)\n",
    "\n",
    "                next_state_vector_1 = dronesEnv.drones[0].state\n",
    "                next_map_1 = dronesEnv.drones[0].observation\n",
    "\n",
    "                next_state_vector_1 = torch.tensor(next_state_vector_1, device=device, dtype=torch.float)\n",
    "                next_map_1 = torch.tensor(next_map_1, device=device, dtype=torch.float)\n",
    "\n",
    "                next_state_vector_2 = dronesEnv.drones[1].state\n",
    "                next_map_2 = dronesEnv.drones[1].observation\n",
    "\n",
    "                next_state_vector_2 = torch.tensor(next_state_vector_2, device=device, dtype=torch.float)\n",
    "                next_map_2 = torch.tensor(next_map_2, device=device, dtype=torch.float)\n",
    "\n",
    "                reward_1 = torch.tensor([reward_1], device=device)\n",
    "                reward_2 = torch.tensor([reward_2], device=device)  \n",
    "\n",
    "                episode_transitions_1.push(map_1, state_vector_1, action1, log_probability_1, reward_1)\n",
    "                episode_transitions_2.push(map_2, state_vector_2, action2, log_probability_2, reward_2)\n",
    "\n",
    "                state_vector_1 = next_state_vector_1\n",
    "                state_vector_2 = next_state_vector_2\n",
    "\n",
    "                map_1 = next_map_1\n",
    "                map_2 = next_map_2\n",
    "\n",
    "            if not fireEnv.fire_in_range(6):\n",
    "                hidden_1 = None\n",
    "                hidden_2 = None\n",
    "                memory_buffer.push(episode_transitions_1)\n",
    "                memory_buffer.push(episode_transitions_2)\n",
    "                episode_transitions_1 = EpisodeTransitions()\n",
    "                episode_transitions_2 = EpisodeTransitions()\n",
    "                episode_i += 1\n",
    "                observation = fireEnv.reset()\n",
    "                dronesEnv.reset(observation)\n",
    "            \n",
    "    return memory_buffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(belief_maps, state_vectors, actions):\n",
    "    action_probs, _ = actor(belief_maps, state_vectors)\n",
    "    dist = Categorical(action_probs)\n",
    "    action_logprobs = dist.log_prob(actions)\n",
    "    dist_entropy = dist.entropy()\n",
    "    state_values, _ = critic(belief_maps, state_vectors)\n",
    "    \n",
    "    return action_logprobs, state_values, dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(total_timesteps):\n",
    "    t_so_far = 0 # Timesteps simulated so far\n",
    "    i_so_far = 0 # Iterations ran so far\n",
    "\n",
    "    while t_so_far < total_timesteps:  \n",
    "        memory_buffer = rollout()\n",
    "        batches = memory_buffer.get_batches\n",
    "\n",
    "        for batch in batches[0:1]:\n",
    "\n",
    "            t_so_far += BATCH_SIZE\n",
    "            i_so_far += 1\n",
    "\n",
    "            batch = Transition(*zip(*batch))\n",
    "\n",
    "            belief_map_batch = torch.cat(batch.belief_map)\n",
    "            state_vector_batch = torch.cat(batch.state_vector)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            log_probs_batch = torch.cat(batch.log_probability)\n",
    "            \n",
    "            reward_to_go = compute_reward_to_go(batch.reward)\n",
    "            reward_to_go = (reward_to_go - reward_to_go.mean()) / (reward_to_go.std() + 1e-7)\n",
    "            \n",
    "            \n",
    "            for _ in range(5):   \n",
    "                \n",
    "                \n",
    "                logprobs, state_values, dist_entropy = evaluate(belief_map_batch , state_vector_batch, action_batch)\n",
    "                state_values = torch.squeeze(state_values)\n",
    "                ratios = torch.exp(logprobs - log_probs_batch)\n",
    "                advantages = reward_to_go - state_values.detach() \n",
    "\n",
    "                surr1 = ratios * advantages\n",
    "                surr2 = torch.clamp(ratios, 1-CLIP, 1+CLIP) * advantages\n",
    "\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                critic_loss = nn.MSELoss()(state_values, reward_to_go)\n",
    "      \n",
    "\t\t\t\t# Calculate gradients and perform backward propagation for actor network\n",
    "                optimizer_actor.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                optimizer_actor.step()\n",
    "\n",
    "\t\t\t\t# Calculate gradients and perform backward propagation for critic network\n",
    "                optimizer_critic.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                optimizer_critic.step()\n",
    "\n",
    "\n",
    "                print(\"one step\")\n",
    "\n",
    "            print(f'Timesteps: {t_so_far}')\n",
    "            if i_so_far % SAVE_FREQ == 0:\n",
    "                torch.save(actor.state_dict(), './ppo_actor.pth')\n",
    "                torch.save(critic.state_dict(), './ppo_critic.pth')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one step\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [200, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance/run_independent_ppo.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_independent_ppo.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m learn(\u001b[39m200_000_000\u001b[39;49m)\n",
      "\u001b[1;32m/home/jeremy/Desktop/projects/notebooks/Distributed Wildfire Surveillance/run_independent_ppo.ipynb Cell 10\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(total_timesteps)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_independent_ppo.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \t\t\t\t\u001b[39m# Calculate gradients and perform backward propagation for actor network\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_independent_ppo.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                 optimizer_actor\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_independent_ppo.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                 actor_loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_independent_ppo.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                 optimizer_actor\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeremy/Desktop/projects/notebooks/Distributed%20Wildfire%20Surveillance/run_independent_ppo.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \t\t\t\t\u001b[39m# Calculate gradients and perform backward propagation for critic network\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [200, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "learn(200_000_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
